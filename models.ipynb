{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYzv0jf0ZIi8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import copy\n",
        "import argparse\n",
        "import pathlib\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "import torch.nn as nn\n",
        "import scipy.spatial\n",
        "from collections import OrderedDict\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import configparser\n",
        "from scipy.interpolate import CubicSpline\n",
        "from google.colab import drive\n",
        "from random import shuffle\n",
        "import math\n",
        "\n",
        "\n",
        "# from engine.util import parse_config\n",
        "# from engine.data_io import load_ucr_pretrain as load_dataset\n",
        "# from engine.model import get_model\n",
        "# from engine.train_test import nn_pretrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRBztnJ6aSNq",
        "outputId": "753ceb43-7f5b-414a-c168-a43550e22602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /mnt/drive\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "drive.mount('/mnt/drive', force_remount=True)\n",
        "drive_path = '/mnt/drive/MyDrive/STAT940'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzB9mBkZwIVq"
      },
      "source": [
        "# Data Stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDO3P73kBBbs"
      },
      "source": [
        "### Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ov_ZxOTJunsu"
      },
      "outputs": [],
      "source": [
        "def _relabel(label):\n",
        "    label_set = np.unique(label)\n",
        "    n_class = len(label_set)\n",
        "\n",
        "    label_re = np.zeros(label.shape[0], dtype=int)\n",
        "    for i, label_i in enumerate(label_set):\n",
        "        label_re[label == label_i] = i\n",
        "    return label_re, n_class\n",
        "\n",
        "\n",
        "def _normalize_dataset(data):\n",
        "    data_mu = np.mean(data, axis=2, keepdims=True)\n",
        "    data_sigma = np.std(data, axis=2, keepdims=True)\n",
        "    data_sigma[data_sigma <= 0] = 1\n",
        "    data = (data - data_mu) / data_sigma\n",
        "    return data\n",
        "\n",
        "def get_ucr_data_names():\n",
        "    names = [\n",
        "        'Adiac',\n",
        "        'ArrowHead',\n",
        "        'Beef',\n",
        "        'BeetleFly',\n",
        "        'BirdChicken',\n",
        "        'Car',\n",
        "        'CBF',\n",
        "        'ChlorineConcentration',\n",
        "        'CinCECGTorso',\n",
        "        'Coffee',\n",
        "        'Computers',\n",
        "        'CricketX',\n",
        "        'CricketY',\n",
        "        'CricketZ',\n",
        "        'DiatomSizeReduction',\n",
        "        'DistalPhalanxOutlineAgeGroup',\n",
        "        'DistalPhalanxOutlineCorrect',\n",
        "        'DistalPhalanxTW',\n",
        "        'Earthquakes',\n",
        "        'ECG200',\n",
        "        'ECG5000',\n",
        "        'ECGFiveDays',\n",
        "        'ElectricDevices',\n",
        "        'FaceAll',\n",
        "        'FaceFour',\n",
        "        'FacesUCR',\n",
        "        'FiftyWords',\n",
        "        'Fish',\n",
        "        'FordA',\n",
        "        'FordB',\n",
        "        'GunPoint',\n",
        "        'Ham',\n",
        "        'HandOutlines',\n",
        "        'Haptics',\n",
        "        'Herring',\n",
        "        'InlineSkate',\n",
        "        'InsectWingbeatSound',\n",
        "        'ItalyPowerDemand',\n",
        "        'LargeKitchenAppliances',\n",
        "        'Lightning2',\n",
        "        'Lightning7',\n",
        "        'Mallat',\n",
        "        'Meat',\n",
        "        'MedicalImages',\n",
        "        'MiddlePhalanxOutlineAgeGroup',\n",
        "        'MiddlePhalanxOutlineCorrect',\n",
        "        'MiddlePhalanxTW',\n",
        "        'MoteStrain',\n",
        "        'NonInvasiveFetalECGThorax1',\n",
        "        'NonInvasiveFetalECGThorax2',\n",
        "        'OliveOil',\n",
        "        'OSULeaf',\n",
        "        'PhalangesOutlinesCorrect',\n",
        "        'Phoneme',\n",
        "        'Plane',\n",
        "        'ProximalPhalanxOutlineAgeGroup',\n",
        "        'ProximalPhalanxOutlineCorrect',\n",
        "        'ProximalPhalanxTW',\n",
        "        'RefrigerationDevices',\n",
        "        'ScreenType',\n",
        "        'ShapeletSim',\n",
        "        'ShapesAll',\n",
        "        'SmallKitchenAppliances',\n",
        "        'SonyAIBORobotSurface1',\n",
        "        'SonyAIBORobotSurface2',\n",
        "        'StarLightCurves',\n",
        "        'Strawberry',\n",
        "        'SwedishLeaf',\n",
        "        'Symbols',\n",
        "        'SyntheticControl',\n",
        "        'ToeSegmentation1',\n",
        "        'ToeSegmentation2',\n",
        "        'Trace',\n",
        "        'TwoLeadECG',\n",
        "        'TwoPatterns',\n",
        "        'UWaveGestureLibraryAll',\n",
        "        'UWaveGestureLibraryX',\n",
        "        'UWaveGestureLibraryY',\n",
        "        'UWaveGestureLibraryZ',\n",
        "        'Wafer',\n",
        "        'Wine',\n",
        "        'WordSynonyms',\n",
        "        'Worms',\n",
        "        'WormsTwoClass',\n",
        "        'Yoga',\n",
        "        'ACSF1',\n",
        "        'AllGestureWiimoteX',\n",
        "        'AllGestureWiimoteY',\n",
        "        'AllGestureWiimoteZ',\n",
        "        'BME',\n",
        "        'Chinatown',\n",
        "        'Crop',\n",
        "        'DodgerLoopDay',\n",
        "        'DodgerLoopGame',\n",
        "        'DodgerLoopWeekend',\n",
        "        'EOGHorizontalSignal',\n",
        "        'EOGVerticalSignal',\n",
        "        'EthanolLevel',\n",
        "        'FreezerRegularTrain',\n",
        "        'FreezerSmallTrain',\n",
        "        'Fungi',\n",
        "        'GestureMidAirD1',\n",
        "        'GestureMidAirD2',\n",
        "        'GestureMidAirD3',\n",
        "        'GesturePebbleZ1',\n",
        "        'GesturePebbleZ2',\n",
        "        'GunPointAgeSpan',\n",
        "        'GunPointMaleVersusFemale',\n",
        "        'GunPointOldVersusYoung',\n",
        "        'HouseTwenty',\n",
        "        'InsectEPGRegularTrain',\n",
        "        'InsectEPGSmallTrain',\n",
        "        'MelbournePedestrian',\n",
        "        'MixedShapesRegularTrain',\n",
        "        'MixedShapesSmallTrain',\n",
        "        'PickupGestureWiimoteZ',\n",
        "        'PigAirwayPressure',\n",
        "        'PigArtPressure',\n",
        "        'PigCVP',\n",
        "        'PLAID',\n",
        "        'PowerCons',\n",
        "        'Rock',\n",
        "        'SemgHandGenderCh2',\n",
        "        'SemgHandMovementCh2',\n",
        "        'SemgHandSubjectCh2',\n",
        "        'ShakeGestureWiimoteZ',\n",
        "        'SmoothSubspace',\n",
        "        'UMD',\n",
        "    ]\n",
        "    return names\n",
        "\n",
        "def load_ucr_dataset(data_name, data_config):\n",
        "    data_config = data_config['data']\n",
        "    data_dir = data_config['data_dir']\n",
        "    max_len = int(data_config['max_len'])\n",
        "    seed = int(data_config['seed'])\n",
        "    pretrain_frac = float(data_config['pretrain_frac'])\n",
        "    train_frac = float(data_config['train_frac'])\n",
        "    valid_frac = float(data_config['valid_frac'])\n",
        "    test_frac = float(data_config['test_frac'])\n",
        "    is_same_length = data_config['is_same_length']\n",
        "    is_same_length = is_same_length.lower() == 'true'\n",
        "    # assert pretrain_frac + train_frac + valid_frac + test_frac == 1.0\n",
        "\n",
        "    reduced_train = 0\n",
        "    if 'reduced_train' in data_config:\n",
        "        reduced_train = float(data_config['reduced_train'])\n",
        "\n",
        "    train_path = os.path.join(\n",
        "        data_dir, 'Missing_value_and_variable_length_datasets_adjusted',\n",
        "        '{0}', '{0}_TRAIN.tsv')\n",
        "    test_path = os.path.join(\n",
        "        data_dir, 'Missing_value_and_variable_length_datasets_adjusted',\n",
        "        '{0}', '{0}_TEST.tsv')\n",
        "    if not os.path.isfile(train_path.format(data_name)):\n",
        "        train_path = os.path.join(data_dir, '{0}', '{0}_TRAIN.tsv')\n",
        "        test_path = os.path.join(data_dir, '{0}', '{0}_TEST.tsv')\n",
        "\n",
        "    train_path = train_path.format(data_name)\n",
        "    test_path = test_path.format(data_name)\n",
        "    data = np.concatenate(\n",
        "        (np.loadtxt(train_path),\n",
        "         np.loadtxt(test_path), ), axis=0)\n",
        "    n_data = data.shape[0]\n",
        "    data_len = data.shape[1] - 1\n",
        "\n",
        "    np.random.seed(seed=seed)\n",
        "    random_vec = np.random.permutation(n_data)\n",
        "    data = data[random_vec, :]\n",
        "\n",
        "    label = data[:, 0]\n",
        "    label = label.astype(int)\n",
        "    data = data[:, 1:]\n",
        "    data = np.expand_dims(data, 1)\n",
        "\n",
        "    if is_same_length:\n",
        "        if data_len != max_len:\n",
        "            data = signal.resample(\n",
        "                data, max_len, axis=2)\n",
        "    else:\n",
        "        if data_len > max_len:\n",
        "            data = signal.resample(\n",
        "                data, max_len, axis=2)\n",
        "\n",
        "    label, n_class = _relabel(label)\n",
        "    if np.isclose(pretrain_frac, 1.0):\n",
        "        data_pretrain = data\n",
        "        data_train = None\n",
        "        data_valid = None\n",
        "        data_test = None\n",
        "        label_train = None\n",
        "        label_valid = None\n",
        "        label_test = None\n",
        "    else:\n",
        "        data_pretrain = []\n",
        "        data_train = []\n",
        "        data_valid = []\n",
        "        data_test = []\n",
        "        label_train = []\n",
        "        label_valid = []\n",
        "        label_test = []\n",
        "        for i in range(n_class):\n",
        "            data_i = data[label == i, :, :]\n",
        "            label_i = label[label == i]\n",
        "\n",
        "            n_data_i = label_i.shape[0]\n",
        "            n_train_i = np.round(train_frac * n_data_i)\n",
        "            n_train_i = int(n_train_i)\n",
        "            n_train_i = max(n_train_i, 1)\n",
        "\n",
        "            n_valid_i = np.round(valid_frac * n_data_i)\n",
        "            n_valid_i = int(n_valid_i)\n",
        "            n_valid_i = max(n_valid_i, 1)\n",
        "\n",
        "            n_test_i = np.round(test_frac * n_data_i)\n",
        "            n_test_i = int(n_test_i)\n",
        "            n_test_i = max(n_test_i, 1)\n",
        "\n",
        "            n_pretrain_i = n_data_i - n_train_i - n_valid_i - n_test_i\n",
        "\n",
        "            train_start = 0\n",
        "            train_end = n_train_i\n",
        "\n",
        "            valid_start = train_end\n",
        "            valid_end = valid_start + n_valid_i\n",
        "\n",
        "            test_start = valid_end\n",
        "            test_end = test_start + n_test_i\n",
        "\n",
        "            pretrain_start = test_end\n",
        "            pretrain_end = pretrain_start + n_pretrain_i\n",
        "\n",
        "            if reduced_train > 0:\n",
        "                train_end *= reduced_train\n",
        "                train_end = np.round(train_end)\n",
        "                if train_end < 1:\n",
        "                    train_end = 1\n",
        "                train_end = int(train_end)\n",
        "\n",
        "            data_train.append(data_i[train_start:train_end, :, :])\n",
        "            data_valid.append(data_i[valid_start:valid_end, :, :])\n",
        "            data_test.append(data_i[test_start:test_end, :, :])\n",
        "            data_pretrain.append(data_i[pretrain_start:pretrain_end, :, :])\n",
        "\n",
        "            label_train.append(label_i[train_start:train_end])\n",
        "            label_valid.append(label_i[valid_start:valid_end])\n",
        "            label_test.append(label_i[test_start:test_end])\n",
        "\n",
        "        data_train = np.concatenate(data_train, axis=0)\n",
        "        data_valid = np.concatenate(data_valid, axis=0)\n",
        "        data_test = np.concatenate(data_test, axis=0)\n",
        "        data_pretrain = np.concatenate(data_pretrain, axis=0)\n",
        "        label_train = np.concatenate(label_train, axis=0)\n",
        "        label_valid = np.concatenate(label_valid, axis=0)\n",
        "        label_test = np.concatenate(label_test, axis=0)\n",
        "\n",
        "    dataset_ = {}\n",
        "    dataset_['data_pretrain'] = data_pretrain\n",
        "    dataset_['data_train'] = data_train\n",
        "    dataset_['data_valid'] = data_valid\n",
        "    dataset_['data_test'] = data_test\n",
        "    dataset_['label_train'] = label_train\n",
        "    dataset_['label_valid'] = label_valid\n",
        "    dataset_['label_test'] = label_test\n",
        "    dataset_['n_class'] = n_class\n",
        "    dataset_['n_dim'] = data_pretrain.shape[1]\n",
        "    dataset_['data_len'] = data_pretrain.shape[2]\n",
        "    return dataset_\n",
        "\n",
        "\n",
        "def load_dataset(data_config):\n",
        "    data_names = get_ucr_data_names()\n",
        "    pretrain_data = []\n",
        "    max_len = 0\n",
        "    for data_name in data_names:\n",
        "        dataset = load_ucr_dataset(data_name, data_config)\n",
        "        data_pretrain = _normalize_dataset(dataset['data_pretrain'])\n",
        "        pretrain_data.append(data_pretrain)\n",
        "        if max_len < dataset['data_len']:\n",
        "            max_len = dataset['data_len']\n",
        "\n",
        "    for i in range(len(pretrain_data)):\n",
        "        data_len = pretrain_data[i].shape[2]\n",
        "        if data_len < max_len:\n",
        "            n_data = pretrain_data[i].shape[0]\n",
        "            data_i = np.zeros((n_data, 1, max_len))\n",
        "            data_i[:, :, :data_len] = pretrain_data[i]\n",
        "            pretrain_data[i] = data_i\n",
        "    pretrain_data = np.concatenate(pretrain_data, axis=0)\n",
        "    return pretrain_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crZgCh4rATFA"
      },
      "source": [
        "### Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-804csgSASYR"
      },
      "outputs": [],
      "source": [
        "def jittering(data, strength=0.1, seed=None):\n",
        "    is_matrix = len(data.shape) == 2\n",
        "    if not is_matrix:\n",
        "        data = np.expand_dims(data, 0)\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    data_len = data.shape[1]\n",
        "    sigma = np.std(data, axis=1, keepdims=True)\n",
        "\n",
        "    sigma_jitter = strength * sigma\n",
        "    sigma_jitter[sigma[:, 0] == 0, 0] = strength\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed=seed)\n",
        "    noise = np.random.randn(n_data, data_len) * sigma_jitter\n",
        "    data_aug = data + noise\n",
        "\n",
        "    if not is_matrix:\n",
        "        data_aug = data_aug[0, :]\n",
        "    return data_aug\n",
        "\n",
        "def add_offset(data, strength=1, seed=None):\n",
        "    is_matrix = len(data.shape) == 2\n",
        "    if not is_matrix:\n",
        "        data = np.expand_dims(data, 0)\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    data_len = data.shape[1]\n",
        "    sigma = np.std(data, axis=1, keepdims=True)\n",
        "\n",
        "    sigma_scaling = strength * sigma\n",
        "    sigma_scaling[sigma[:, 0] == 0, 0] = strength\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed=seed)\n",
        "    noise = np.random.randn(n_data, 1) * sigma_scaling\n",
        "    data_aug = data + noise\n",
        "\n",
        "    if not is_matrix:\n",
        "        data_aug = data_aug[0, :]\n",
        "    return data_aug\n",
        "\n",
        "def add_slope(data, strength=1, seed=None):\n",
        "    is_matrix = len(data.shape) == 2\n",
        "    if not is_matrix:\n",
        "        data = np.expand_dims(data, 0)\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    data_len = data.shape[1]\n",
        "    sigma = np.std(data, axis=1, keepdims=True)\n",
        "\n",
        "    sigma_scaling = strength * sigma\n",
        "    sigma_scaling[sigma[:, 0] == 0, 0] = strength\n",
        "\n",
        "    slope = np.arange(0, 1, step=1 / data_len)\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed=seed)\n",
        "    noise = np.random.randn(n_data, 1) * sigma_scaling * slope\n",
        "    noise = noise + np.random.randn(n_data, 1) * sigma_scaling\n",
        "    data_aug = data + noise\n",
        "\n",
        "    if not is_matrix:\n",
        "        data_aug = data_aug[0, :]\n",
        "    return data_aug\n",
        "\n",
        "def add_spike(data, strength=3, seed=None):\n",
        "    is_matrix = len(data.shape) == 2\n",
        "    if not is_matrix:\n",
        "        data = np.expand_dims(data, 0)\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    data_len = data.shape[1]\n",
        "    sigma = np.std(data, axis=1, keepdims=True)\n",
        "\n",
        "    sigma_scaling = strength * sigma\n",
        "    sigma_scaling[sigma[:, 0] == 0, 0] = strength\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed=seed)\n",
        "    location = np.random.randint(data_len, size=n_data)\n",
        "    noise = np.random.randn(n_data) * sigma_scaling\n",
        "\n",
        "    data_aug = copy.deepcopy(data)\n",
        "    for i in range(n_data):\n",
        "        data_aug[i, location[i]] += noise[i]\n",
        "\n",
        "    if not is_matrix:\n",
        "        data_aug = data_aug[0, :]\n",
        "    return data_aug\n",
        "\n",
        "def add_step(data, min_ratio=0.1, strength=1, seed=None):\n",
        "    is_matrix = len(data.shape) == 2\n",
        "    if not is_matrix:\n",
        "        data = np.expand_dims(data, 0)\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    data_len = data.shape[1]\n",
        "    sigma = np.std(data, axis=1, keepdims=True)\n",
        "\n",
        "    sigma_scaling = strength * sigma\n",
        "    sigma_scaling[sigma[:, 0] == 0, 0] = strength\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed=seed)\n",
        "    segment_len = np.random.rand(n_data)\n",
        "    segment_len = segment_len * (1 - min_ratio) + min_ratio\n",
        "    segment_len = segment_len * data_len\n",
        "    segment_start = np.random.rand(n_data)\n",
        "    segment_start = segment_start * (data_len - segment_len)\n",
        "    segment_end = segment_start + segment_len\n",
        "\n",
        "    segment_start = _cleanup(segment_start, 0, data_len)\n",
        "    segment_end = _cleanup(segment_end, 0, data_len)\n",
        "\n",
        "    data_aug = copy.deepcopy(data)\n",
        "    for i in range(n_data):\n",
        "        data_aug[i, segment_start[i]:segment_end[i]] += sigma_scaling[i]\n",
        "\n",
        "    if not is_matrix:\n",
        "        data_aug = data_aug[0, :]\n",
        "    return data_aug\n",
        "\n",
        "def cropping(data, min_ratio=0.1, seed=None):\n",
        "    is_matrix = len(data.shape) == 2\n",
        "    if not is_matrix:\n",
        "        data = np.expand_dims(data, 0)\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    data_len = data.shape[1]\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed=seed)\n",
        "    segment_len = np.random.rand(n_data)\n",
        "    segment_len = segment_len * (1 - min_ratio) + min_ratio\n",
        "    segment_len = segment_len * data_len\n",
        "    segment_start = np.random.rand(n_data)\n",
        "    segment_start = segment_start * (data_len - segment_len)\n",
        "    segment_end = segment_start + segment_len\n",
        "\n",
        "    segment_start = _cleanup(segment_start, 0, data_len)\n",
        "    segment_end = _cleanup(segment_end, 0, data_len)\n",
        "    data_aug = np.zeros((n_data, data_len))\n",
        "    for i in range(n_data):\n",
        "        segment = data[i, segment_start[i]:segment_end[i]]\n",
        "        data_aug[i, :] = signal.resample(segment, data_len)\n",
        "\n",
        "    if not is_matrix:\n",
        "        data_aug = data_aug[0, :]\n",
        "    return data_aug\n",
        "\n",
        "def flipping(data):\n",
        "    is_matrix = len(data.shape) == 2\n",
        "    if not is_matrix:\n",
        "        data = np.expand_dims(data, 0)\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    mu = np.mean(data, axis=1, keepdims=True)\n",
        "\n",
        "    data_aug = 2 * mu - data\n",
        "\n",
        "    if not is_matrix:\n",
        "        data_aug = data_aug[0, :]\n",
        "    return data_aug\n",
        "\n",
        "def inverting(data):\n",
        "    is_matrix = len(data.shape) == 2\n",
        "    if not is_matrix:\n",
        "        data = np.expand_dims(data, 0)\n",
        "\n",
        "    data_aug = copy.deepcopy(data)\n",
        "    data_aug = np.flip(data_aug, axis=1)\n",
        "\n",
        "    if not is_matrix:\n",
        "        data_aug = data_aug[0, :]\n",
        "    return data_aug\n",
        "\n",
        "def _apply_warping(data, n_knot, strength, seed):\n",
        "    np.random.seed(seed=seed)\n",
        "    data_len = data.shape[0]\n",
        "    knot_step = data_len / (n_knot - 1)\n",
        "    # knot_t = np.arange(knot_step, data_len, knot_step)\n",
        "    knot_t = np.arange(0, data_len + knot_step, knot_step)\n",
        "    knot_mag = np.random.randn(n_knot) * strength + 1\n",
        "    data_aug_t = np.arange(data_len)\n",
        "\n",
        "    if knot_t.shape[0] != knot_mag.shape[0]:\n",
        "        knot_t = knot_t[:knot_mag.shape[0]]\n",
        "    # data_aug = data + CubicSpline(knot_t, knot_mag)(data_aug_t)\n",
        "    data_aug = data * CubicSpline(knot_t, knot_mag)(data_aug_t)\n",
        "    return data_aug\n",
        "\n",
        "\n",
        "def mag_warping(data, strength=1, seed=None):\n",
        "    is_matrix = len(data.shape) == 2\n",
        "    if not is_matrix:\n",
        "        data = np.expand_dims(data, 0)\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    data_len = data.shape[1]\n",
        "    sigma = np.std(data, axis=1, keepdims=False)\n",
        "\n",
        "    sigma_scaling = strength * sigma\n",
        "    sigma_scaling[sigma == 0] = strength\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed=seed)\n",
        "    n_knot = np.random.randint(3, high=data_len, size=n_data)\n",
        "    seeds = np.random.randint(2 ** 32 - 1, size=n_data)\n",
        "\n",
        "    data_aug = np.zeros((n_data, data_len))\n",
        "    for i in range(n_data):\n",
        "        data_aug[i, :] = _apply_warping(\n",
        "            data[i, :], n_knot[i], sigma_scaling[i], seeds[i])\n",
        "\n",
        "    if not is_matrix:\n",
        "        data_aug = data_aug[0, :]\n",
        "    return data_aug\n",
        "\n",
        "def masking(data, max_ratio=0.5, seed=None):\n",
        "    is_matrix = len(data.shape) == 2\n",
        "    if not is_matrix:\n",
        "        data = np.expand_dims(data, 0)\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    data_len = data.shape[1]\n",
        "    mu = np.mean(data, axis=1, keepdims=False)\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed=seed)\n",
        "    segment_len = np.random.rand(n_data)\n",
        "    segment_len = segment_len * max_ratio * data_len\n",
        "    segment_start = np.random.rand(n_data)\n",
        "    segment_start = segment_start * (data_len - segment_len)\n",
        "    segment_end = segment_start + segment_len\n",
        "\n",
        "    segment_start = _cleanup(segment_start, 0, data_len)\n",
        "    segment_end = _cleanup(segment_end, 0, data_len)\n",
        "    data_aug = copy.deepcopy(data)\n",
        "    for i in range(n_data):\n",
        "        data_aug[i, segment_start[i]:segment_end[i]] = mu[i]\n",
        "\n",
        "    if not is_matrix:\n",
        "        data_aug = data_aug[0, :]\n",
        "    return data_aug\n",
        "\n",
        "def _cleanup(data, value_min, value_max):\n",
        "    data = np.round(data)\n",
        "    data[data < value_min] = value_min\n",
        "    data[data > value_max] = value_max\n",
        "    data = data.astype(int)\n",
        "    return data\n",
        "\n",
        "def scaling(data, strength=1, seed=None):\n",
        "    is_matrix = len(data.shape) == 2\n",
        "    if not is_matrix:\n",
        "        data = np.expand_dims(data, 0)\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    data_len = data.shape[1]\n",
        "    sigma = np.std(data, axis=1, keepdims=True)\n",
        "    mu = np.mean(data, axis=1, keepdims=True)\n",
        "\n",
        "    sigma_scaling = strength * sigma\n",
        "    sigma_scaling[sigma[:, 0] == 0, 0] = strength\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed=seed)\n",
        "    noise = np.random.randn(n_data, 1) * sigma_scaling + 1\n",
        "    data_aug = (data - mu) * noise + mu\n",
        "\n",
        "    if not is_matrix:\n",
        "        data_aug = data_aug[0, :]\n",
        "    return data_aug\n",
        "\n",
        "def shifting(data, seed=None):\n",
        "    is_matrix = len(data.shape) == 2\n",
        "    if not is_matrix:\n",
        "        data = np.expand_dims(data, 0)\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    data_len = data.shape[1]\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed=seed)\n",
        "    shift_len = np.random.randn(n_data) * data_len\n",
        "    shift_len = np.round(shift_len)\n",
        "    shift_len = shift_len.astype(int)\n",
        "\n",
        "    data_aug = copy.deepcopy(data)\n",
        "    for i in range(n_data):\n",
        "        data_aug[i, :] = np.roll(data_aug[i, :], shift_len[i])\n",
        "\n",
        "    if not is_matrix:\n",
        "        data_aug = data_aug[0, :]\n",
        "    return data_aug\n",
        "\n",
        "def smoothing(data, max_ratio=0.5, min_ratio=0.01, seed=None):\n",
        "    is_matrix = len(data.shape) == 2\n",
        "    if not is_matrix:\n",
        "        data = np.expand_dims(data, 0)\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    data_len = data.shape[1]\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed=seed)\n",
        "    ratio = np.random.rand(n_data) * (max_ratio - min_ratio) + min_ratio\n",
        "    data_len_down = np.ceil(data_len * ratio).astype(int)\n",
        "\n",
        "    data_aug = np.zeros((n_data, data_len))\n",
        "    for i in range(n_data):\n",
        "        data_aug_ = signal.resample(data[i, :], data_len_down[i])\n",
        "        data_aug[i, :] = signal.resample(data_aug_, data_len)\n",
        "\n",
        "    if not is_matrix:\n",
        "        data_aug = data_aug[0, :]\n",
        "    return data_aug\n",
        "\n",
        "def time_warping(data, min_ratio=0.5, seed=None):\n",
        "    is_matrix = len(data.shape) == 2\n",
        "    if not is_matrix:\n",
        "        data = np.expand_dims(data, 0)\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    data_len = data.shape[1]\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed=seed)\n",
        "    ratio = (1 - min_ratio) * np.random.rand(n_data) + min_ratio\n",
        "    wrap_len = np.round(data_len * ratio)\n",
        "    wrap_len[wrap_len > data_len] = data_len\n",
        "    wrap_len = wrap_len.astype(int)\n",
        "\n",
        "    data_aug = np.zeros((n_data, data_len))\n",
        "    for i in range(n_data):\n",
        "        random_vec = np.random.permutation(data_len)\n",
        "        random_vec = random_vec[:wrap_len[i]]\n",
        "        random_vec = np.sort(random_vec)\n",
        "        data_aug_ = data[i, random_vec]\n",
        "        data_aug[i, :] = signal.resample(data_aug_, data_len)\n",
        "\n",
        "    if not is_matrix:\n",
        "        data_aug = data_aug[0, :]\n",
        "    return data_aug\n",
        "\n",
        "def _normalize_t(t_, normalize):\n",
        "    if not torch.is_tensor(t_):\n",
        "        t_ = torch.from_numpy(t_)\n",
        "    if len(t_.size()) == 1:\n",
        "        t_ = torch.unsqueeze(t_, 0)\n",
        "    if len(t_.size()) == 2:\n",
        "        t_ = torch.unsqueeze(t_, 1)\n",
        "    if normalize:\n",
        "        t_mu = np.mean(t_, axis=2, keepdims=True)\n",
        "        t_sigma = np.std(t_, axis=2, keepdims=True)\n",
        "        t_sigma[t_sigma <= 0] = 1.0\n",
        "        t_ = (t_ - t_mu) / t_sigma\n",
        "    return t_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6tqqoplwNRo"
      },
      "source": [
        "# Model Stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgsN2EGw_3dM"
      },
      "source": [
        "### Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWAZdH0PyMNj"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, encoder, n_class, n_dim=64, n_layer=2):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.add_module('encoder', encoder)\n",
        "\n",
        "        in_dim_ = self.encoder.out_dim\n",
        "        out_dim_ = n_dim\n",
        "        layers = OrderedDict()\n",
        "        for i in range(n_layer - 1):\n",
        "            layers[f'linear_{i:02d}'] = nn.Linear(\n",
        "                in_dim_, out_dim_)\n",
        "            layers[f'relu_{i:02d}'] = nn.ReLU()\n",
        "            in_dim_ = out_dim_\n",
        "            out_dim_ = n_dim\n",
        "\n",
        "        layers[f'linear_{n_layer - 1:02d}'] = nn.Linear(\n",
        "            in_dim_, n_class)\n",
        "        self.classifier = nn.Sequential(layers)\n",
        "\n",
        "    def forward(self, ts, normalize=True, to_numpy=False):\n",
        "        hidden = self.encoder.encode(\n",
        "            ts, normalize=normalize, to_numpy=False)\n",
        "        logit = self.classifier(hidden)\n",
        "        if to_numpy:\n",
        "            return logit.cpu().detach().numpy()\n",
        "        else:\n",
        "            return logit\n",
        "\n",
        "\n",
        "def get_classifier(model_config, encoder):\n",
        "    n_class = int(model_config['classifier']['n_class'])\n",
        "    n_dim = int(model_config['classifier']['n_dim'])\n",
        "    n_layer = int(model_config['classifier']['n_layer'])\n",
        "    model = Classifier(\n",
        "        encoder, n_class, n_dim=n_dim, n_layer=n_layer)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7caGl-J-B1h"
      },
      "source": [
        "### TimeFreq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BX76hEtm-Bja"
      },
      "outputs": [],
      "source": [
        "class TimeFreqEncoder(nn.Module):\n",
        "    def __init__(self, encoder,\n",
        "                 jitter_strength=0.1, freq_ratio=0.1,\n",
        "                 freq_strength=0.1, project_norm=None):\n",
        "        r\"\"\"\n",
        "        The TF-C model described in the paper 'Self-Supervised Contrastive\n",
        "        Pre-Training For Time Series via Time-Frequency Consistency'. The\n",
        "        implementation is abased on the authors' github repository\n",
        "        https://github.com/mims-harvard/TFC-pretraining\n",
        "\n",
        "        Args:\n",
        "            encoder (Module): The base encoder for both time/frequency-based\n",
        "                contrastive learning\n",
        "            jitter_strength (float, optional): the strength of jitter added\n",
        "                when creating augmented time series in time domain.\n",
        "            freq_ratio (float, optional): ratio of perturbed frequencies for\n",
        "                frequency removal/amplification when creating augmented time\n",
        "                series in frequency domain.\n",
        "            freq_strength (float, optional): strength of frequency\n",
        "                amplification  when creating augmented time series in\n",
        "                frequency domain.\n",
        "            project_norm (string, optional): If set to ``BN``, the projector\n",
        "                will use batch normalization. If set to ``LN``, the projector\n",
        "                will use layer normalization. If set to None, the projector\n",
        "                will not use normalization. Default: None (no normalization).\n",
        "\n",
        "        Shape:\n",
        "            - Input: :math:`(N, C_{in}, L_{in})`.\n",
        "            - Output: :math:`(N, C_{out})`.\n",
        "        \"\"\"\n",
        "        super(TimeFreqEncoder, self).__init__()\n",
        "        assert project_norm in ['BN', 'LN', None]\n",
        "\n",
        "        self.pretrain_name = 'timefreq'\n",
        "        self.encoder_t = copy.deepcopy(encoder)\n",
        "        self.encoder_f = copy.deepcopy(encoder)\n",
        "        self.add_module('encoder_t', self.encoder_t)\n",
        "        self.add_module('encoder_f', self.encoder_f)\n",
        "\n",
        "        self.jitter_strength = jitter_strength\n",
        "        self.freq_ratio = freq_ratio\n",
        "        self.freq_strength = freq_strength\n",
        "\n",
        "        out_dim_t = self.encoder_t.out_dim\n",
        "        out_dim_f = self.encoder_f.out_dim\n",
        "        self.out_dim = out_dim_t + out_dim_f\n",
        "\n",
        "        if project_norm == 'BN':\n",
        "            self.projector_t = nn.Sequential(\n",
        "                nn.BatchNorm1d(out_dim_t),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(out_dim_t, out_dim_t * 2),\n",
        "                nn.BatchNorm1d(out_dim_t * 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(out_dim_t * 2, out_dim_t)\n",
        "            )\n",
        "        elif project_norm == 'LN':\n",
        "            self.projector_t = nn.Sequential(\n",
        "                nn.ReLU(),\n",
        "                nn.LayerNorm(out_dim_t),\n",
        "                nn.Linear(out_dim_t, out_dim_t * 2),\n",
        "                nn.ReLU(),\n",
        "                nn.LayerNorm(out_dim_t * 2),\n",
        "                nn.Linear(out_dim_t * 2, out_dim_t)\n",
        "            )\n",
        "        else:\n",
        "            self.projector_t = nn.Sequential(\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(out_dim_t, out_dim_t * 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(out_dim_t * 2, out_dim_t)\n",
        "            )\n",
        "        self.add_module('projector_t', self.projector_t)\n",
        "\n",
        "        if project_norm == 'BN':\n",
        "            self.projector_f = nn.Sequential(\n",
        "                nn.BatchNorm1d(out_dim_f),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(out_dim_f, out_dim_f * 2),\n",
        "                nn.BatchNorm1d(out_dim_f * 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(out_dim_f * 2, out_dim_f)\n",
        "            )\n",
        "        elif project_norm == 'LN':\n",
        "            self.projector_f = nn.Sequential(\n",
        "                nn.ReLU(),\n",
        "                nn.LayerNorm(out_dim_f),\n",
        "                nn.Linear(out_dim_f, out_dim_f * 2),\n",
        "                nn.ReLU(),\n",
        "                nn.LayerNorm(out_dim_f * 2),\n",
        "                nn.Linear(out_dim_f * 2, out_dim_f)\n",
        "            )\n",
        "        else:\n",
        "            self.projector_f = nn.Sequential(\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(out_dim_f, out_dim_f * 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(out_dim_f * 2, out_dim_f)\n",
        "            )\n",
        "        self.add_module('projector_f', self.projector_f)\n",
        "        self.dummy = nn.Parameter(torch.empty(0))\n",
        "\n",
        "    def forward(self, ts, normalize=True, to_numpy=False, is_augment=False):\n",
        "        ts_t = _normalize_t(ts, normalize)\n",
        "        n_dim = ts_t.shape[1]\n",
        "        ts_f = np.fft.fft(ts_t, axis=2)\n",
        "        ts_f = np.abs(ts_f)\n",
        "\n",
        "        if is_augment:\n",
        "            jitter_strength = self.jitter_strength\n",
        "            for i in range(n_dim):\n",
        "                ts_t[:, i, :] = jittering(\n",
        "                    ts_t[:, i, :], strength=jitter_strength,\n",
        "                    seed=None)\n",
        "\n",
        "            freq_ratio = self.freq_ratio\n",
        "            freq_strength = self.freq_strength\n",
        "            for i in range(n_dim):\n",
        "                ts_f[:, i, :] = _freq_perturb(\n",
        "                    ts_f[:, i, :], ratio=freq_ratio, strength=freq_strength,\n",
        "                    seed=None)\n",
        "\n",
        "        h_t = self.encoder_t.encode(\n",
        "            ts_t, normalize=False, to_numpy=False)\n",
        "        z_t = self.projector_t(h_t)\n",
        "\n",
        "        h_f = self.encoder_f.encode(\n",
        "            ts_f, normalize=False, to_numpy=False)\n",
        "        z_f = self.projector_f(h_f)\n",
        "\n",
        "        if to_numpy:\n",
        "            h_t = h_t.cpu().detach().numpy()\n",
        "            z_t = z_t.cpu().detach().numpy()\n",
        "            h_f = h_f.cpu().detach().numpy()\n",
        "            z_f = z_f.cpu().detach().numpy()\n",
        "        return h_t, z_t, h_f, z_f\n",
        "\n",
        "    def encode(self, ts, normalize=True, to_numpy=False):\n",
        "        _, z_t, _, z_f = self.forward(\n",
        "            ts, normalize=normalize, to_numpy=False, is_augment=False)\n",
        "        feature = torch.cat((z_t, z_f), dim=1)\n",
        "        if to_numpy:\n",
        "            return feature.cpu().detach().numpy()\n",
        "        else:\n",
        "            return feature\n",
        "\n",
        "def _freq_perturb(data, ratio=0.1, strength=0.1, seed=None):\n",
        "    n_data = data.shape[0]\n",
        "    data_len = data.shape[1]\n",
        "\n",
        "    data_aug = copy.deepcopy(data)\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed=seed)\n",
        "    if ratio < 1:\n",
        "        mask_remove = np.random.rand(n_data, data_len)\n",
        "        mask_remove = mask_remove < ratio\n",
        "        data_aug[mask_remove] = 0.0\n",
        "\n",
        "        mask_perturb = np.random.rand(n_data, data_len)\n",
        "        mask_perturb = mask_perturb < ratio\n",
        "\n",
        "    sigma = np.std(data, axis=1, keepdims=True)\n",
        "    sigma_scaling = strength * sigma\n",
        "    sigma_scaling[sigma == 0] = strength\n",
        "\n",
        "    noise = np.random.rand(n_data, data_len) * sigma_scaling\n",
        "    if ratio < 1:\n",
        "        data_aug[mask_perturb] = data_aug[mask_perturb] + noise[mask_perturb]\n",
        "    else:\n",
        "        data_aug = data_aug + noise\n",
        "    return data_aug\n",
        "\n",
        "def get_timefreq(model_config, encoder):\n",
        "    jitter_strength = float(model_config['timefreq']['jitter_strength'])\n",
        "    freq_ratio = float(model_config['timefreq']['freq_ratio'])\n",
        "    freq_strength = float(model_config['timefreq']['freq_strength'])\n",
        "    project_norm = model_config['timefreq']['project_norm']\n",
        "    encoder_ = TimeFreqEncoder(\n",
        "        encoder, jitter_strength=jitter_strength, freq_ratio=freq_ratio,\n",
        "        freq_strength=freq_strength, project_norm=project_norm)\n",
        "    encoder_ = load_pretrain(model_config['timefreq'], encoder_)\n",
        "    return encoder_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSvrdUhC95tO"
      },
      "source": [
        "### MixingUp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNIkrF6v95eI"
      },
      "outputs": [],
      "source": [
        "class MixingUpEncoder(nn.Module):\n",
        "    def __init__(self, encoder, alpha=1.0):\n",
        "        r\"\"\"\n",
        "        The MixingUp model described in the paper 'Self-Supervised Representation\n",
        "        Learning for Time Series '. The implementation is abased on the\n",
        "        github repository https://github.com/mims-harvard/TFC-pretraining\n",
        "\n",
        "        Args:\n",
        "            encoder (Module): The base encoder\n",
        "            alpha (float, optional): the alpha for beta distribution.\n",
        "                Default: 1.0.\n",
        "\n",
        "        Shape:\n",
        "            - Input: :math:`(N, C_{in}, L_{in})`.\n",
        "            - Output: :math:`(N, C_{out})`.\n",
        "        \"\"\"\n",
        "        super(MixingUpEncoder, self).__init__()\n",
        "\n",
        "        self.pretrain_name = 'mixup'\n",
        "        self.encoder = copy.deepcopy(encoder)\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.out_dim = self.encoder.out_dim\n",
        "        self.dummy = nn.Parameter(torch.empty(0))\n",
        "\n",
        "    def forward(self, ts, normalize=True, to_numpy=False, is_augment=False):\n",
        "        if not is_augment:\n",
        "            ts_emb = self.encoder.encode(\n",
        "                ts, normalize=normalize, to_numpy=to_numpy)\n",
        "            return ts_emb\n",
        "\n",
        "        alpha = self.alpha\n",
        "\n",
        "        n_ts = ts.shape[0]\n",
        "        ts_0 = copy.deepcopy(ts)\n",
        "        ts_1 = copy.deepcopy(ts)\n",
        "\n",
        "        order = np.random.permutation(n_ts)\n",
        "        ts_1 = ts_1[order, :, :]\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "\n",
        "        ts_aug = lam * ts_0 + (1 - lam) * ts_1\n",
        "\n",
        "        ts_emb_0 = self.encoder.encode(\n",
        "            ts_0, normalize=normalize, to_numpy=to_numpy)\n",
        "        ts_emb_1 = self.encoder.encode(\n",
        "            ts_1, normalize=normalize, to_numpy=to_numpy)\n",
        "        ts_emb_aug = self.encoder.encode(\n",
        "            ts_aug, normalize=normalize, to_numpy=to_numpy)\n",
        "\n",
        "        if to_numpy:\n",
        "            ts_emb_0 = ts_emb_0.cpu().detach().numpy()\n",
        "            ts_emb_1 = ts_emb_1.cpu().detach().numpy()\n",
        "            ts_emb_aug = ts_emb_aug.cpu().detach().numpy()\n",
        "        return ts_emb_0, ts_emb_1, ts_emb_aug, lam\n",
        "\n",
        "    def encode(self, ts, normalize=True, to_numpy=False):\n",
        "        ts_emb = self.encoder.encode(\n",
        "            ts, normalize=normalize, to_numpy=to_numpy)\n",
        "        return ts_emb\n",
        "\n",
        "def get_mixup(model_config, encoder):\n",
        "    encoder_ = MixingUpEncoder(encoder)\n",
        "    encoder_ = load_pretrain(model_config['mixup'], encoder_)\n",
        "    return encoder_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6wYhLx79xdD"
      },
      "source": [
        "### SimCLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHCwAu6f9xJ0"
      },
      "outputs": [],
      "source": [
        "class SimCLREncoder(nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        r\"\"\"\n",
        "        The SimCLR model described in the paper 'Exploring Contrastive\n",
        "        Learning in Human Activity Recognition for Healthcare'. The\n",
        "        implementation is abased on the github repository\n",
        "        https://github.com/mims-harvard/TFC-pretraining\n",
        "\n",
        "        Args:\n",
        "            encoder (Module): The base encoder\n",
        "\n",
        "        Shape:\n",
        "            - Input: :math:`(N, C_{in}, L_{in})`.\n",
        "            - Output: :math:`(N, C_{out})`.\n",
        "        \"\"\"\n",
        "        super(SimCLREncoder, self).__init__()\n",
        "\n",
        "        self.pretrain_name = 'simclr'\n",
        "        self.encoder = copy.deepcopy(encoder)\n",
        "\n",
        "        self.out_dim = self.encoder.out_dim\n",
        "        self.dummy = nn.Parameter(torch.empty(0))\n",
        "\n",
        "    def forward(self, ts, normalize=True, to_numpy=False, is_augment=False):\n",
        "        if not is_augment:\n",
        "            ts_emb = self.encoder.encode(\n",
        "                ts, normalize=normalize, to_numpy=to_numpy)\n",
        "            return ts_emb\n",
        "\n",
        "        ts_aug = _augment_ts(ts)\n",
        "        ts_emb_aug = self.encoder.encode(\n",
        "            ts, normalize=normalize, to_numpy=to_numpy)\n",
        "        return ts_emb_aug\n",
        "\n",
        "    def encode(self, ts, normalize=True, to_numpy=False):\n",
        "        ts_emb = self.encoder.encode(\n",
        "            ts, normalize=normalize, to_numpy=to_numpy)\n",
        "        return ts_emb\n",
        "\n",
        "\n",
        "def _augment_ts(ts):\n",
        "    ts_aug = copy.deepcopy(ts)\n",
        "    ts_aug = _scaling_transform_vectorized(ts_aug)\n",
        "    ts_aug = _negate_transform_vectorized(ts_aug)\n",
        "    return ts_aug\n",
        "\n",
        "\n",
        "def _scaling_transform_vectorized(X, sigma=0.1):\n",
        "    \"\"\"\n",
        "    Scaling by a random factor\n",
        "    \"\"\"\n",
        "    scaling_factor = np.random.normal(\n",
        "        loc=1.0, scale=sigma, size=(X.shape[0], 1, X.shape[2]))\n",
        "    return X * scaling_factor\n",
        "\n",
        "\n",
        "def _negate_transform_vectorized(X):\n",
        "    \"\"\"\n",
        "    Inverting the signals\n",
        "    \"\"\"\n",
        "    return X * -1\n",
        "\n",
        "def get_simclr(model_config, encoder):\n",
        "    encoder_ = SimCLREncoder(encoder)\n",
        "    encoder_ = load_pretrain(model_config['simclr'], encoder_)\n",
        "    return encoder_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOOZ89uJ9tQU"
      },
      "source": [
        "### TimeCLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7psBWOc9sc0"
      },
      "outputs": [],
      "source": [
        "class TimeCLREncoder(nn.Module):\n",
        "    def __init__(self, encoder, aug_bank):\n",
        "        r\"\"\"\n",
        "        The proposed TimeCLR method\n",
        "\n",
        "        Args:\n",
        "            encoder (Module): The base encoder\n",
        "            aug_bank (list): A list of augmentation methods.\n",
        "\n",
        "        Shape:\n",
        "            - Input: :math:`(N, C_{in}, L_{in})`.\n",
        "            - Output: :math:`(N, C_{out})`.\n",
        "        \"\"\"\n",
        "        super(TimeCLREncoder, self).__init__()\n",
        "\n",
        "        self.pretrain_name = 'timeclr'\n",
        "        self.encoder = copy.deepcopy(encoder)\n",
        "\n",
        "        self.aug_bank = aug_bank\n",
        "        n_aug = len(aug_bank)\n",
        "        self.n_aug = n_aug\n",
        "\n",
        "        self.out_dim = self.encoder.out_dim\n",
        "        self.dummy = nn.Parameter(torch.empty(0))\n",
        "\n",
        "    def forward(self, ts, normalize=True, to_numpy=False, is_augment=False):\n",
        "        if is_augment:\n",
        "            ts = self._augment_ts(ts)\n",
        "\n",
        "        ts_emb = self.encoder.encode(\n",
        "            ts, normalize=normalize, to_numpy=to_numpy)\n",
        "        return ts_emb\n",
        "\n",
        "    def encode(self, ts, normalize=True, to_numpy=False):\n",
        "        ts_emb = self.encoder.encode(\n",
        "            ts, normalize=normalize, to_numpy=to_numpy)\n",
        "        return ts_emb\n",
        "\n",
        "    def _augment_ts(self, ts):\n",
        "        n_ts = ts.shape[0]\n",
        "        n_aug = self.n_aug\n",
        "        ts_aug = copy.deepcopy(ts)\n",
        "        aug_bank = self.aug_bank\n",
        "        for i in range(n_ts):\n",
        "            aug_idx = np.random.randint(n_aug)\n",
        "            ts_aug[i, 0, :] = aug_bank[aug_idx](ts_aug[i, 0, :])\n",
        "        return ts_aug\n",
        "\n",
        "def get_timeclr(model_config, encoder):\n",
        "    aug_bank_ver = int(model_config['timeclr']['aug_bank_ver'])\n",
        "    if aug_bank_ver == 0:\n",
        "        aug_bank = [\n",
        "            lambda x:jittering(x, strength=0.1, seed=None),\n",
        "            lambda x:smoothing(x, max_ratio=0.5, min_ratio=0.01, seed=None),\n",
        "            lambda x:mag_warping(x, strength=1, seed=None),\n",
        "            lambda x:add_slope(x, strength=1, seed=None),\n",
        "            lambda x:add_spike(x, strength=3, seed=None),\n",
        "            lambda x:add_step(x, min_ratio=0.1, strength=1, seed=None),\n",
        "            lambda x:cropping(x, min_ratio=0.1, seed=None),\n",
        "            lambda x:masking(x, max_ratio=0.5, seed=None),\n",
        "            lambda x:shifting(x, seed=None),\n",
        "            lambda x:time_warping(x, min_ratio=0.5, seed=None),\n",
        "        ]\n",
        "\n",
        "    encoder_ = TimeCLREncoder(encoder, aug_bank)\n",
        "    encoder_ = load_pretrain(model_config['timeclr'], encoder_)\n",
        "    return encoder_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVAfib1w9kvr"
      },
      "source": [
        "### TS2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKJClB1twdIS"
      },
      "outputs": [],
      "source": [
        "class TS2VecEncoder(nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        r\"\"\"\n",
        "        The TS2Vec model described in the paper 'TS2Vec: Towards Universal\n",
        "        Representation of Time Series'. The implementation is abased on the\n",
        "        github repository https://github.com/mims-harvard/TFC-pretraining\n",
        "\n",
        "        Args:\n",
        "            encoder (Module): The base encoder\n",
        "\n",
        "        Shape:\n",
        "            - Input: :math:`(N, C_{in}, L_{in})`.\n",
        "            - Output: :math:`(N, C_{out})`.\n",
        "        \"\"\"\n",
        "        super(TS2VecEncoder, self).__init__()\n",
        "\n",
        "        self.pretrain_name = 'ts2vec'\n",
        "        self.encoder = copy.deepcopy(encoder)\n",
        "\n",
        "        self.out_dim = self.encoder.out_dim\n",
        "        self.dummy = nn.Parameter(torch.empty(0))\n",
        "\n",
        "    def forward(self, ts, normalize=True, to_numpy=False, is_augment=False):\n",
        "        if not is_augment:\n",
        "            ts_emb = self.encoder.encode_seq(\n",
        "                ts, normalize=normalize, to_numpy=False)\n",
        "            ts_emb = nn.AdaptiveMaxPool1d(1)(ts_emb)\n",
        "            ts_emb = ts_emb[:, :, 0]\n",
        "            if to_numpy:\n",
        "                ts_emb = ts_emb.cpu().detach().numpy()\n",
        "            return ts_emb\n",
        "\n",
        "        n_ts = ts.shape[0]\n",
        "        ts_len = ts.shape[2]\n",
        "        corp_len = np.random.randint(low=4, high=ts_len - 3)\n",
        "        crop_r_start = np.random.randint(\n",
        "            low=4, high=ts_len - corp_len + 1,\n",
        "            size=n_ts)\n",
        "\n",
        "        low_val = crop_r_start - corp_len + 1\n",
        "        low_val[low_val < 0] = 0\n",
        "        crop_l_start = np.random.randint(\n",
        "            low=low_val, high=crop_r_start,\n",
        "            size=n_ts)\n",
        "\n",
        "        corp_len = int(corp_len)\n",
        "        crop_l_start = crop_l_start.astype(int)\n",
        "        crop_r_start = crop_r_start.astype(int)\n",
        "\n",
        "        ts_l = _get_corp(ts, crop_l_start, corp_len)\n",
        "        ts_r = _get_corp(ts, crop_r_start, corp_len)\n",
        "        ts_emb_l = self.encoder.encode_seq(\n",
        "            ts_l, normalize=normalize, to_numpy=False)\n",
        "        ts_emb_r = self.encoder.encode_seq(\n",
        "            ts_r, normalize=normalize, to_numpy=False)\n",
        "        if to_numpy:\n",
        "            ts_emb_l = ts_emb_l.cpu().detach().numpy()\n",
        "            ts_emb_r = ts_emb_r.cpu().detach().numpy()\n",
        "        return ts_emb_l, ts_emb_r\n",
        "\n",
        "    def encode(self, ts, normalize=True, to_numpy=False):\n",
        "        ts_emb = self.forward(\n",
        "            ts, normalize=normalize, to_numpy=to_numpy, is_augment=False)\n",
        "        return ts_emb\n",
        "\n",
        "\n",
        "def _get_corp(ts, corp_start, corp_len):\n",
        "    n_ts = ts.shape[0]\n",
        "    n_dim = ts.shape[1]\n",
        "    corp = np.zeros((n_ts, n_dim, corp_len))\n",
        "    for i in range(n_ts):\n",
        "        corp[i, :, :] = ts[i, :, corp_start[i]:corp_start[i] + corp_len]\n",
        "    return corp\n",
        "\n",
        "def get_ts2vec(model_config, encoder):\n",
        "    encoder_ = TS2VecEncoder(encoder)\n",
        "    encoder_ = load_pretrain(model_config['ts2vec'], encoder_)\n",
        "    return encoder_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUG4bA6f8J9M"
      },
      "source": [
        "### ResNet 1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VhBfOef8Jf_"
      },
      "outputs": [],
      "source": [
        "class ResNet1D(nn.Module):\n",
        "    def __init__(self, in_dim=1, out_dim=128, n_dim=64,\n",
        "                 block_type='standard', norm=None,\n",
        "                 is_projector=True, project_norm=None):\n",
        "        r\"\"\"\n",
        "        1D ResNet-based time series encoder\n",
        "\n",
        "        Args:\n",
        "            in_dim (int, optional): Number of dimension for the input time\n",
        "                series. Default: 1.\n",
        "            out_dim (int, optional): Number of dimension for the output\n",
        "                representation. Default: 128.\n",
        "            n_dim (int, optional): Number of base dimension for the\n",
        "                intermediate representation. Default: 64.\n",
        "            block_type (string, optional): If set to ``standard``, the encoder\n",
        "                will use the standard residual block for 1D ResNet. If set to\n",
        "                ``alternative``, the encoder will use the alternative residual\n",
        "                block inspired by the paper 'On Layer Normalization in the\n",
        "                Transformer Architecture'. Default: ``standard``. See 'Deep\n",
        "                learning for time series classification: a review' for the\n",
        "                details.\n",
        "            norm (string, optional): If set to ``BN``, the encoder will\n",
        "                use batch normalization. If set to ``LN``, the encoder will\n",
        "                use layer normalization. If set to None, the encoder will\n",
        "                not use normalization. Default: None (no normalization).\n",
        "            is_projector (bool, optional): If set to ``False``, the encoder\n",
        "                will not use additional projection layers. Default: ``True``.\n",
        "            project_norm (string, optional): If set to ``BN``, the projector\n",
        "                will use batch normalization. If set to ``LN``, the projector\n",
        "                will use layer normalization. If set to None, the projector\n",
        "                will not use normalization. Default: None (no normalization).\n",
        "\n",
        "        Shape:\n",
        "            - Input: :math:`(N, C_{in}, L_{in})`, :math:`(N, L_{in})`, or\n",
        "                :math:`(L_{in})`.\n",
        "            - Output: :math:`(N, C_{out})`.\n",
        "        \"\"\"\n",
        "        super(ResNet1D, self).__init__()\n",
        "        assert block_type in ['standard', 'alternative', ]\n",
        "        assert norm in ['BN', 'LN', None]\n",
        "        assert project_norm in ['BN', 'LN', None]\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.n_dim = n_dim\n",
        "        self.is_projector = is_projector\n",
        "\n",
        "        if block_type == 'standard':\n",
        "            Block = Block_Standard\n",
        "        elif block_type == 'alternative':\n",
        "            Block = Block_Alt\n",
        "\n",
        "        self.in_net = nn.Conv1d(\n",
        "            in_dim, n_dim, 7, stride=2, padding=3, dilation=1)\n",
        "        self.add_module('in_net', self.in_net)\n",
        "        res_net_layer = OrderedDict()\n",
        "        res_net_layer['block_0'] = Block(n_dim, n_dim, norm)\n",
        "        res_net_layer['block_1'] = Block(n_dim, n_dim * 2, norm)\n",
        "        res_net_layer['block_2'] = Block(n_dim * 2, n_dim * 2, norm)\n",
        "        res_net_layer['pooling'] = nn.AdaptiveAvgPool1d(1)\n",
        "        self.res_net_layer = res_net_layer\n",
        "        self.res_net = nn.Sequential(res_net_layer)\n",
        "\n",
        "        self.out_net = nn.Linear(n_dim * 2, out_dim)\n",
        "        self.project_norm = project_norm\n",
        "        if is_projector:\n",
        "            if project_norm == 'BN':\n",
        "                self.projector = nn.Sequential(\n",
        "                    nn.BatchNorm1d(out_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim, out_dim * 2),\n",
        "                    nn.BatchNorm1d(out_dim * 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim * 2, out_dim)\n",
        "                )\n",
        "            elif project_norm == 'LN':\n",
        "                self.projector = nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.LayerNorm(out_dim),\n",
        "                    nn.Linear(out_dim, out_dim * 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.LayerNorm(out_dim * 2),\n",
        "                    nn.Linear(out_dim * 2, out_dim)\n",
        "                )\n",
        "            else:\n",
        "                self.projector = nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim, out_dim * 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim * 2, out_dim)\n",
        "                )\n",
        "        self.dummy = nn.Parameter(torch.empty(0))\n",
        "\n",
        "    def forward(self, ts, normalize=True, to_numpy=False):\n",
        "        device = self.dummy.device\n",
        "        is_projector = self.is_projector\n",
        "\n",
        "        ts = _normalize_t(ts, normalize)\n",
        "        ts = ts.to(device, dtype=torch.float32)\n",
        "\n",
        "        ts_emb = self.in_net(ts)\n",
        "        ts_emb = self.res_net(ts_emb)\n",
        "        ts_emb = ts_emb[:, :, 0]\n",
        "        ts_emb = self.out_net(ts_emb)\n",
        "\n",
        "        if is_projector:\n",
        "            ts_emb = self.projector(ts_emb)\n",
        "\n",
        "        if to_numpy:\n",
        "            return ts_emb.cpu().detach().numpy()\n",
        "        else:\n",
        "            return ts_emb\n",
        "\n",
        "    def encode(self, ts, normalize=True, to_numpy=False):\n",
        "        return self.forward(ts, normalize=normalize, to_numpy=to_numpy)\n",
        "\n",
        "    def encode_seq(self, ts, normalize=True, to_numpy=False):\n",
        "        device = self.dummy.device\n",
        "        is_projector = self.is_projector\n",
        "\n",
        "        ts = _normalize_t(ts, normalize)\n",
        "        ts = ts.to(device, dtype=torch.float32)\n",
        "\n",
        "        ts_emb = self.in_net(ts)\n",
        "        ts_emb = self.res_net_layer['block_0'](ts_emb)\n",
        "        ts_emb = self.res_net_layer['block_1'](ts_emb)\n",
        "        ts_emb = self.res_net_layer['block_2'](ts_emb)\n",
        "        ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "        ts_emb = self.out_net(ts_emb)\n",
        "\n",
        "        if is_projector:\n",
        "            project_norm = self.project_norm\n",
        "            if project_norm == 'BN':\n",
        "                layers = [module for module in projector.modules()]\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[1](ts_emb)\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[2](ts_emb)\n",
        "                ts_emb = layers[3](ts_emb)\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[4](ts_emb)\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[5](ts_emb)\n",
        "                ts_emb = layers[6](ts_emb)\n",
        "            else:\n",
        "                ts_emb = self.projector(ts_emb)\n",
        "        ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "\n",
        "        if to_numpy:\n",
        "            return ts_emb.cpu().detach().numpy()\n",
        "        else:\n",
        "            return ts_emb\n",
        "\n",
        "\n",
        "class Block_Standard(nn.Module):\n",
        "    def __init__(self, in_dim, n_dim, norm):\n",
        "        super(Block_Standard, self).__init__()\n",
        "\n",
        "        main_pass = OrderedDict()\n",
        "        main_pass['cov_0'] = nn.Conv1d(\n",
        "            in_dim, n_dim, 7, stride=1, padding=3, dilation=1)\n",
        "        if norm == 'BN':\n",
        "            main_pass['bn_0'] = nn.BatchNorm1d(n_dim)\n",
        "        elif norm == 'LN':\n",
        "            main_pass['ln_0'] = LayerNormT(n_dim)\n",
        "        main_pass['relu_0'] = nn.ReLU()\n",
        "\n",
        "        main_pass['cov_1'] = nn.Conv1d(\n",
        "            n_dim, n_dim, 5, stride=1, padding=2, dilation=1)\n",
        "        if norm == 'BN':\n",
        "            main_pass['bn_1'] = nn.BatchNorm1d(n_dim)\n",
        "        elif norm == 'LN':\n",
        "            main_pass['ln_1'] = LayerNormT(n_dim)\n",
        "        main_pass['relu_1'] = nn.ReLU()\n",
        "\n",
        "        main_pass['cov_2'] = nn.Conv1d(\n",
        "            n_dim, n_dim, 3, stride=1, padding=1, dilation=1)\n",
        "        if norm == 'BN':\n",
        "            main_pass['bn_2'] = nn.BatchNorm1d(n_dim)\n",
        "        elif norm == 'LN':\n",
        "            main_pass['ln_2'] = LayerNormT(n_dim)\n",
        "        self.main_pass = nn.Sequential(main_pass)\n",
        "\n",
        "        shortcut = OrderedDict()\n",
        "        if in_dim != n_dim:\n",
        "            shortcut['cov_0'] = nn.Conv1d(\n",
        "                in_dim, n_dim, 1, stride=1, padding=0, dilation=1)\n",
        "            if norm == 'BN':\n",
        "                shortcut['bn_0'] = nn.BatchNorm1d(n_dim)\n",
        "            elif norm == 'LN':\n",
        "                main_pass['ln_0'] = LayerNormT(n_dim)\n",
        "        else:\n",
        "            shortcut['id_0'] = nn.Identity()\n",
        "        self.shortcut = nn.Sequential(shortcut)\n",
        "\n",
        "    def forward(self, data):\n",
        "        hodden_0 = self.main_pass(data)\n",
        "        hodden_1 = self.shortcut(data)\n",
        "        output = nn.ReLU()(hodden_0 + hodden_1)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Block_Alt(nn.Module):\n",
        "    def __init__(self, in_dim, n_dim, norm):\n",
        "        super(Block_Alt, self).__init__()\n",
        "\n",
        "        main_pass = OrderedDict()\n",
        "        if norm == 'BN':\n",
        "            main_pass['bn_0'] = nn.BatchNorm1d(in_dim)\n",
        "        elif norm == 'LN':\n",
        "            main_pass['ln_0'] = LayerNormT(in_dim)\n",
        "        main_pass['cov_0'] = nn.Conv1d(\n",
        "            in_dim, n_dim, 7, stride=1, padding=3, dilation=1)\n",
        "        main_pass['relu_0'] = nn.ReLU()\n",
        "\n",
        "        if norm == 'BN':\n",
        "            main_pass['bn_1'] = nn.BatchNorm1d(n_dim)\n",
        "        elif norm == 'LN':\n",
        "            main_pass['ln_1'] = LayerNormT(n_dim)\n",
        "        main_pass['cov_1'] = nn.Conv1d(\n",
        "            n_dim, n_dim, 5, stride=1, padding=2, dilation=1)\n",
        "        main_pass['relu_1'] = nn.ReLU()\n",
        "\n",
        "        if norm == 'BN':\n",
        "            main_pass['bn_2'] = nn.BatchNorm1d(n_dim)\n",
        "        elif norm == 'LN':\n",
        "            main_pass['ln_2'] = LayerNormT(n_dim)\n",
        "        main_pass['cov_2'] = nn.Conv1d(\n",
        "            n_dim, n_dim, 3, stride=1, padding=1, dilation=1)\n",
        "        main_pass['relu_2'] = nn.ReLU()\n",
        "        self.main_pass = nn.Sequential(main_pass)\n",
        "\n",
        "        shortcut = OrderedDict()\n",
        "        if in_dim != n_dim:\n",
        "            if norm == 'BN':\n",
        "                shortcut['bn_0'] = nn.BatchNorm1d(in_dim)\n",
        "            elif norm == 'LN':\n",
        "                main_pass['ln_0'] = LayerNormT(in_dim)\n",
        "            shortcut['cov_0'] = nn.Conv1d(\n",
        "                in_dim, n_dim, 1, stride=1, padding=0, dilation=1)\n",
        "        else:\n",
        "            shortcut['id_0'] = nn.Identity()\n",
        "        self.shortcut = nn.Sequential(shortcut)\n",
        "\n",
        "    def forward(self, data):\n",
        "        hodden_0 = self.main_pass(data)\n",
        "        hodden_1 = self.shortcut(data)\n",
        "        output = hodden_0 + hodden_1\n",
        "        return output\n",
        "\n",
        "\n",
        "class LayerNormT(nn.Module):\n",
        "    def __init__(self, n_dim):\n",
        "        super(LayerNormT, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_dim)\n",
        "        self.add_module('layer_norm', self.layer_norm)\n",
        "\n",
        "    def forward(self, data):\n",
        "        data = torch.transpose(data, 1, 2)\n",
        "        self.layer_norm.forward(data)\n",
        "        data = torch.transpose(data, 1, 2)\n",
        "        return data\n",
        "\n",
        "def get_resnet1d(model_config):\n",
        "    in_dim = int(model_config['encoder']['in_dim'])\n",
        "    out_dim = int(model_config['encoder']['out_dim'])\n",
        "    n_dim = int(model_config['encoder']['n_dim'])\n",
        "    block_type = model_config['encoder']['block_type']\n",
        "    norm = model_config['encoder']['norm']\n",
        "    is_projector = model_config['encoder']['is_projector']\n",
        "    is_projector = is_projector.lower() == 'true'\n",
        "    project_norm = model_config['encoder']['project_norm']\n",
        "    encoder = ResNet1D(\n",
        "        in_dim=in_dim, out_dim=out_dim, n_dim=n_dim,\n",
        "        block_type=block_type, norm=norm,\n",
        "        is_projector=is_projector, project_norm=project_norm)\n",
        "\n",
        "    encoder = load_pretrain(model_config['encoder'], encoder)\n",
        "    return encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyzY5qaa8VRp"
      },
      "source": [
        "### RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPa90LZo8U-N"
      },
      "outputs": [],
      "source": [
        "class ALSTM(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, bidirectional=True, dropout = 0.0):\n",
        "        \"\"\"\n",
        "        Initialize the network.\n",
        "\n",
        "        Args:\n",
        "            config:\n",
        "            input_size: (int): size of the input\n",
        "        \"\"\"\n",
        "        super(ALSTM, self).__init__()\n",
        "        self.in_dim = input_size\n",
        "        self.n_dim = hidden_size\n",
        "        self.directions = 2\n",
        "        self.n_layer = num_layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.in_dim,\n",
        "            hidden_size=self.n_dim,\n",
        "            num_layers=num_layers, batch_first=True,\n",
        "            bidirectional= True\n",
        "        )\n",
        "        # self.concat_linear = nn.Linear( self.directions * self.n_dim *2 , self.n_dim *2)\n",
        "        # self.attn = nn.Linear(self.n_dim*2, self.n_dim*2)\n",
        "\n",
        "        self.attention = nn.Linear( self.directions * self.n_dim , 1)\n",
        "        self.fc = nn.Linear(self.n_dim * self.directions, self.n_dim * self.directions)\n",
        "\n",
        "\n",
        "    def forward(self, input_data: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Forward computation.\n",
        "\n",
        "        Args:\n",
        "            input_data: (torch.Tensor): tensor of input data\n",
        "        \"\"\"\n",
        "        # h0 = torch.zeros(self.n_layer * 2, input_data.size(0), self.n_dim).to(device)  # 2 for bidirection\n",
        "        # c0 = torch.zeros(self.n_layer * 2, input_data.size(0), self.n_dim).to(device)\n",
        "        # rnn_output, hidden_states = self.lstm(input_data, (h0, c0))\n",
        "        # final_state = hidden_states[0].view(self.n_layer, self.directions, input_data.size(0), self.n_dim)[-1]\n",
        "        # h_1, h_2 = final_state[0], final_state[1]\n",
        "        # final_hidden_state = torch.cat((h_1, h_2), 1)\n",
        "        # attn_weights = self.attn(rnn_output)\n",
        "        # attn_weights = torch.bmm(attn_weights, final_hidden_state.unsqueeze(2))\n",
        "        # attn_weights = F.softmax(attn_weights.squeeze(2), dim=1)\n",
        "        # context = torch.bmm(rnn_output.transpose(1, 2), attn_weights.unsqueeze(2)).squeeze(2)\n",
        "        # output = torch.tanh(self.concat_linear(torch.cat((context, final_hidden_state), dim=1)))\n",
        "\n",
        "\n",
        "        h0 = torch.zeros(self.n_layer * 2, input_data.size(0), self.n_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layer * 2, input_data.size(0), self.n_dim).to(device)\n",
        "        rnn_output, hidden_states = self.lstm(input_data, (h0, c0))\n",
        "\n",
        "        # Compute attention weights\n",
        "        attn_weights = F.softmax(self.attention(rnn_output), dim=1)\n",
        "\n",
        "        # Compute context vector\n",
        "        context = torch.sum(attn_weights * rnn_output, dim=1)\n",
        "\n",
        "        # Apply linear layer\n",
        "        output = torch.tanh(self.fc(context))\n",
        "\n",
        "\n",
        "        return output, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AGRU(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, bidirectional=True, dropout = 0.0):\n",
        "        \"\"\"\n",
        "        Initialize the network.\n",
        "        Args:\n",
        "            config:\n",
        "            input_size: (int): size of the input\n",
        "        \"\"\"\n",
        "        super(AGRU, self).__init__()\n",
        "        self.in_dim = input_size\n",
        "        self.n_dim = hidden_size\n",
        "        self.directions = 2\n",
        "        self.n_layer = num_layers\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.in_dim,\n",
        "            hidden_size=self.n_dim,\n",
        "            num_layers=num_layers, batch_first=True,\n",
        "            bidirectional= True\n",
        "        )\n",
        "        # self.concat_linear = nn.Linear( self.directions * self.n_dim *2 , self.n_dim *2)\n",
        "        # self.attn = nn.Linear(self.n_dim*2, self.n_dim*2)\n",
        "\n",
        "        self.attention = nn.Linear(self.directions * self.n_dim , 1)\n",
        "        self.fc = nn.Linear(self.n_dim * self.directions, self.n_dim * self.directions)\n",
        "\n",
        "\n",
        "    def forward(self, input_data: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Forward computation.\n",
        "\n",
        "        Args:\n",
        "            input_data: (torch.Tensor): tensor of input data\n",
        "        \"\"\"\n",
        "\n",
        "        h0 = torch.zeros(self.n_layer * 2, input_data.size(0), self.n_dim).to(device)\n",
        "        rnn_output, hidden_states = self.gru(input_data, h0)\n",
        "\n",
        "        # Compute attention weights\n",
        "        attn_weights = F.softmax(self.attention(rnn_output), dim=1)\n",
        "\n",
        "        # Compute context vector\n",
        "        context = torch.sum(attn_weights * rnn_output, dim=1)\n",
        "\n",
        "        # Apply linear layer\n",
        "        output = torch.tanh(self.fc(context))\n",
        "\n",
        "\n",
        "        return output, attn_weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class RNNet(nn.Module):\n",
        "    def __init__(self, in_dim=1, out_dim=128, rnn_type='GRU',\n",
        "                 n_layer=2, n_dim=64, seq_len = 512, is_projector=True,\n",
        "                 project_norm=None, dropout=0.0):\n",
        "        r\"\"\"\n",
        "        RNN-based time series encoder\n",
        "\n",
        "        Args:\n",
        "            in_dim (int, optional): Number of dimension for the input time\n",
        "                series. Default: 1.\n",
        "            out_dim (int, optional): Number of dimension for the output\n",
        "                representation. Default: 128.\n",
        "            rnn_type (string, optional): The type of RNN cell to use. Can be\n",
        "                either ``'GRU'`` or ``'LSTM'``. Default: ``'GRU'``\n",
        "            n_layer (int, optional): Number of layer for the transformer\n",
        "                encoder. Default: 8.\n",
        "            n_dim (int, optional): Number of dimension for the intermediate\n",
        "                representation. Default: 64.\n",
        "            is_projector (bool, optional): If set to ``False``, the encoder\n",
        "                will not use additional projection layers. Default: ``True``.\n",
        "            project_norm (string, optional): If set to ``BN``, the projector\n",
        "                will use batch normalization. If set to ``LN``, the projector\n",
        "                will use layer normalization. If set to None, the projector\n",
        "                will not use normalization. Default: None (no normalization).\n",
        "            dropout (float, optional): The probability of an element to be\n",
        "                zeroed for the dropout layers. Default: 0.0.\n",
        "\n",
        "        Shape:\n",
        "            - Input: :math:`(N, C_{in}, L_{in})`, :math:`(N, L_{in})`, or\n",
        "                :math:`(L_{in})`.\n",
        "            - Output: :math:`(N, C_{out})`.\n",
        "        \"\"\"\n",
        "        super(RNNet, self).__init__()\n",
        "        assert project_norm in ['BN', 'LN', None]\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.n_dim = n_dim\n",
        "        self.is_projector = is_projector\n",
        "        self.seq_len = seq_len\n",
        "        self.rnn_type = rnn_type\n",
        "\n",
        "\n",
        "        self.in_net = nn.Conv1d(\n",
        "            in_dim, n_dim, 7, stride=2, padding=3, dilation=1)\n",
        "        self.add_module('in_net', self.in_net)\n",
        "        if rnn_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(\n",
        "                input_size=n_dim, hidden_size=n_dim, num_layers=n_layer,\n",
        "                batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        elif rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(\n",
        "                input_size=n_dim, hidden_size=n_dim, num_layers=n_layer,\n",
        "                batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        elif rnn_type == 'ALSTM':\n",
        "            self.rnn = ALSTM(\n",
        "                input_size=n_dim, hidden_size=n_dim, num_layers=n_layer,\n",
        "                 dropout=dropout, bidirectional=True)\n",
        "        elif rnn_type == 'AGRU':\n",
        "            self.rnn = ALSTM(\n",
        "                input_size=n_dim, hidden_size=n_dim, num_layers=n_layer,\n",
        "                 dropout=dropout, bidirectional=True)\n",
        "\n",
        "        self.out_net = nn.Linear(n_dim * 2, out_dim)\n",
        "        self.project_norm = project_norm\n",
        "        if is_projector:\n",
        "            if project_norm == 'BN':\n",
        "                self.is_projector = nn.Sequential(\n",
        "                    nn.BatchNorm1d(out_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim, out_dim * 2),\n",
        "                    nn.BatchNorm1d(out_dim * 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim * 2, out_dim)\n",
        "                )\n",
        "            elif project_norm == 'LN':\n",
        "                self.is_projector = nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.LayerNorm(out_dim),\n",
        "                    nn.Linear(out_dim, out_dim * 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.LayerNorm(out_dim * 2),\n",
        "                    nn.Linear(out_dim * 2, out_dim)\n",
        "                )\n",
        "            else:\n",
        "                self.is_projector = nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim, out_dim * 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim * 2, out_dim)\n",
        "                )\n",
        "        self.dummy = nn.Parameter(torch.empty(0))\n",
        "\n",
        "    def forward(self, ts, normalize=True, to_numpy=False):\n",
        "        device = self.dummy.device\n",
        "        is_projector = self.is_projector\n",
        "\n",
        "        ts = _normalize_t(ts, normalize)\n",
        "        ts = ts.to(device, dtype=torch.float32)\n",
        "        ts_emb = self.in_net(ts)\n",
        "        ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "        ts_emb, _ = self.rnn(ts_emb)\n",
        "        if self.rnn_type != 'ALSTM' and self.rnn_type != 'AGRU':\n",
        "          ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "          ts_emb = ts_emb[:, :, 0]\n",
        "        ts_emb = self.out_net(ts_emb)\n",
        "\n",
        "        if is_projector:\n",
        "            ts_emb = self.is_projector(ts_emb)\n",
        "\n",
        "        if to_numpy:\n",
        "            return ts_emb.cpu().detach().numpy()\n",
        "        else:\n",
        "            return ts_emb\n",
        "\n",
        "    def encode(self, ts, normalize=True, to_numpy=False):\n",
        "        return self.forward(ts, normalize=normalize, to_numpy=to_numpy)\n",
        "\n",
        "    def encode_seq(self, ts, normalize=True, to_numpy=False):\n",
        "        device = self.dummy.device\n",
        "        is_projector = self.is_projector\n",
        "\n",
        "        ts = _normalize_t(ts, normalize)\n",
        "        ts = ts.to(device, dtype=torch.float32)\n",
        "\n",
        "        ts_emb = self.in_net(ts)\n",
        "        ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "        ts_emb, _ = self.rnn(ts_emb)\n",
        "        ts_emb = self.out_net(ts_emb)\n",
        "\n",
        "        if is_projector:\n",
        "            project_norm = self.project_norm\n",
        "            if project_norm == 'BN':\n",
        "                layers = [module for module in is_projector.modules()]\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[1](ts_emb)\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[2](ts_emb)\n",
        "                ts_emb = layers[3](ts_emb)\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[4](ts_emb)\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[5](ts_emb)\n",
        "                ts_emb = layers[6](ts_emb)\n",
        "            else:\n",
        "                ts_emb = self.is_projector(ts_emb)\n",
        "        ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "\n",
        "        if to_numpy:\n",
        "            return ts_emb.cpu().detach().numpy()\n",
        "        else:\n",
        "            return ts_emb\n",
        "\n",
        "def get_rnnet(model_config):\n",
        "    in_dim = int(model_config['encoder']['in_dim'])\n",
        "    out_dim = int(model_config['encoder']['out_dim'])\n",
        "    rnn_type = model_config['encoder']['rnn_type']\n",
        "    n_layer = int(model_config['encoder']['n_layer'])\n",
        "    n_dim = int(model_config['encoder']['n_dim'])\n",
        "    is_projector = model_config['encoder']['is_projector']\n",
        "    is_projector = is_projector.lower() == 'true'\n",
        "    project_norm = model_config['encoder']['project_norm']\n",
        "    dropout = float(model_config['encoder']['dropout'])\n",
        "    encoder = RNNet(\n",
        "        in_dim=in_dim, out_dim=out_dim, rnn_type=rnn_type,\n",
        "        n_layer=n_layer, n_dim=n_dim, is_projector=is_projector,\n",
        "        project_norm=project_norm, dropout=dropout)\n",
        "    encoder = load_pretrain(model_config['encoder'], encoder)\n",
        "    return encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky82j1EG8mcI"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GRZYCp78nq9"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, in_dim=1, out_dim=128, n_layer=8, n_dim=64, n_head=8,\n",
        "                 norm_first=False, is_pos=True, is_projector=True,\n",
        "                 project_norm=None, dropout=0.0):\n",
        "        r\"\"\"\n",
        "        Transformer-based time series encoder\n",
        "\n",
        "        Args:\n",
        "            in_dim (int, optional): Number of dimension for the input time\n",
        "                series. Default: 1.\n",
        "            out_dim (int, optional): Number of dimension for the output\n",
        "                representation. Default: 128.\n",
        "            n_layer (int, optional): Number of layer for the transformer\n",
        "                encoder. Default: 8.\n",
        "            n_dim (int, optional): Number of dimension for the intermediate\n",
        "                representation. Default: 64.\n",
        "            n_head (int, optional): Number of head for the transformer\n",
        "                encoder. Default: 8.\n",
        "            norm_first: if ``True``, layer norm is done prior to attention and\n",
        "                feedforward operations, respectively. Otherwise it's done\n",
        "                after. Default: ``False`` (after).\n",
        "            is_pos (bool, optional): If set to ``False``, the encoder will\n",
        "                not use position encoding. Default: ``True``.\n",
        "            is_projector (bool, optional): If set to ``False``, the encoder\n",
        "                will not use additional projection layers. Default: ``True``.\n",
        "            project_norm (string, optional): If set to ``BN``, the projector\n",
        "                will use batch normalization. If set to ``LN``, the projector\n",
        "                will use layer normalization. If set to None, the projector\n",
        "                will not use normalization. Default: None (no normalization).\n",
        "            dropout (float, optional): The probability of an element to be\n",
        "                zeroed for the dropout layers. Default: 0.0.\n",
        "\n",
        "        Shape:\n",
        "            - Input: :math:`(N, C_{in}, L_{in})`, :math:`(N, L_{in})`, or\n",
        "                :math:`(L_{in})`.\n",
        "            - Output: :math:`(N, C_{out})`.\n",
        "        \"\"\"\n",
        "        super(Transformer, self).__init__()\n",
        "        assert project_norm in ['BN', 'LN', None]\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.n_dim = n_dim\n",
        "        self.is_projector = is_projector\n",
        "        self.is_pos = is_pos\n",
        "        self.max_len = 0\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.in_net = nn.Conv1d(\n",
        "            in_dim, n_dim, 7, stride=2, padding=3, dilation=1)\n",
        "        self.add_module('in_net', self.in_net)\n",
        "        transformer = OrderedDict()\n",
        "        for i in range(n_layer):\n",
        "            transformer[f'encoder_{i:02d}'] = nn.TransformerEncoderLayer(\n",
        "                n_dim, n_head, dim_feedforward=n_dim,\n",
        "                dropout=dropout, batch_first=True,\n",
        "                norm_first=norm_first)\n",
        "        self.transformer = nn.Sequential(transformer)\n",
        "\n",
        "        self.start_token = nn.Parameter(\n",
        "            torch.randn(1, n_dim, 1))\n",
        "        self.register_parameter(\n",
        "            name='start_token',\n",
        "            param=self.start_token)\n",
        "\n",
        "        self.out_net = nn.Linear(n_dim, out_dim)\n",
        "        self.project_norm = project_norm\n",
        "        if is_projector:\n",
        "            if project_norm == 'BN':\n",
        "                self.projector = nn.Sequential(\n",
        "                    nn.BatchNorm1d(out_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim, out_dim * 2),\n",
        "                    nn.BatchNorm1d(out_dim * 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim * 2, out_dim)\n",
        "                )\n",
        "            elif project_norm == 'LN':\n",
        "                self.projector = nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.LayerNorm(out_dim),\n",
        "                    nn.Linear(out_dim, out_dim * 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.LayerNorm(out_dim * 2),\n",
        "                    nn.Linear(out_dim * 2, out_dim)\n",
        "                )\n",
        "            else:\n",
        "                self.projector = nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim, out_dim * 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim * 2, out_dim)\n",
        "                )\n",
        "        self.dummy = nn.Parameter(torch.empty(0))\n",
        "\n",
        "    def forward(self, ts, normalize=True, to_numpy=False):\n",
        "        device = self.dummy.device\n",
        "        is_projector = self.is_projector\n",
        "        is_pos = self.is_pos\n",
        "\n",
        "        ts = _normalize_t(ts, normalize)\n",
        "        ts = ts.to(device, dtype=torch.float32)\n",
        "\n",
        "        ts_emb = self.in_net(ts)\n",
        "\n",
        "\n",
        "        if is_pos:\n",
        "            n_dim = self.n_dim\n",
        "            dropout = self.dropout\n",
        "            ts_len = ts_emb.size()[2]\n",
        "            if ts_len > self.max_len:\n",
        "                self.max_len = ts_len\n",
        "                self.pos_net = PositionalEncoding(\n",
        "                    n_dim, ts_len, dropout=dropout)\n",
        "                self.pos_net.to(device)\n",
        "            ts_emb = self.pos_net(ts_emb)\n",
        "\n",
        "        start_tokens = self.start_token.expand(ts_emb.size()[0], -1, -1)\n",
        "        ts_emb = torch.cat((start_tokens, ts_emb, ), dim=2)\n",
        "        ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "\n",
        "        ts_emb = self.transformer(ts_emb)\n",
        "        ts_emb = ts_emb[:, 0, :]\n",
        "        ts_emb = self.out_net(ts_emb)\n",
        "\n",
        "        if is_projector:\n",
        "            ts_emb = self.projector(ts_emb)\n",
        "\n",
        "        if to_numpy:\n",
        "            return ts_emb.cpu().detach().numpy()\n",
        "        else:\n",
        "            return ts_emb\n",
        "\n",
        "    def encode(self, ts, normalize=True, to_numpy=False):\n",
        "        return self.forward(ts, normalize=normalize, to_numpy=to_numpy)\n",
        "\n",
        "    def encode_seq(self, ts, normalize=True, to_numpy=False):\n",
        "        device = self.dummy.device\n",
        "        is_projector = self.is_projector\n",
        "        is_pos = self.is_pos\n",
        "\n",
        "        ts = _normalize_t(ts, normalize)\n",
        "        ts = ts.to(device, dtype=torch.float32)\n",
        "\n",
        "        ts_emb = self.in_net(ts)\n",
        "        if is_pos:\n",
        "            n_dim = self.n_dim\n",
        "            dropout = self.dropout\n",
        "            ts_len = ts_emb.size()[2]\n",
        "            if ts_len > self.max_len:\n",
        "                self.max_len = ts_len\n",
        "                self.pos_net = PositionalEncoding(\n",
        "                    n_dim, ts_len, dropout=dropout)\n",
        "                self.pos_net.to(device)\n",
        "            ts_emb = self.pos_net(ts_emb)\n",
        "\n",
        "        start_tokens = self.start_token.expand(ts_emb.size()[0], -1, -1)\n",
        "        ts_emb = torch.cat((start_tokens, ts_emb, ), dim=2)\n",
        "        ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "\n",
        "        ts_emb = self.transformer(ts_emb)\n",
        "        ts_emb = self.out_net(ts_emb)\n",
        "        if is_projector:\n",
        "            project_norm = self.project_norm\n",
        "            if project_norm == 'BN':\n",
        "                layers = [module for module in is_projector.modules()]\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[1](ts_emb)\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[2](ts_emb)\n",
        "                ts_emb = layers[3](ts_emb)\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[4](ts_emb)\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[5](ts_emb)\n",
        "                ts_emb = layers[6](ts_emb)\n",
        "            else:\n",
        "                ts_emb = self.projector(ts_emb)\n",
        "\n",
        "        ts_emb = ts_emb[:, 1:, :]\n",
        "        start_tokens = ts_emb[:, 0:1, :]\n",
        "        start_tokens = start_tokens.expand(-1, ts_emb.size()[1], -1)\n",
        "        ts_emb = ts_emb + start_tokens\n",
        "        ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "\n",
        "        if to_numpy:\n",
        "            return ts_emb.cpu().detach().numpy()\n",
        "        else:\n",
        "            return ts_emb\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, n_dim, max_len, dropout=0.0):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, n_dim, 2) * (-math.log(10000.0) / n_dim))\n",
        "        pos_emb = torch.zeros(1, n_dim, max_len)\n",
        "\n",
        "        position = position.unsqueeze(0)\n",
        "        div_term = div_term.unsqueeze(1)\n",
        "        pos_emb[0, 0::2, :] = torch.sin(div_term * position)\n",
        "        pos_emb[0, 1::2, :] = torch.cos(div_term * position)\n",
        "        self.register_buffer('pos_emb', pos_emb, persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pos_emb[:, :, :x.size()[2]]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "\n",
        "def get_transformer(model_config):\n",
        "    in_dim = int(model_config['encoder']['in_dim'])\n",
        "    out_dim = int(model_config['encoder']['out_dim'])\n",
        "    n_layer = int(model_config['encoder']['n_layer'])\n",
        "    n_dim = int(model_config['encoder']['n_dim'])\n",
        "    n_head = int(model_config['encoder']['n_head'])\n",
        "    norm_first = model_config['encoder']['norm_first']\n",
        "    norm_first = norm_first.lower() == 'true'\n",
        "    is_pos = model_config['encoder']['is_pos']\n",
        "    is_pos = is_pos.lower() == 'true'\n",
        "    is_projector = model_config['encoder']['is_projector']\n",
        "    is_projector = is_projector.lower() == 'true'\n",
        "    project_norm = model_config['encoder']['project_norm']\n",
        "    dropout = float(model_config['encoder']['dropout'])\n",
        "\n",
        "    encoder = Transformer(\n",
        "        in_dim=in_dim, out_dim=out_dim, n_layer=n_layer,\n",
        "        n_dim=n_dim, n_head=n_head, norm_first=norm_first,\n",
        "        is_pos=is_pos, is_projector=is_projector,\n",
        "        project_norm=project_norm, dropout=dropout)\n",
        "    encoder = load_pretrain(model_config['encoder'], encoder)\n",
        "    return encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### WaveNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WaveNet(nn.Module):\n",
        "    def __init__(self, in_dim=1, out_dim=128, n_layer = 10, n_dim=64, norm=None,\n",
        "                 is_projector=True, project_norm=None):\n",
        "\n",
        "        super(WaveNet, self).__init__()\n",
        "\n",
        "        assert project_norm in ['BN', 'LN', None]\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.n_dim = n_dim\n",
        "        self.is_projector = is_projector\n",
        "        self.n_layer = n_layer\n",
        "        self.kernel_size = 2\n",
        "        self.n_block = 4\n",
        "\n",
        "        #Input layer\n",
        "        self.in_net = nn.Conv1d(\n",
        "            in_dim, n_dim, 7, stride=2, padding=3, dilation=1)\n",
        "        self.add_module('in_net', self.in_net)\n",
        "\n",
        "        #WaveNet\n",
        "        self.dilations = []\n",
        "        self.bias = False\n",
        "        self.filter_convs = nn.ModuleList()\n",
        "        self.gate_convs = nn.ModuleList()\n",
        "        self.residual_convs = nn.ModuleList()\n",
        "        self.skip_convs = nn.ModuleList()\n",
        "\n",
        "        for b in range(self.n_block):\n",
        "            additional_scope = self.kernel_size - 1\n",
        "            new_dilation = 1\n",
        "            for i in range(n_layer):\n",
        "\n",
        "                self.dilations.append(new_dilation)\n",
        "                self.filter_convs.append(nn.Conv1d(in_channels=self.n_dim,\n",
        "                                                   out_channels=self.n_dim,\n",
        "                                                   kernel_size=self.kernel_size,\n",
        "                                                   dilation=new_dilation,\n",
        "                                                   bias=self.bias))\n",
        "\n",
        "                self.gate_convs.append(nn.Conv1d(in_channels=self.n_dim,\n",
        "                                                 out_channels=self.n_dim,\n",
        "                                                 kernel_size=self.kernel_size,\n",
        "                                                 dilation=new_dilation,\n",
        "                                                 bias=self.bias))\n",
        "\n",
        "                self.residual_convs.append(nn.Conv1d(in_channels=self.n_dim,\n",
        "                                                     out_channels=self.n_dim,\n",
        "                                                     kernel_size=1,\n",
        "                                                     bias=self.bias))\n",
        "\n",
        "                self.skip_convs.append(nn.Conv1d(in_channels=self.n_dim,\n",
        "                                                 out_channels=self.n_dim,\n",
        "                                                 kernel_size=1,\n",
        "                                                 bias=self.bias))\n",
        "                additional_scope *= 2\n",
        "                new_dilation *= 2\n",
        "        self.conv_end = nn.Conv1d(in_channels=self.n_dim,\n",
        "                                        out_channels=1,\n",
        "                                        kernel_size=1,\n",
        "                                        bias=self.bias)\n",
        "        self.out_net = nn.Linear(n_dim*4, out_dim)\n",
        "\n",
        "        #Projector\n",
        "        self.project_norm = project_norm\n",
        "        if is_projector:\n",
        "            if project_norm == 'BN':\n",
        "                self.projector = nn.Sequential(\n",
        "                    nn.BatchNorm1d(out_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim, out_dim * 2),\n",
        "                    nn.BatchNorm1d(out_dim * 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim * 2, out_dim)\n",
        "                )\n",
        "            elif project_norm == 'LN':\n",
        "                self.projector = nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.LayerNorm(out_dim),\n",
        "                    nn.Linear(out_dim, out_dim * 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.LayerNorm(out_dim * 2),\n",
        "                    nn.Linear(out_dim * 2, out_dim)\n",
        "                )\n",
        "            else:\n",
        "                self.projector = nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim, out_dim * 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim * 2, out_dim)\n",
        "                )\n",
        "        self.dummy = nn.Parameter(torch.empty(0))\n",
        "\n",
        "    def wave_net(self, input):\n",
        "        x = input\n",
        "        for i in range(self.n_block * self.n_layer):\n",
        "\n",
        "            # filter convolution\n",
        "            causal_padding = (int((self.kernel_size - 1) * (self.dilations[i])),0)\n",
        "            padded_x = F.pad(x, causal_padding)\n",
        "            filter = self.filter_convs[i](padded_x)\n",
        "            filter = F.tanh(filter)\n",
        "            #gated convolution\n",
        "            gate = self.gate_convs[i](padded_x)\n",
        "            gate = F.sigmoid(gate)\n",
        "            z = filter * gate\n",
        "            residual = self.residual_convs[i](z)\n",
        "            x = x + residual\n",
        "            if i == 0:\n",
        "                output = self.skip_convs[i](z)\n",
        "            else:\n",
        "                output = self.skip_convs[i](z) + output\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, ts, normalize=True, to_numpy=False):\n",
        "        device = self.dummy.device\n",
        "        is_projector = self.is_projector\n",
        "\n",
        "        ts = _normalize_t(ts, normalize)\n",
        "        ts = ts.to(device, dtype=torch.float32)\n",
        "\n",
        "        ts_emb = self.in_net(ts)\n",
        "        ts_emb = F.relu(self.wave_net(ts_emb))\n",
        "        ts_emb = F.relu(self.conv_end(ts_emb))\n",
        "        # print(ts_emb.shape)\n",
        "        ts_emb = ts_emb[:, 0, :]\n",
        "        # print(ts_emb.shape)\n",
        "        ts_emb = self.out_net(ts_emb)\n",
        "\n",
        "        if is_projector:\n",
        "            ts_emb = self.projector(ts_emb)\n",
        "\n",
        "        if to_numpy:\n",
        "            return ts_emb.cpu().detach().numpy()\n",
        "        else:\n",
        "            return ts_emb\n",
        "\n",
        "    def encode(self, ts, normalize=True, to_numpy=False):\n",
        "        return self.forward(ts, normalize=normalize, to_numpy=to_numpy)\n",
        "\n",
        "    def encode_seq(self, ts, normalize=True, to_numpy=False):\n",
        "        device = self.dummy.device\n",
        "        is_projector = self.is_projector\n",
        "        projector = self.projector\n",
        "\n",
        "        ts = _normalize_t(ts, normalize)\n",
        "        ts = ts.to(device, dtype=torch.float32)\n",
        "\n",
        "        ts_emb = self.in_net(ts)\n",
        "        ts_emb = F.relu(self.wave_net(ts_emb))\n",
        "        ts_emb = F.relu(self.conv_end(ts_emb))\n",
        "        ts_emb = ts_emb[:, 0, :]\n",
        "\n",
        "        if is_projector:\n",
        "            project_norm = self.project_norm\n",
        "            if project_norm == 'BN':\n",
        "                layers = [module for module in projector.modules()]\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[1](ts_emb)\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[2](ts_emb)\n",
        "                ts_emb = layers[3](ts_emb)\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[4](ts_emb)\n",
        "                ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "                ts_emb = layers[5](ts_emb)\n",
        "                ts_emb = layers[6](ts_emb)\n",
        "            else:\n",
        "                ts_emb = self.projector(ts_emb)\n",
        "        ts_emb = torch.transpose(ts_emb, 1, 2)\n",
        "\n",
        "        if to_numpy:\n",
        "            return ts_emb.cpu().detach().numpy()\n",
        "        else:\n",
        "            return ts_emb\n",
        "\n",
        "def get_wavenet(model_config):\n",
        "    in_dim = int(model_config['encoder']['in_dim'])\n",
        "    out_dim = int(model_config['encoder']['out_dim'])\n",
        "    n_layer = int(model_config['encoder']['n_layer'])\n",
        "    n_dim = int(model_config['encoder']['n_dim'])\n",
        "    is_projector = model_config['encoder']['is_projector']\n",
        "    is_projector = is_projector.lower() == 'true'\n",
        "    project_norm = model_config['encoder']['project_norm']\n",
        "\n",
        "    encoder = WaveNet(\n",
        "        in_dim=in_dim, out_dim=out_dim, n_layer=n_layer, n_dim=n_dim,\n",
        "        is_projector=is_projector,project_norm=project_norm)\n",
        "    encoder = load_pretrain(model_config['encoder'], encoder)\n",
        "    return encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeQpHLGbvhSH"
      },
      "source": [
        "### TCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHAuPnctvjWo"
      },
      "outputs": [],
      "source": [
        "class GatedActivation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GatedActivation, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.tanh(x) * torch.sigmoid(x)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, n_dim, dilation):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.dilation = dilation\n",
        "        self.kernel_size = 4\n",
        "        self.n_dim = n_dim\n",
        "        self.conv1 = nn.Conv1d(in_channels=self.n_dim, out_channels=self.n_dim, kernel_size=4, stride=1,  dilation=self.dilation)\n",
        "        self.activation = GatedActivation()\n",
        "        self.batchnorm = nn.BatchNorm1d(self.n_dim, momentum=0.6)\n",
        "        self.conv2 = nn.Conv1d(in_channels=self.n_dim, out_channels=self.n_dim, kernel_size=4, stride=1, dilation=self.dilation)\n",
        "        self.residual = nn.Conv1d(in_channels=self.n_dim, out_channels=self.n_dim, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.residual(x)\n",
        "        causal_padding=(self.dilation*(self.kernel_size - 1), 0)\n",
        "        out = F.pad(x, causal_padding)\n",
        "        out = self.conv1(x)\n",
        "        out = self.batchnorm(out)\n",
        "        out = self.activation(out)\n",
        "        out = F.pad(x, causal_padding)\n",
        "        out = self.conv2(out)\n",
        "        out = self.batchnorm(out)\n",
        "        out = self.activation(out)\n",
        "\n",
        "        out = out + residual\n",
        "        return out\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, in_dim=1, out_dim=128, n_layer = 1, n_dim=64, norm=None,\n",
        "                 is_projector=True, project_norm=None):\n",
        "\n",
        "        super(TemporalConvNet, self).__init__()\n",
        "\n",
        "        assert project_norm in ['BN', 'LN', None]\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.n_dim = n_dim\n",
        "        self.is_projector = is_projector\n",
        "        self.n_layer = n_layer\n",
        "        self.kernel_size = 2\n",
        "        self.n_block = 2\n",
        "        # self.seq_length = seq_length\n",
        "\n",
        "        #Input layer\n",
        "        self.in_net = nn.Conv1d(\n",
        "            in_dim, n_dim, 7, stride=2, padding=3, dilation=1)\n",
        "        self.add_module('in_net', self.in_net)\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.dilations = []\n",
        "        for j in range(self.n_layer):\n",
        "            dilation = 1\n",
        "            for i in range(self.n_block):\n",
        "              dilation = 2 * dilation\n",
        "              self.dilations.append(dilation)\n",
        "              self.layers.append(ResidualBlock(self.n_dim, dilation))\n",
        "\n",
        "        self.out_net = nn.Linear(n_dim*4, out_dim)\n",
        "\n",
        "        #Projector\n",
        "        self.project_norm = project_norm\n",
        "        if is_projector:\n",
        "            if project_norm == 'BN':\n",
        "                self.projector = nn.Sequential(\n",
        "                    nn.BatchNorm1d(out_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim, out_dim * 2),\n",
        "                    nn.BatchNorm1d(out_dim * 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim * 2, out_dim)\n",
        "                )\n",
        "            elif project_norm == 'LN':\n",
        "                self.projector = nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.LayerNorm(out_dim),\n",
        "                    nn.Linear(out_dim, out_dim * 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.LayerNorm(out_dim * 2),\n",
        "                    nn.Linear(out_dim * 2, out_dim)\n",
        "                )\n",
        "            else:\n",
        "                self.projector = nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim, out_dim * 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(out_dim * 2, out_dim)\n",
        "                )\n",
        "        self.dummy = nn.Parameter(torch.empty(0))\n",
        "\n",
        "\n",
        "    def forward(self, ts, normalize=True, to_numpy=False):\n",
        "        device = self.dummy.device\n",
        "        is_projector = self.is_projector\n",
        "        ts = _normalize_t(ts, normalize)\n",
        "        ts = ts.to(device, dtype=torch.float32)\n",
        "        ts_emb = self.in_net(ts)\n",
        "        for i in range(self.n_block*self.n_layer):\n",
        "            ts_emb = self.layers[i](ts_emb)\n",
        "\n",
        "        ts_emb = self.out_net(ts_emb)\n",
        "        if is_projector:\n",
        "            ts_emb = self.projector(ts_emb)\n",
        "\n",
        "        if to_numpy:\n",
        "            return ts_emb.cpu().detach().numpy()\n",
        "        else:\n",
        "            return ts_emb\n",
        "\n",
        "    def encode(self, ts, normalize=True, to_numpy=False):\n",
        "        return self.forward(ts, normalize=normalize, to_numpy=to_numpy)\n",
        "\n",
        "\n",
        "\n",
        "def get_tcn(model_config):\n",
        "    in_dim = int(model_config['encoder']['in_dim'])\n",
        "    out_dim = int(model_config['encoder']['out_dim'])\n",
        "    n_layer = int(model_config['encoder']['n_layer'])\n",
        "    n_dim = int(model_config['encoder']['n_dim'])\n",
        "    is_projector = model_config['encoder']['is_projector']\n",
        "    is_projector = is_projector.lower() == 'true'\n",
        "    project_norm = model_config['encoder']['project_norm']\n",
        "\n",
        "    encoder = TemporalConvNet(in_dim=in_dim, out_dim=out_dim, n_layer=n_layer, n_dim=n_dim,\n",
        "                              is_projector=is_projector,project_norm=project_norm)\n",
        "    encoder = load_pretrain(model_config['encoder'], encoder)\n",
        "    return encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdDdqEypgZ8l"
      },
      "source": [
        "### Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_bBfMBRgZNb"
      },
      "outputs": [],
      "source": [
        "# Hierarchial Loss\n",
        "class HierContrastLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.5, temporal_unit=0):\n",
        "        super(HierContrastLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.temporal_unit = temporal_unit\n",
        "\n",
        "    def forward(self, data_i, data_j):\n",
        "        alpha = self.alpha\n",
        "        temporal_unit = self.temporal_unit\n",
        "        data_i = data_i.transpose(1, 2)\n",
        "        data_j = data_j.transpose(1, 2)\n",
        "        loss = hierarchical_contrastive_loss(\n",
        "            data_i, data_j, alpha=alpha, temporal_unit=temporal_unit)\n",
        "        return loss\n",
        "\n",
        "\n",
        "def hierarchical_contrastive_loss(z1, z2, alpha=0.5, temporal_unit=0):\n",
        "    loss = torch.tensor(0., device=z1.device)\n",
        "    d = 0\n",
        "    while z1.size(1) > 1:\n",
        "        if alpha != 0:\n",
        "            loss += alpha * instance_contrastive_loss(z1, z2)\n",
        "        if d >= temporal_unit:\n",
        "            if 1 - alpha != 0:\n",
        "                loss += (1 - alpha) * temporal_contrastive_loss(z1, z2)\n",
        "        d += 1\n",
        "        z1 = F.max_pool1d(z1.transpose(1, 2), kernel_size=2).transpose(1, 2)\n",
        "        z2 = F.max_pool1d(z2.transpose(1, 2), kernel_size=2).transpose(1, 2)\n",
        "    if z1.size(1) == 1:\n",
        "        if alpha != 0:\n",
        "            loss += alpha * instance_contrastive_loss(z1, z2)\n",
        "        d += 1\n",
        "    return loss / d\n",
        "\n",
        "\n",
        "def instance_contrastive_loss(z1, z2):\n",
        "    B, T = z1.size(0), z1.size(1)\n",
        "    if B == 1:\n",
        "        return z1.new_tensor(0.)\n",
        "    z = torch.cat([z1, z2], dim=0)  # 2B x T x C\n",
        "    z = z.transpose(0, 1)  # T x 2B x C\n",
        "    sim = torch.matmul(z, z.transpose(1, 2))  # T x 2B x 2B\n",
        "    logits = torch.tril(sim, diagonal=-1)[:, :, :-1]    # T x 2B x (2B-1)\n",
        "    logits += torch.triu(sim, diagonal=1)[:, :, 1:]\n",
        "    logits = -F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    i = torch.arange(B, device=z1.device)\n",
        "    loss = (logits[:, i, B + i - 1].mean() + logits[:, B + i, i].mean()) / 2\n",
        "    return loss\n",
        "\n",
        "\n",
        "def temporal_contrastive_loss(z1, z2):\n",
        "    B, T = z1.size(0), z1.size(1)\n",
        "    if T == 1:\n",
        "        return z1.new_tensor(0.)\n",
        "    z = torch.cat([z1, z2], dim=1)  # B x 2T x C\n",
        "    sim = torch.matmul(z, z.transpose(1, 2))  # B x 2T x 2T\n",
        "    logits = torch.tril(sim, diagonal=-1)[:, :, :-1]    # B x 2T x (2T-1)\n",
        "    logits += torch.triu(sim, diagonal=1)[:, :, 1:]\n",
        "    logits = -F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    t = torch.arange(T, device=z1.device)\n",
        "    loss = (logits[:, t, T + t - 1].mean() + logits[:, T + t, t].mean()) / 2\n",
        "    return loss\n",
        "\n",
        "# MixUp Loss\n",
        "class MixupLoss(nn.Module):\n",
        "    def __init__(self, tau=0.5):\n",
        "        super(MixupLoss, self).__init__()\n",
        "        self.tau = tau\n",
        "\n",
        "    def forward(self, ts_emb_0, ts_emb_1, ts_emb_aug, lam):\n",
        "        batch_size = ts_emb_0.size()[0]\n",
        "        device = ts_emb_0.device\n",
        "\n",
        "        tau = self.tau\n",
        "\n",
        "        ts_emb_0 = nn.functional.normalize(ts_emb_0)\n",
        "        ts_emb_1 = nn.functional.normalize(ts_emb_1)\n",
        "        ts_emb_aug = nn.functional.normalize(ts_emb_aug)\n",
        "\n",
        "        labels_lam_0 = lam * torch.eye(batch_size)\n",
        "        labels_lam_1 = (1 - lam) * torch.eye(batch_size)\n",
        "        labels = torch.cat((labels_lam_0, labels_lam_1), 1)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits = torch.cat((torch.mm(ts_emb_aug, ts_emb_0.T),\n",
        "                            torch.mm(ts_emb_aug, ts_emb_1.T)), 1)\n",
        "        loss = _cross_entropy(logits / tau, labels)\n",
        "        return loss\n",
        "\n",
        "\n",
        "def _cross_entropy(logits, labels):\n",
        "    logits = nn.LogSoftmax(dim=1)(logits)\n",
        "    loss = torch.mean(torch.sum(-labels * logits, 1))\n",
        "    return loss\n",
        "\n",
        "#NTXent Loss\n",
        "\n",
        "def _dot_similarity(x):\n",
        "    return torch.mm(x, x.T)\n",
        "\n",
        "\n",
        "def _cosine_similarity(x):\n",
        "    return torch.nn.CosineSimilarity(dim=-1)(\n",
        "        x.unsqueeze(1), x.unsqueeze(0))\n",
        "\n",
        "\n",
        "def _get_mask(batch_size, device):\n",
        "    diag_0 = np.eye(2 * batch_size)\n",
        "    diag_1 = np.eye(2 * batch_size, k=-batch_size)\n",
        "    diag_2 = np.eye(2 * batch_size, k=batch_size)\n",
        "\n",
        "    mask = diag_0 + diag_1 + diag_2\n",
        "    mask = 1 - mask\n",
        "    mask = torch.from_numpy(mask)\n",
        "    mask = mask.to(device, dtype=torch.bool)\n",
        "    return mask\n",
        "\n",
        "\n",
        "class NTXentLossPoly(nn.Module):\n",
        "    def __init__(self, temperature=0.2, is_cosine=True):\n",
        "        r\"\"\"\n",
        "        modified from the implementation of NTXentLoss_poly from\n",
        "        https://github.com/mims-harvard/TFC-pretraining\n",
        "        \"\"\"\n",
        "        super(NTXentLossPoly, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.is_cosine = is_cosine\n",
        "\n",
        "    def _get_similarity(self, data):\n",
        "        is_cosine = self.is_cosine\n",
        "        if is_cosine:\n",
        "            return _cosine_similarity(data)\n",
        "        return _dot_similarity(data)\n",
        "\n",
        "    def forward(self, data_i, data_j):\n",
        "        batch_size = data_i.size()[0]\n",
        "        device = data_i.device\n",
        "\n",
        "        data = torch.cat((data_i, data_j, ), dim=0)\n",
        "        similarity = self._get_similarity(data)\n",
        "\n",
        "        positive_upper = torch.diag(similarity, batch_size)\n",
        "        positive_lower = torch.diag(similarity, -batch_size)\n",
        "        positive = torch.cat((positive_upper, positive_lower, ), dim=0)\n",
        "        positive = positive.unsqueeze(1)\n",
        "\n",
        "        negative_mask = _get_mask(batch_size, device)\n",
        "        negative = similarity[negative_mask].view(\n",
        "            2 * batch_size, 2 * batch_size - 2)\n",
        "\n",
        "        logits = torch.cat((positive, negative), dim=1)\n",
        "        logits = logits / self.temperature\n",
        "\n",
        "        labels = torch.zeros(2 * batch_size)\n",
        "        labels = labels.to(device, dtype=torch.long)\n",
        "        cross_entropy = nn.CrossEntropyLoss(reduction='sum')(logits, labels)\n",
        "\n",
        "        labels_onthot = torch.zeros((2 * batch_size, 2 * batch_size - 1))\n",
        "        labels_onthot[:, 0] = 1\n",
        "        labels_onthot = labels_onthot.to(device)\n",
        "        poly_loss = torch.mean(labels_onthot * nn.Softmax(dim=-1)(logits))\n",
        "\n",
        "        loss = (cross_entropy / (2 * batch_size) +\n",
        "                batch_size * (1 / batch_size - poly_loss))\n",
        "        return loss\n",
        "\n",
        "\n",
        "class NTXentLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.2, is_cosine=True):\n",
        "        r\"\"\"\n",
        "        modified from the implementation of NTXentLoss from\n",
        "        https://github.com/mims-harvard/TFC-pretraining\n",
        "        \"\"\"\n",
        "        super(NTXentLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.is_cosine = is_cosine\n",
        "\n",
        "    def _get_similarity(self, data):\n",
        "        is_cosine = self.is_cosine\n",
        "        if is_cosine:\n",
        "            return _cosine_similarity(data)\n",
        "        return _dot_similarity(data)\n",
        "\n",
        "    def forward(self, data_i, data_j):\n",
        "        batch_size = data_i.size()[0]\n",
        "        device = data_i.device\n",
        "\n",
        "        data = torch.cat((data_i, data_j, ), dim=0)\n",
        "        similarity = self._get_similarity(data)\n",
        "\n",
        "        positive_upper = torch.diag(similarity, batch_size)\n",
        "        positive_lower = torch.diag(similarity, -batch_size)\n",
        "        positive = torch.cat((positive_upper, positive_lower, ), dim=0)\n",
        "        positive = positive.unsqueeze(1)\n",
        "\n",
        "        negative_mask = _get_mask(batch_size, device)\n",
        "        negative = similarity[negative_mask].view(\n",
        "            2 * batch_size, 2 * batch_size - 2)\n",
        "\n",
        "        logits = torch.cat((positive, negative), dim=1)\n",
        "        logits = logits / self.temperature\n",
        "\n",
        "        labels = torch.zeros(2 * batch_size)\n",
        "        labels = labels.to(device, dtype=torch.long)\n",
        "        loss = nn.CrossEntropyLoss(reduction='sum')(logits, labels)\n",
        "        return loss / (2 * batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_qlPjIW8hod"
      },
      "source": [
        "### Model Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0jhh8oqwM3i"
      },
      "outputs": [],
      "source": [
        "def load_pretrain(model_config, encoder):\n",
        "    if 'pre_train_model' not in model_config:\n",
        "        return encoder\n",
        "\n",
        "    pre_train_model = model_config['pre_train_model']\n",
        "    pkl = torch.load(pre_train_model, map_location='cpu')\n",
        "    encoder.load_state_dict(pkl['model_state_dict'])\n",
        "    return encoder\n",
        "\n",
        "def get_model(model_config):\n",
        "    model_name = model_config['model']['model_name']\n",
        "    print(f'get model for {model_name}')\n",
        "\n",
        "    if model_config['encoder']['in_dim'] == None:\n",
        "        model_config['encoder']['in_dim'] = model_config['in_dim']\n",
        "\n",
        "    if 'rnnet' in model_name:\n",
        "        print('  get rnnet')\n",
        "        encoder = get_rnnet(model_config)\n",
        "    elif 'resnet1d' in model_name:\n",
        "        print('  get resnet1d')\n",
        "        encoder = get_resnet1d(model_config)\n",
        "    elif 'transform' in model_name:\n",
        "        print('  get transform')\n",
        "        encoder = get_transformer(model_config)\n",
        "    elif 'wavenet' in model_name:\n",
        "        print('  get wavenet')\n",
        "        encoder = get_wavenet(model_config)\n",
        "    else:\n",
        "        raise Exception(f'unknown encoder name: {model_name}')\n",
        "\n",
        "    if 'timefreq' in model_name:\n",
        "        print('  get timefreq')\n",
        "        encoder = get_timefreq(model_config, encoder)\n",
        "    elif 'ts2vec' in model_name:\n",
        "        print('  get ts2vec')\n",
        "        encoder = get_ts2vec(model_config, encoder)\n",
        "    elif 'mixup' in model_name:\n",
        "        print('  get mixup')\n",
        "        encoder = get_mixup(model_config, encoder)\n",
        "    elif 'simclr' in model_name:\n",
        "        print('  get simclr')\n",
        "        encoder = get_simclr(model_config, encoder)\n",
        "    elif 'timeclr' in model_name:\n",
        "        print('  get timeclr')\n",
        "        encoder = get_timeclr(model_config, encoder)\n",
        "\n",
        "    if 'classifier' in model_name:\n",
        "        print('  get classifier')\n",
        "        model_config['classifier']['n_class'] = model_config['n_class']\n",
        "        model = get_classifier(model_config, encoder)\n",
        "    else:\n",
        "        model = encoder\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS45wFgxdWkE"
      },
      "source": [
        "# Config Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KwVD74QaHdt"
      },
      "outputs": [],
      "source": [
        "def write_config(config_dict, config_path):\n",
        "    config_parser = configparser.ConfigParser()\n",
        "    for key in config_dict:\n",
        "        config_parser[key] = config_dict[key]\n",
        "\n",
        "    with open(config_path, 'w') as f:\n",
        "        config_parser.write(f)\n",
        "\n",
        "def get_dataset(ucr_dir):\n",
        "    config_dict = {}\n",
        "    config_dict['data'] = {}\n",
        "    config_dict['data']['data_dir'] = ucr_dir\n",
        "    config_dict['data']['max_len'] = 512\n",
        "    config_dict['data']['seed'] = 666\n",
        "    config_dict['data']['pretrain_frac'] = 0.5\n",
        "    config_dict['data']['train_frac'] = 0.3\n",
        "    config_dict['data']['valid_frac'] = 0.1\n",
        "    config_dict['data']['test_frac'] = 0.1\n",
        "    config_dict['data']['is_same_length'] = 'True'\n",
        "\n",
        "    config_path = os.path.join(\n",
        "        drive_path, 'config_files', 'ucr_00.config')\n",
        "    write_config(config_dict, config_path)\n",
        "\n",
        "def get_dist_classifier():\n",
        "    config_dict = {}\n",
        "    config_dict['model'] = {'metric': 'ed'}\n",
        "    config_path = os.path.join(\n",
        "        drive_path, 'config_files', 'dist_0000.config')\n",
        "    write_config(config_dict, config_path)\n",
        "\n",
        "    config_dict = {}\n",
        "    config_dict['model'] = {'metric': 'dtw'}\n",
        "    config_path = os.path.join(\n",
        "        drive_path, 'config_files', 'dist_0001.config')\n",
        "    write_config(config_dict, config_path)\n",
        "\n",
        "\n",
        "def get_gru_setting():\n",
        "    encoder = {}\n",
        "    encoder['in_dim'] = 'None'\n",
        "    encoder['out_dim'] = 128\n",
        "    encoder['rnn_type'] = 'GRU'\n",
        "    encoder['n_layer'] = 2\n",
        "    encoder['n_dim'] = 64\n",
        "    encoder['is_projector'] = 'False'\n",
        "    encoder['project_norm'] = 'None'\n",
        "    encoder['dropout'] = 0.0\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def get_lst_setting():\n",
        "    encoder = {}\n",
        "    encoder['in_dim'] = 'None'\n",
        "    encoder['out_dim'] = 128\n",
        "    encoder['rnn_type'] = 'LSTM'\n",
        "    encoder['n_layer'] = 2\n",
        "    encoder['n_dim'] = 64\n",
        "    encoder['is_projector'] = 'False'\n",
        "    encoder['project_norm'] = 'None'\n",
        "    encoder['dropout'] = 0.0\n",
        "    return encoder\n",
        "\n",
        "def get_alst_setting():\n",
        "    encoder = {}\n",
        "    encoder['in_dim'] = 'None'\n",
        "    encoder['out_dim'] = 128\n",
        "    encoder['rnn_type'] = 'ALSTM'\n",
        "    encoder['n_layer'] = 2\n",
        "    encoder['n_dim'] = 64\n",
        "    encoder['is_projector'] = 'False'\n",
        "    encoder['project_norm'] = 'None'\n",
        "    encoder['dropout'] = 0.0\n",
        "    encoder['seq_len'] = 512\n",
        "    return encoder\n",
        "\n",
        "def get_agru_setting():\n",
        "    encoder = {}\n",
        "    encoder['in_dim'] = 'None'\n",
        "    encoder['out_dim'] = 128\n",
        "    encoder['rnn_type'] = 'AGRU'\n",
        "    encoder['n_layer'] = 2\n",
        "    encoder['n_dim'] = 64\n",
        "    encoder['is_projector'] = 'False'\n",
        "    encoder['project_norm'] = 'None'\n",
        "    encoder['dropout'] = 0.0\n",
        "    encoder['seq_len'] = 512\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def get_r1d_setting():\n",
        "    encoder = {}\n",
        "    encoder['in_dim'] = 'None'\n",
        "    encoder['out_dim'] = 128\n",
        "    encoder['n_dim'] = 64\n",
        "    encoder['block_type'] = 'alternative'\n",
        "    encoder['norm'] = 'None'\n",
        "    encoder['is_projector'] = 'False'\n",
        "    encoder['project_norm'] = 'None'\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def get_trf_setting():\n",
        "    encoder = {}\n",
        "    encoder['in_dim'] = 'None'\n",
        "    encoder['out_dim'] = 128\n",
        "    encoder['n_layer'] = 4\n",
        "    encoder['n_dim'] = 64\n",
        "    encoder['n_head'] = 8\n",
        "    encoder['norm_first'] = 'True'\n",
        "    encoder['is_pos'] = 'True'\n",
        "    encoder['is_projector'] = 'False'\n",
        "    encoder['project_norm'] = 'None'\n",
        "    encoder['dropout'] = 0.0\n",
        "    return encoder\n",
        "\n",
        "def get_wvnt_setting():\n",
        "    encoder = {}\n",
        "    encoder['in_dim'] = 'None'\n",
        "    encoder['out_dim'] = 128\n",
        "    encoder['n_dim'] = 64\n",
        "    encoder['n_layer'] = 4\n",
        "    encoder['norm'] = 'None'\n",
        "    encoder['is_projector'] = 'False'\n",
        "    encoder['project_norm'] = 'None'\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def get_timefreq_setting():\n",
        "    timefreq = {}\n",
        "    timefreq['jitter_strength'] = 0.1\n",
        "    timefreq['freq_ratio'] = 0.1\n",
        "    timefreq['freq_strength'] = 0.1\n",
        "    timefreq['project_norm'] = 'None'\n",
        "    return timefreq\n",
        "\n",
        "\n",
        "def get_ts2vec_setting():\n",
        "    return {'ph': 0}\n",
        "\n",
        "\n",
        "def get_mixup_setting():\n",
        "    return {'ph': 0}\n",
        "\n",
        "\n",
        "def get_simclr_setting():\n",
        "    return {'ph': 0}\n",
        "\n",
        "\n",
        "def get_timeclr_setting():\n",
        "    timeclr = {}\n",
        "    timeclr['aug_bank_ver'] = 0\n",
        "    return timeclr\n",
        "\n",
        "\n",
        "def get_classifier_setting():\n",
        "    classifier = {}\n",
        "    classifier['n_dim'] = 64\n",
        "    classifier['n_layer'] = 2\n",
        "    return classifier\n",
        "\n",
        "\n",
        "def get_train_setting():\n",
        "    train = {}\n",
        "    train['lr'] = 0.001\n",
        "    train['batch_size'] = 64\n",
        "    train['n_epoch'] = 250\n",
        "    train['n_ckpt'] = 50\n",
        "    return train\n",
        "\n",
        "def get_base_classifier():\n",
        "    norm = 'LN'\n",
        "\n",
        "    prefix = 'gru_c'\n",
        "    config_id = 0\n",
        "\n",
        "    config_dict = {}\n",
        "    config_dict['model'] = {'model_name': 'classifier_rnnet'}\n",
        "    config_dict['classifier'] = get_classifier_setting()\n",
        "    config_dict['encoder'] = get_gru_setting()\n",
        "    config_dict['train'] = get_train_setting()\n",
        "\n",
        "    config_dict['encoder']['is_projector'] = 'True'\n",
        "    config_dict['encoder']['project_norm'] = norm\n",
        "\n",
        "    config_path = os.path.join(\n",
        "        drive_path, 'config_files', f'{prefix}_{config_id:04d}.config')\n",
        "\n",
        "    config_id += 1\n",
        "    write_config(config_dict, config_path)\n",
        "\n",
        "    prefix = 'agru_c'\n",
        "    config_id = 0\n",
        "\n",
        "    config_dict = {}\n",
        "    config_dict['model'] = {'model_name': 'classifier_rnnet'}\n",
        "    config_dict['classifier'] = get_classifier_setting()\n",
        "    config_dict['encoder'] = get_agru_setting()\n",
        "    config_dict['train'] = get_train_setting()\n",
        "\n",
        "    config_dict['encoder']['is_projector'] = 'True'\n",
        "    config_dict['encoder']['project_norm'] = norm\n",
        "\n",
        "    config_path = os.path.join(\n",
        "        drive_path, 'config_files', f'{prefix}_{config_id:04d}.config')\n",
        "\n",
        "    config_id += 1\n",
        "    write_config(config_dict, config_path)\n",
        "\n",
        "    prefix = 'alst_c'\n",
        "    config_id = 0\n",
        "\n",
        "    config_dict = {}\n",
        "    config_dict['model'] = {'model_name': 'classifier_rnnet'}\n",
        "    config_dict['classifier'] = get_classifier_setting()\n",
        "    config_dict['encoder'] = get_alst_setting()\n",
        "    config_dict['train'] = get_train_setting()\n",
        "\n",
        "    config_dict['encoder']['is_projector'] = 'True'\n",
        "    config_dict['encoder']['project_norm'] = norm\n",
        "\n",
        "    config_path = os.path.join(\n",
        "        drive_path, 'config_files', f'{prefix}_{config_id:04d}.config')\n",
        "\n",
        "    config_id += 1\n",
        "    write_config(config_dict, config_path)\n",
        "\n",
        "    prefix = 'lst_c'\n",
        "    config_id = 0\n",
        "\n",
        "    config_dict = {}\n",
        "    config_dict['model'] = {'model_name': 'classifier_rnnet'}\n",
        "    config_dict['classifier'] = get_classifier_setting()\n",
        "    config_dict['encoder'] = get_lst_setting()\n",
        "    config_dict['train'] = get_train_setting()\n",
        "\n",
        "    config_dict['encoder']['is_projector'] = 'True'\n",
        "    config_dict['encoder']['project_norm'] = norm\n",
        "\n",
        "    config_path = os.path.join(\n",
        "        drive_path, 'config_files', f'{prefix}_{config_id:04d}.config')\n",
        "\n",
        "    config_id += 1\n",
        "    write_config(config_dict, config_path)\n",
        "\n",
        "    prefix = 'r1d_c'\n",
        "    config_id = 0\n",
        "\n",
        "    config_dict = {}\n",
        "    config_dict['model'] = {'model_name': 'classifier_resnet1d'}\n",
        "    config_dict['classifier'] = get_classifier_setting()\n",
        "    config_dict['encoder'] = get_r1d_setting()\n",
        "    config_dict['train'] = get_train_setting()\n",
        "\n",
        "    config_dict['encoder']['is_projector'] = 'True'\n",
        "    config_dict['encoder']['project_norm'] = norm\n",
        "    config_dict['encoder']['norm'] = norm\n",
        "\n",
        "    config_path = os.path.join(\n",
        "        drive_path, 'config_files', f'{prefix}_{config_id:04d}.config')\n",
        "\n",
        "    config_id += 1\n",
        "    write_config(config_dict, config_path)\n",
        "\n",
        "    prefix = 'trf_c'\n",
        "    config_id = 0\n",
        "\n",
        "    config_dict = {}\n",
        "    config_dict['model'] = {'model_name': 'classifier_transform'}\n",
        "    config_dict['classifier'] = get_classifier_setting()\n",
        "    config_dict['encoder'] = get_trf_setting()\n",
        "    config_dict['train'] = get_train_setting()\n",
        "\n",
        "    config_dict['encoder']['is_projector'] = 'True'\n",
        "    config_dict['encoder']['project_norm'] = norm\n",
        "\n",
        "    config_path = os.path.join(\n",
        "        drive_path, 'config_files', f'{prefix}_{config_id:04d}.config')\n",
        "\n",
        "    config_id += 1\n",
        "    write_config(config_dict, config_path)\n",
        "\n",
        "    prefix = 'wvnt_c'\n",
        "    config_id = 0\n",
        "\n",
        "    config_dict = {}\n",
        "    config_dict['model'] = {'model_name': 'classifier_wavenet'}\n",
        "    config_dict['classifier'] = get_classifier_setting()\n",
        "    config_dict['encoder'] = get_wvnt_setting()\n",
        "    config_dict['train'] = get_train_setting()\n",
        "\n",
        "    config_dict['encoder']['is_projector'] = 'True'\n",
        "    config_dict['encoder']['project_norm'] = norm\n",
        "\n",
        "    config_path = os.path.join(\n",
        "        drive_path, 'config_files', f'{prefix}_{config_id:04d}.config')\n",
        "\n",
        "    config_id += 1\n",
        "    write_config(config_dict, config_path)\n",
        "\n",
        "\n",
        "def _get_pretrain_setting(short_name, setting_fun, setting_str,\n",
        "                          pretrain_setting):\n",
        "    prefix = f'{short_name}_{pretrain_setting[0]}'\n",
        "    model_name = f'{pretrain_setting[1]}_{setting_str}'\n",
        "\n",
        "    config_id = 0\n",
        "    batch_size = 256\n",
        "    norm = 'LN'\n",
        "    config_dict = {}\n",
        "    config_dict['model'] = {'model_name': model_name}\n",
        "\n",
        "    config_dict['encoder'] = setting_fun()\n",
        "    if 'norm' in config_dict['encoder']:\n",
        "        config_dict['encoder']['norm'] = norm\n",
        "    config_dict['encoder']['in_dim'] = 1\n",
        "\n",
        "    if pretrain_setting[1] == 'timefreq':\n",
        "        if 'out_dim' in config_dict['encoder']:\n",
        "            config_dict['encoder']['out_dim'] = int(\n",
        "                config_dict['encoder']['out_dim'] / 2)\n",
        "        if 'n_dim' in config_dict['encoder']:\n",
        "            config_dict['encoder']['n_dim'] = int(\n",
        "                config_dict['encoder']['n_dim'] / 2)\n",
        "        config_dict['timefreq'] = get_timefreq_setting()\n",
        "        config_dict['timefreq']['project_norm'] = norm\n",
        "\n",
        "    elif pretrain_setting[1] == 'ts2vec':\n",
        "        config_dict['ts2vec'] = get_ts2vec_setting()\n",
        "        config_dict['encoder']['is_projector'] = 'True'\n",
        "        config_dict['encoder']['project_norm'] = norm\n",
        "\n",
        "    elif pretrain_setting[1] == 'mixup':\n",
        "        config_dict['mixup'] = get_mixup_setting()\n",
        "        config_dict['encoder']['is_projector'] = 'True'\n",
        "        config_dict['encoder']['project_norm'] = norm\n",
        "\n",
        "    elif pretrain_setting[1] == 'simclr':\n",
        "        config_dict['simclr'] = get_simclr_setting()\n",
        "        config_dict['encoder']['is_projector'] = 'True'\n",
        "        config_dict['encoder']['project_norm'] = norm\n",
        "\n",
        "    elif pretrain_setting[1] == 'timeclr':\n",
        "        config_dict['timeclr'] = get_timeclr_setting()\n",
        "        config_dict['encoder']['is_projector'] = 'True'\n",
        "        config_dict['encoder']['project_norm'] = norm\n",
        "\n",
        "    config_dict['train'] = get_train_setting()\n",
        "    # config_dict['train']['n_ckpt'] = 50\n",
        "    config_dict['train']['batch_size'] = batch_size\n",
        "\n",
        "    config_path = os.path.join(\n",
        "        drive_path, 'config_files', f'{prefix}_{config_id:04d}.config')\n",
        "\n",
        "    config_id += 1\n",
        "    write_config(config_dict, config_path)\n",
        "\n",
        "\n",
        "def _get_classifier_setting(short_name, setting_fun, setting_str,\n",
        "                            pretrain_setting):\n",
        "    prefix = f'{short_name}_{pretrain_setting[0]}_c'\n",
        "    model_name = f'classifier_{pretrain_setting[1]}_{setting_str}'\n",
        "\n",
        "    config_id = 0\n",
        "    batch_size = 256\n",
        "    norm = 'LN'\n",
        "    pretrain_data = 'ucr_00_pretrain'\n",
        "    config_dict = {}\n",
        "    config_dict['model'] = {'model_name': model_name}\n",
        "\n",
        "    config_dict['classifier'] = get_classifier_setting()\n",
        "\n",
        "    config_dict['encoder'] = setting_fun()\n",
        "    if 'norm' in config_dict['encoder']:\n",
        "        config_dict['encoder']['norm'] = norm\n",
        "    config_dict['encoder']['in_dim'] = 1\n",
        "\n",
        "    if pretrain_setting[1] == 'timefreq':\n",
        "        if 'out_dim' in config_dict['encoder']:\n",
        "            config_dict['encoder']['out_dim'] = int(\n",
        "                config_dict['encoder']['out_dim'] / 2)\n",
        "        if 'n_dim' in config_dict['encoder']:\n",
        "            config_dict['encoder']['n_dim'] = int(\n",
        "                config_dict['encoder']['n_dim'] / 2)\n",
        "        config_dict['timefreq'] = get_timefreq_setting()\n",
        "        config_dict['timefreq']['project_norm'] = norm\n",
        "        config_dict['timefreq']['pre_train_model'] = (\n",
        "            os.path.join(\n",
        "                drive_path, 'model', pretrain_data,\n",
        "                f'{short_name}_tf_0000_0249.npz'))\n",
        "\n",
        "    elif pretrain_setting[1] == 'ts2vec':\n",
        "        config_dict['ts2vec'] = get_ts2vec_setting()\n",
        "        config_dict['encoder']['is_projector'] = 'True'\n",
        "        config_dict['encoder']['project_norm'] = norm\n",
        "        config_dict['ts2vec']['pre_train_model'] = (\n",
        "            os.path.join(\n",
        "                drive_path, 'model', pretrain_data,\n",
        "                f'{short_name}_tv_0000_0249.npz'))\n",
        "\n",
        "    elif pretrain_setting[1] == 'mixup':\n",
        "        config_dict['mixup'] = get_mixup_setting()\n",
        "        config_dict['encoder']['is_projector'] = 'True'\n",
        "        config_dict['encoder']['project_norm'] = norm\n",
        "        config_dict['mixup']['pre_train_model'] = (\n",
        "            os.path.join(\n",
        "                drive_path, 'model', pretrain_data,\n",
        "                f'{short_name}_mu_0000_0249.npz'))\n",
        "\n",
        "    elif pretrain_setting[1] == 'simclr':\n",
        "        config_dict['simclr'] = get_simclr_setting()\n",
        "        config_dict['encoder']['is_projector'] = 'True'\n",
        "        config_dict['encoder']['project_norm'] = norm\n",
        "        config_dict['simclr']['pre_train_model'] = (\n",
        "            os.path.join(\n",
        "                drive_path, 'model', pretrain_data,\n",
        "                f'{short_name}_sc_0000_0249.npz'))\n",
        "\n",
        "    elif pretrain_setting[1] == 'timeclr':\n",
        "        config_dict['timeclr'] = get_timeclr_setting()\n",
        "        config_dict['encoder']['is_projector'] = 'True'\n",
        "        config_dict['encoder']['project_norm'] = norm\n",
        "        config_dict['timeclr']['pre_train_model'] = (\n",
        "            os.path.join(\n",
        "                drive_path, 'model', pretrain_data,\n",
        "                f'{short_name}_tc_0000_0249.npz'))\n",
        "\n",
        "    config_dict['train'] = get_train_setting()\n",
        "    config_path = os.path.join(\n",
        "        drive_path, 'config_files', f'{prefix}_{config_id:04d}.config')\n",
        "\n",
        "    config_id += 1\n",
        "    write_config(config_dict, config_path)\n",
        "\n",
        "\n",
        "def get_pretrain_model():\n",
        "    pretrain_settings = [\n",
        "        ['tf', 'timefreq', ],\n",
        "        ['tv', 'ts2vec', ],\n",
        "        ['mu', 'mixup', ],\n",
        "        ['sc', 'simclr', ],\n",
        "        ['tc', 'timeclr', ],\n",
        "    ]\n",
        "\n",
        "    for pretrain_setting in pretrain_settings:\n",
        "        _get_pretrain_setting(\n",
        "            'agru', get_agru_setting,\n",
        "            'rnnet', pretrain_setting)\n",
        "        _get_pretrain_setting(\n",
        "            'gru', get_gru_setting,\n",
        "            'rnnet', pretrain_setting)\n",
        "        _get_pretrain_setting(\n",
        "            'lst', get_lst_setting,\n",
        "            'rnnet', pretrain_setting)\n",
        "        _get_pretrain_setting(\n",
        "            'alst', get_alst_setting,\n",
        "            'rnnet', pretrain_setting)\n",
        "        _get_pretrain_setting(\n",
        "            'r1d', get_r1d_setting,\n",
        "            'resnet1d', pretrain_setting)\n",
        "        _get_pretrain_setting(\n",
        "            'trf', get_trf_setting,\n",
        "            'transform', pretrain_setting)\n",
        "        _get_pretrain_setting(\n",
        "            'wvnt', get_wvnt_setting,\n",
        "            'wavenet', pretrain_setting)\n",
        "\n",
        "    for pretrain_setting in pretrain_settings:\n",
        "        _get_classifier_setting(\n",
        "            'agru', get_agru_setting,\n",
        "            'rnnet', pretrain_setting)\n",
        "        _get_classifier_setting(\n",
        "            'gru', get_gru_setting,\n",
        "            'rnnet', pretrain_setting)\n",
        "        _get_classifier_setting(\n",
        "            'lst', get_lst_setting,\n",
        "            'rnnet', pretrain_setting)\n",
        "        _get_classifier_setting(\n",
        "            'alst', get_alst_setting,\n",
        "            'rnnet', pretrain_setting)\n",
        "        _get_classifier_setting(\n",
        "            'r1d', get_r1d_setting,\n",
        "            'resnet1d', pretrain_setting)\n",
        "        _get_classifier_setting(\n",
        "            'trf', get_trf_setting,\n",
        "            'transform', pretrain_setting)\n",
        "        _get_classifier_setting(\n",
        "            'wvnt', get_wvnt_setting,\n",
        "            'wavenet', pretrain_setting)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W1Onpb9hKhm"
      },
      "outputs": [],
      "source": [
        "ucr_dir = os.path.join(drive_path, 'UCRArchive_2018')\n",
        "config_dir = os.path.join(drive_path, 'config_files')\n",
        "\n",
        "path = pathlib.Path(config_dir)\n",
        "path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "get_dataset(ucr_dir)\n",
        "get_dist_classifier()\n",
        "get_base_classifier()\n",
        "get_pretrain_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZoDEqHduoZP"
      },
      "source": [
        "# Pretraining Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UaSOTykEAYZ"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WD_wEJkD_O9"
      },
      "outputs": [],
      "source": [
        "def parse_config(config_path, verbose=True):\n",
        "    \"\"\"method for parsing configs\"\"\"\n",
        "    parser = configparser.ConfigParser()\n",
        "    parser.read(config_path)\n",
        "    if verbose:\n",
        "        print(config_path)\n",
        "    config_dict = OrderedDict()\n",
        "    for key_0 in parser:\n",
        "        config_dict[key_0] = OrderedDict()\n",
        "        for key_1 in parser[key_0]:\n",
        "            val = parser[key_0][key_1]\n",
        "            if val == 'None':\n",
        "                val = None\n",
        "            config_dict[key_0][key_1] = val\n",
        "            if verbose:\n",
        "                print(f'  {key_0}.{key_1}={val}')\n",
        "    return config_dict\n",
        "\n",
        "def _get_checkpoint(n_ckpt, n_epoch):\n",
        "    if n_ckpt >= n_epoch:\n",
        "        ckpts = np.arange(n_epoch)\n",
        "    else:\n",
        "        ckpts = np.arange(1, n_ckpt + 1)\n",
        "        ckpts = n_epoch * (ckpts / n_ckpt) - 1\n",
        "    ckpts = ckpts.astype(int)\n",
        "    ckpts_dict = {}\n",
        "    for ckpt in ckpts:\n",
        "        ckpts_dict[ckpt] = 0\n",
        "\n",
        "    last_ckpt = n_epoch - 1\n",
        "    if last_ckpt not in ckpts_dict:\n",
        "        ckpts_dict[last_ckpt] = 0\n",
        "    return ckpts_dict\n",
        "\n",
        "def _get_start_epoch(model_path, ckpts):\n",
        "    start_epoch = 0\n",
        "    for i in ckpts:\n",
        "        model_path_i = model_path.format(i)\n",
        "        if os.path.isfile(model_path_i):\n",
        "            start_epoch = i\n",
        "\n",
        "    model_path_i = model_path.format(start_epoch)\n",
        "    if not os.path.isfile(model_path_i):\n",
        "        return start_epoch\n",
        "\n",
        "    try:\n",
        "        pkl = torch.load(model_path_i, map_location='cpu')\n",
        "    except:\n",
        "        print(f'{model_path_i} can not be opened. It is removed!')\n",
        "        os.remove(model_path_i)\n",
        "        start_epoch = _get_start_epoch(model_path, ckpts)\n",
        "    return start_epoch\n",
        "\n",
        "def _timefreq_encoder_forward(model, idx_batch, data):\n",
        "    data_batch = data[idx_batch, :, :]\n",
        "    h_t, z_t, h_f, z_f = model.forward(\n",
        "        data_batch, normalize=False, to_numpy=False, is_augment=False)\n",
        "    h_t_aug, z_t_aug, h_f_aug, z_f_aug = model.forward(\n",
        "        data_batch, normalize=False, to_numpy=False, is_augment=True)\n",
        "    loss_fun = NTXentLossPoly()\n",
        "    loss_t = loss_fun(h_t, h_t_aug)\n",
        "    loss_f = loss_fun(h_f, h_f_aug)\n",
        "    loss_tf = loss_fun(z_t, z_f)\n",
        "    loss = 0.2 * (loss_t + loss_f) + loss_tf\n",
        "    return loss\n",
        "\n",
        "\n",
        "def _simclr_encoder_forward(model, idx_batch, data):\n",
        "    data_batch = data[idx_batch, :, :]\n",
        "    ts_emb_aug_0 = model.forward(\n",
        "        data_batch, normalize=False, to_numpy=False, is_augment=True)\n",
        "    ts_emb_aug_1 = model.forward(\n",
        "        data_batch, normalize=False, to_numpy=False, is_augment=True)\n",
        "    loss_fun = NTXentLoss()\n",
        "    loss = loss_fun(ts_emb_aug_0, ts_emb_aug_1)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def _timeclr_encoder_forward(model, idx_batch, data):\n",
        "    data_batch = data[idx_batch, :, :]\n",
        "    ts_emb_aug_0 = model.forward(\n",
        "        data_batch, normalize=False, to_numpy=False, is_augment=True)\n",
        "    ts_emb_aug_1 = model.forward(\n",
        "        data_batch, normalize=False, to_numpy=False, is_augment=True)\n",
        "    loss_fun = NTXentLossPoly()\n",
        "    loss = loss_fun(ts_emb_aug_0, ts_emb_aug_1)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def _ts2vec_encoder_forward(model, idx_batch, data):\n",
        "    data_batch = data[idx_batch, :, :]\n",
        "    ts_emb_l, ts_emb_r = model.forward(\n",
        "        data_batch, normalize=False, to_numpy=False, is_augment=True)\n",
        "    loss_fun = HierContrastLoss()\n",
        "    loss = loss_fun(ts_emb_l, ts_emb_r)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def _mixup_encoder_forward(model, idx_batch, data):\n",
        "    data_batch = data[idx_batch, :, :]\n",
        "    ts_emb_0, ts_emb_1, ts_emb_aug, lam = model.forward(\n",
        "        data_batch, normalize=False, to_numpy=False, is_augment=True)\n",
        "    loss_fun = MixupLoss()\n",
        "    loss = loss_fun(ts_emb_0, ts_emb_1, ts_emb_aug, lam)\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg5ZlxS-eXbN"
      },
      "source": [
        "### Pretraining Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wqOQbDeD1-k"
      },
      "outputs": [],
      "source": [
        "def nn_pretrain(data, model, model_path, train_config, device):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    pretrain_name = model.pretrain_name\n",
        "    lr = float(train_config['lr'])\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', verbose=True)\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    batch_size = int(train_config['batch_size'])\n",
        "    n_iter = np.ceil(n_data / batch_size)\n",
        "    n_iter = int(n_iter)\n",
        "    n_epoch = int(train_config['n_epoch'])\n",
        "    n_ckpt = int(train_config['n_ckpt'])\n",
        "\n",
        "    ckpts = _get_checkpoint(n_ckpt, n_epoch)\n",
        "    start_epoch = _get_start_epoch(model_path, ckpts)\n",
        "\n",
        "    loss_train = np.zeros(n_epoch)\n",
        "    toc_train = np.zeros(n_epoch)\n",
        "    for i in range(start_epoch, n_epoch):\n",
        "        if start_epoch != 0 and i == start_epoch:\n",
        "            print(f'resume training from epoch {i + 1:d}')\n",
        "        model_path_i = model_path.format(i)\n",
        "        if os.path.isfile(model_path_i):\n",
        "            print(f'loading {model_path_i}')\n",
        "            pkl = torch.load(model_path_i, map_location='cpu')\n",
        "            loss_train = pkl['loss_train']\n",
        "            toc_train = pkl['toc_train']\n",
        "            loss_epoch = loss_train[i]\n",
        "            toc_epoch = toc_train[i]\n",
        "\n",
        "            model.load_state_dict(\n",
        "                pkl['model_state_dict'])\n",
        "            model.to(device)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.load_state_dict(\n",
        "                pkl['optimizer_state_dict'])\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer, mode='min', verbose=True)\n",
        "            print((f'epoch {i + 1}/{n_epoch}, '\n",
        "                   f'loss={loss_epoch:0.4f}, '\n",
        "                   f'time={toc_epoch:0.2f}.'))\n",
        "            continue\n",
        "\n",
        "        model_state_dict_old = copy.deepcopy(\n",
        "            model.state_dict())\n",
        "        optimizer_state_dict_old = copy.deepcopy(\n",
        "            optimizer.state_dict())\n",
        "        while True:\n",
        "            tic = time.time()\n",
        "            loss_epoch = 0\n",
        "            idx_order = np.random.permutation(n_data)\n",
        "            for j in range(n_iter):\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                idx_start = j * batch_size\n",
        "                idx_end = (j + 1) * batch_size\n",
        "                if idx_end > n_data:\n",
        "                    idx_end = n_data\n",
        "                idx_batch = idx_order[idx_start:idx_end]\n",
        "\n",
        "                batch_size_ = idx_end - idx_start\n",
        "                if batch_size_ < batch_size:\n",
        "                    n_fill = batch_size - batch_size_\n",
        "                    idx_fill = idx_order[:n_fill]\n",
        "                    idx_batch = np.concatenate(\n",
        "                        (idx_batch, idx_fill, ), axis=0)\n",
        "\n",
        "                pretrain_name = model.pretrain_name\n",
        "                if pretrain_name == 'timefreq':\n",
        "                    loss = _timefreq_encoder_forward(\n",
        "                        model, idx_batch, data)\n",
        "                elif pretrain_name == 'ts2vec':\n",
        "                    loss = _ts2vec_encoder_forward(\n",
        "                        model, idx_batch, data)\n",
        "                elif pretrain_name == 'mixup':\n",
        "                    loss = _mixup_encoder_forward(\n",
        "                        model, idx_batch, data)\n",
        "                elif pretrain_name == 'simclr':\n",
        "                    loss = _simclr_encoder_forward(\n",
        "                        model, idx_batch, data)\n",
        "                elif pretrain_name == 'timeclr':\n",
        "                    loss = _timeclr_encoder_forward(\n",
        "                        model, idx_batch, data)\n",
        "                else:\n",
        "                    raise Exception(\n",
        "                        f'unknown pretrain name: {pretrain_name}')\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                loss_epoch += loss.item()\n",
        "\n",
        "            loss_epoch /= n_iter\n",
        "            toc_epoch = time.time() - tic\n",
        "\n",
        "            loss_train[i] = loss_epoch\n",
        "            toc_train[i] = toc_epoch\n",
        "\n",
        "            if i in ckpts or i == n_epoch-1:\n",
        "                pkl = {}\n",
        "                pkl['loss_train'] = loss_train\n",
        "                pkl['toc_train'] = toc_train\n",
        "                pkl['model_state_dict'] = model.state_dict()\n",
        "                pkl['optimizer_state_dict'] = optimizer.state_dict()\n",
        "                torch.save(pkl, model_path_i)\n",
        "\n",
        "            print((f'epoch {i + 1}/{n_epoch}, '\n",
        "                   f'loss={loss_epoch:0.4f}, '\n",
        "                   f'time={toc_epoch:0.2f}.'))\n",
        "\n",
        "            if np.isfinite(loss_epoch):\n",
        "                break\n",
        "            else:\n",
        "                print('restart model training...')\n",
        "                model.load_state_dict(\n",
        "                    model_state_dict_old)\n",
        "                model.to(device)\n",
        "                model.train()\n",
        "\n",
        "                optimizer.load_state_dict(\n",
        "                    optimizer_state_dict_old)\n",
        "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                    optimizer, mode='min', verbose=True)\n",
        "\n",
        "        scheduler.step(loss_epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz4pgT8qdv3k"
      },
      "outputs": [],
      "source": [
        "def pretrain_ucr(data_config_name, method_name):\n",
        "    data_config = os.path.join(\n",
        "        drive_path, 'config_files', f'{data_config_name}.config')\n",
        "    data_config = parse_config(data_config)\n",
        "\n",
        "    method_config = os.path.join(\n",
        "        drive_path, 'config_files', f'{method_name}.config')\n",
        "    method_config = parse_config(method_config)\n",
        "\n",
        "    model_dir = os.path.join(\n",
        "        drive_path, 'model', f'{data_config_name}_pretrain')\n",
        "    path = pathlib.Path(model_dir)\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    fmt_str = '{0:04d}'\n",
        "    model_path = os.path.join(\n",
        "        model_dir, f'{method_name}_{fmt_str}.npz')\n",
        "\n",
        "    dataset = load_dataset(data_config)\n",
        "    method_config['in_dim'] = dataset.shape[1]\n",
        "    method_config['data_len'] = dataset.shape[2]\n",
        "    model = get_model(method_config)\n",
        "    nn_pretrain(dataset, model, model_path,\n",
        "                method_config['train'], device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "id": "MofmQWk_eoYB",
        "outputId": "367fba1b-f2bf-4196-a9cb-f8827c2bc223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/mnt/drive/MyDrive/STAT940/config_files/ucr_00.config\n",
            "  data.data_dir=/mnt/drive/MyDrive/STAT940/UCRArchive_2018\n",
            "  data.max_len=512\n",
            "  data.seed=666\n",
            "  data.pretrain_frac=0.5\n",
            "  data.train_frac=0.3\n",
            "  data.valid_frac=0.1\n",
            "  data.test_frac=0.1\n",
            "  data.is_same_length=True\n",
            "/mnt/drive/MyDrive/STAT940/config_files/tcn_sc_0000.config\n",
            "  model.model_name=simclr_temporalnet\n",
            "  encoder.in_dim=1\n",
            "  encoder.out_dim=128\n",
            "  encoder.n_layer=1\n",
            "  encoder.n_dim=64\n",
            "  encoder.norm=LN\n",
            "  encoder.is_projector=True\n",
            "  encoder.project_norm=LN\n",
            "  simclr.ph=0\n",
            "  train.lr=0.001\n",
            "  train.batch_size=256\n",
            "  train.n_epoch=250\n",
            "  train.n_ckpt=100\n",
            "get model for simclr_temporalnet\n",
            "  get temporalnet\n",
            "  get simclr\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 15.77 GiB of which 6.63 GiB is free. Process 17307 has 9.14 GiB memory in use. Of the allocated memory 8.73 GiB is allocated by PyTorch, and 38.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-0f7c61ecf277>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ucr_00'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmethod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tcn_sc_0000'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpretrain_ucr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-2f200ee3d358>\u001b[0m in \u001b[0;36mpretrain_ucr\u001b[0;34m(data_config_name, method_name)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmethod_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_len'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     nn_pretrain(dataset, model, model_path,\n\u001b[0m\u001b[1;32m     24\u001b[0m                 method_config['train'], device)\n",
            "\u001b[0;32m<ipython-input-21-a2436123b604>\u001b[0m in \u001b[0;36mnn_pretrain\u001b[0;34m(data, model, model_path, train_config, device)\u001b[0m\n\u001b[1;32m     83\u001b[0m                         model, idx_batch, data)\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpretrain_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'simclr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     loss = _simclr_encoder_forward(\n\u001b[0m\u001b[1;32m     86\u001b[0m                         model, idx_batch, data)\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpretrain_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'timeclr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-04976f07c600>\u001b[0m in \u001b[0;36m_simclr_encoder_forward\u001b[0;34m(model, idx_batch, data)\u001b[0m\n\u001b[1;32m     73\u001b[0m         data_batch, normalize=False, to_numpy=False, is_augment=True)\n\u001b[1;32m     74\u001b[0m     \u001b[0mloss_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNTXentLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts_emb_aug_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_emb_aug_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-fe872c97a199>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data_i, data_j)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mpositive_upper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-fe872c97a199>\u001b[0m in \u001b[0;36m_get_similarity\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mis_cosine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cosine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_cosine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_dot_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-fe872c97a199>\u001b[0m in \u001b[0;36m_cosine_similarity\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     return torch.nn.CosineSimilarity(dim=-1)(\n\u001b[0m\u001b[1;32m    107\u001b[0m         x.unsqueeze(1), x.unsqueeze(0))\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/distance.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 15.77 GiB of which 6.63 GiB is free. Process 17307 has 9.14 GiB memory in use. Of the allocated memory 8.73 GiB is allocated by PyTorch, and 38.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# SimCLR + transformer -> trf_sc_0000\n",
        "# SimCLR + ResNet1D -> r1d_sc_0000\n",
        "\n",
        "data_name = 'ucr_00'\n",
        "method_name = 'tcn_sc_0000'\n",
        "pretrain_ucr(data_name, method_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqA2Gu0VjeCF"
      },
      "source": [
        "# Fine-tuning / Testing Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2o_pT6BkHcI"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5evByte7kQmd"
      },
      "outputs": [],
      "source": [
        "def nn_train(dataset, model, model_path,\n",
        "             train_config, device):\n",
        "    data = dataset['data_train']\n",
        "    label = dataset['label_train']\n",
        "\n",
        "    data = _normalize_dataset(data)\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    lr = float(train_config['lr'])\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=lr)\n",
        "\n",
        "    n_data = data.shape[0]\n",
        "    batch_size = int(train_config['batch_size'])\n",
        "    n_iter = np.ceil(n_data / batch_size)\n",
        "    n_iter = int(n_iter)\n",
        "    n_epoch = int(train_config['n_epoch'])\n",
        "    n_ckpt = int(train_config['n_ckpt'])\n",
        "\n",
        "    ckpts = _get_checkpoint(n_ckpt, n_epoch)\n",
        "    start_epoch = _get_start_epoch(model_path, ckpts)\n",
        "\n",
        "    loss_train = np.zeros(n_epoch)\n",
        "    toc_train = np.zeros(n_epoch)\n",
        "    for i in range(start_epoch, n_epoch):\n",
        "        if start_epoch != 0 and i == start_epoch:\n",
        "            print(f'resume training from epoch {i + 1:d}')\n",
        "        model_path_i = model_path.format(i)\n",
        "        if os.path.isfile(model_path_i):\n",
        "            print(f'loading {model_path_i}')\n",
        "            pkl = torch.load(model_path_i, map_location='cpu')\n",
        "            loss_train = pkl['loss_train']\n",
        "            toc_train = pkl['toc_train']\n",
        "            loss_epoch = loss_train[i]\n",
        "            toc_epoch = toc_train[i]\n",
        "\n",
        "            model.load_state_dict(\n",
        "                pkl['model_state_dict'])\n",
        "            model.to(device)\n",
        "            model.train()\n",
        "\n",
        "            optimizer.load_state_dict(\n",
        "                pkl['optimizer_state_dict'])\n",
        "            print((f'epoch {i + 1}/{n_epoch}, '\n",
        "                   f'loss={loss_epoch:0.4f}, '\n",
        "                   f'time={toc_epoch:0.2f}.'))\n",
        "            continue\n",
        "\n",
        "        tic = time.time()\n",
        "        loss_epoch = 0\n",
        "        idx_order = np.random.permutation(n_data)\n",
        "        for j in range(n_iter):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            idx_start = j * batch_size\n",
        "            idx_end = (j + 1) * batch_size\n",
        "            if idx_end > n_data:\n",
        "                idx_end = n_data\n",
        "            idx_batch = idx_order[idx_start:idx_end]\n",
        "\n",
        "            batch_size_ = idx_end - idx_start\n",
        "            if batch_size_ < batch_size:\n",
        "                n_fill = batch_size - batch_size_\n",
        "                idx_fill = idx_order[:n_fill]\n",
        "                idx_batch = np.concatenate(\n",
        "                    (idx_batch, idx_fill, ), axis=0)\n",
        "\n",
        "            data_batch = data[idx_batch, :, :]\n",
        "            label_batch = label[idx_batch]\n",
        "\n",
        "            label_batch = torch.from_numpy(label_batch)\n",
        "            label_batch = label_batch.to(device, dtype=torch.long)\n",
        "\n",
        "            logit = model.forward(\n",
        "                data_batch, normalize=False, to_numpy=False)\n",
        "\n",
        "            loss = nn.CrossEntropyLoss()(logit, label_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_epoch += loss.item()\n",
        "\n",
        "        loss_epoch /= n_iter\n",
        "        toc_epoch = time.time() - tic\n",
        "\n",
        "        loss_train[i] = loss_epoch\n",
        "        toc_train[i] = toc_epoch\n",
        "        if i in ckpts:\n",
        "            pkl = {}\n",
        "            pkl['loss_train'] = loss_train\n",
        "            pkl['toc_train'] = toc_train\n",
        "            pkl['model_state_dict'] = model.state_dict()\n",
        "            pkl['optimizer_state_dict'] = optimizer.state_dict()\n",
        "            torch.save(pkl, model_path_i)\n",
        "\n",
        "        print((f'epoch {i + 1}/{n_epoch}, '\n",
        "               f'loss={loss_epoch:0.4f}, '\n",
        "               f'time={toc_epoch:0.2f}.'))\n",
        "\n",
        "def _get_predict(data, label, model, train_config):\n",
        "    n_data = data.shape[0]\n",
        "\n",
        "    batch_size = int(train_config['batch_size'])\n",
        "    n_iter = np.ceil(n_data / batch_size)\n",
        "    n_iter = int(n_iter)\n",
        "\n",
        "    tic = time.time()\n",
        "    predict = np.zeros(n_data, dtype=int)\n",
        "    for i in range(n_iter):\n",
        "        idx_start = i * batch_size\n",
        "        idx_end = (i + 1) * batch_size\n",
        "        if idx_end > n_data:\n",
        "            idx_end = n_data\n",
        "\n",
        "        data_batch = data[idx_start:idx_end, :, :]\n",
        "        logit = model.forward(\n",
        "            data_batch, normalize=False, to_numpy=True)\n",
        "        predict[idx_start:idx_end] = np.argmax(logit, axis=1)\n",
        "    predict_time = time.time() - tic\n",
        "    acc = np.sum(predict == label) / n_data\n",
        "    return predict, acc, predict_time\n",
        "\n",
        "def nn_eval(dataset, model, model_path, result_path, train_config, device):\n",
        "    data_valid = dataset['data_valid']\n",
        "    data_test = dataset['data_test']\n",
        "\n",
        "    label_valid = dataset['label_valid']\n",
        "    label_test = dataset['label_test']\n",
        "\n",
        "    data_valid = _normalize_dataset(data_valid)\n",
        "    data_test = _normalize_dataset(data_test)\n",
        "\n",
        "    n_epoch = int(train_config['n_epoch'])\n",
        "    n_ckpt = int(train_config['n_ckpt'])\n",
        "    ckpts = _get_checkpoint(n_ckpt, n_epoch)\n",
        "    ckpts = [ckpt for ckpt in ckpts]\n",
        "    ckpts = ckpts[::-1]\n",
        "    shuffle(ckpts)\n",
        "\n",
        "    for i in ckpts:\n",
        "        result_path_i = result_path.format(i)\n",
        "        if not os.path.isfile(result_path_i):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            result = np.load(result_path_i, allow_pickle=True)\n",
        "        except:\n",
        "            print(f'{result_path_i} can not be opened. It is removed!')\n",
        "            os.remove(result_path_i)\n",
        "\n",
        "    for i in ckpts:\n",
        "        model_path_i = model_path.format(i)\n",
        "        result_path_i = result_path.format(i)\n",
        "        if os.path.isfile(result_path_i):\n",
        "            result = np.load(result_path_i, allow_pickle=True)\n",
        "            acc_valid = result['acc_valid']\n",
        "            acc_test = result['acc_test']\n",
        "            time_valid = result['time_valid']\n",
        "            time_test = result['time_test']\n",
        "            print((f'{result_path_i}, {acc_valid:0.4f}, {acc_test:0.4f}, '\n",
        "                  f'{time_valid+time_test:0.2f}'))\n",
        "            continue\n",
        "\n",
        "        pkl = torch.load(model_path_i, map_location='cpu')\n",
        "        model.load_state_dict(\n",
        "            pkl['model_state_dict'])\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        predict_valid, acc_valid, time_valid = _get_predict(\n",
        "            data_valid, label_valid, model, train_config)\n",
        "        predict_test, acc_test, time_test = _get_predict(\n",
        "            data_test, label_test, model, train_config)\n",
        "\n",
        "        np.savez(result_path_i,\n",
        "                 label_valid=label_valid,\n",
        "                 label_test=label_test,\n",
        "                 predict_valid=predict_valid,\n",
        "                 predict_test=predict_test,\n",
        "                 acc_valid=acc_valid,\n",
        "                 acc_test=acc_test,\n",
        "                 time_valid=time_valid,\n",
        "                 time_test=time_test)\n",
        "        print((f'{result_path_i}, {acc_valid:0.4f}, {acc_test:0.4f}, '\n",
        "               f'{time_valid+time_test:0.2f}'))\n",
        "\n",
        "def get_agg_result(result_path, result_agg_path, train_config):\n",
        "    \"\"\"method for aggregating results\"\"\"\n",
        "    if os.path.isfile(result_agg_path):\n",
        "        pkl = np.load(result_agg_path)\n",
        "        epoch = pkl['epoch']\n",
        "        return epoch\n",
        "\n",
        "    n_epoch = int(train_config['n_epoch'])\n",
        "    acc_valid_bsf = 0\n",
        "    for i in range(n_epoch):\n",
        "        result_path_ = result_path.format(i)\n",
        "        if not os.path.isfile(result_path_):\n",
        "            return\n",
        "\n",
        "        pkl = np.load(result_path_)\n",
        "        acc_valid = pkl['acc_valid']\n",
        "        acc_test = pkl['acc_test']\n",
        "\n",
        "        if acc_valid_bsf == 0:\n",
        "            acc_valid_bsf = acc_valid\n",
        "            acc_test_bsf = acc_test\n",
        "            epoch_bsf = i\n",
        "        elif acc_valid > acc_valid_bsf:\n",
        "            acc_valid_bsf = acc_valid\n",
        "            acc_test_bsf = acc_test\n",
        "            epoch_bsf = i\n",
        "    np.savez(result_agg_path,\n",
        "             acc_valid=acc_valid_bsf,\n",
        "             acc_test=acc_test_bsf,\n",
        "             epoch=epoch_bsf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAn3CkhjluMh"
      },
      "source": [
        "### Fine tuning Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99uvzM9Ce7Vq"
      },
      "outputs": [],
      "source": [
        "def fine_tune(data_config_name, method_name, dataset_order = 1):\n",
        "\n",
        "    data_config = os.path.join(\n",
        "        drive_path, 'config_files', f'{data_config_name}.config')\n",
        "    data_config = parse_config(data_config)\n",
        "    method_config = os.path.join(\n",
        "        drive_path, 'config_files', f'{method_name}.config')\n",
        "    method_config = parse_config(method_config)\n",
        "    dataset_names = get_ucr_data_names()\n",
        "    if dataset_order == -1:\n",
        "        dataset_names = dataset_names[::-1]\n",
        "    elif dataset_order == 0:\n",
        "        shuffle(dataset_names)\n",
        "    if torch.cuda.is_available():\n",
        "        device = 'cuda'\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "\n",
        "    fmt_str = '{0:04d}'\n",
        "    for dataset_name in dataset_names:\n",
        "        result_dir = os.path.join(\n",
        "            drive_path, 'result', f'{data_config_name}_{dataset_name}')\n",
        "        result_agg_dir = os.path.join(\n",
        "            drive_path, 'result_agg', f'{data_config_name}_{dataset_name}')\n",
        "        model_dir = os.path.join(\n",
        "            drive_path, 'model', f'{data_config_name}_{dataset_name}')\n",
        "\n",
        "        path = pathlib.Path(result_dir)\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "        path = pathlib.Path(result_agg_dir)\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "        path = pathlib.Path(model_dir)\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        result_path = os.path.join(\n",
        "            result_dir, f'{method_name}_{fmt_str}.npz')\n",
        "        result_agg_path = os.path.join(\n",
        "            result_agg_dir, f'{method_name}.npz')\n",
        "        model_path = os.path.join(\n",
        "            model_dir, f'{method_name}_{fmt_str}.npz')\n",
        "        if os.path.isfile(result_agg_path):\n",
        "            continue\n",
        "\n",
        "        dataset = load_ucr_dataset(dataset_name, data_config)\n",
        "        method_config_ = copy.deepcopy(method_config)\n",
        "        method_config_['in_dim'] = dataset['n_dim']\n",
        "        method_config_['n_class'] = dataset['n_class']\n",
        "        method_config_['data_len'] = dataset['data_len']\n",
        "        model = get_model(method_config_)\n",
        "        nn_train(dataset, model, model_path,\n",
        "                 method_config_['train'], device)\n",
        "        nn_eval(dataset, model, model_path,\n",
        "                result_path, method_config_['train'], device)\n",
        "        get_agg_result(result_path, result_agg_path,\n",
        "                       method_config_['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s21wb_vEuBcx",
        "outputId": "9cf0e76e-5ae8-45aa-833f-6b93ed2d6901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "epoch 165/250, loss=0.0001, time=0.02.\n",
            "epoch 166/250, loss=0.0001, time=0.02.\n",
            "epoch 167/250, loss=0.0001, time=0.02.\n",
            "epoch 168/250, loss=0.0001, time=0.02.\n",
            "epoch 169/250, loss=0.0001, time=0.02.\n",
            "epoch 170/250, loss=0.0001, time=0.02.\n",
            "epoch 171/250, loss=0.0001, time=0.02.\n",
            "epoch 172/250, loss=0.0001, time=0.02.\n",
            "epoch 173/250, loss=0.0001, time=0.02.\n",
            "epoch 174/250, loss=0.0001, time=0.02.\n",
            "epoch 175/250, loss=0.0001, time=0.02.\n",
            "epoch 176/250, loss=0.0001, time=0.02.\n",
            "epoch 177/250, loss=0.0001, time=0.02.\n",
            "epoch 178/250, loss=0.0001, time=0.02.\n",
            "epoch 179/250, loss=0.0001, time=0.02.\n",
            "epoch 180/250, loss=0.0001, time=0.02.\n",
            "epoch 181/250, loss=0.0001, time=0.02.\n",
            "epoch 182/250, loss=0.0001, time=0.02.\n",
            "epoch 183/250, loss=0.0000, time=0.02.\n",
            "epoch 184/250, loss=0.0001, time=0.02.\n",
            "epoch 185/250, loss=0.0001, time=0.02.\n",
            "epoch 186/250, loss=0.0001, time=0.02.\n",
            "epoch 187/250, loss=0.0000, time=0.02.\n",
            "epoch 188/250, loss=0.0001, time=0.02.\n",
            "epoch 189/250, loss=0.0001, time=0.02.\n",
            "epoch 190/250, loss=0.0000, time=0.02.\n",
            "epoch 191/250, loss=0.0001, time=0.02.\n",
            "epoch 192/250, loss=0.0001, time=0.02.\n",
            "epoch 193/250, loss=0.0000, time=0.02.\n",
            "epoch 194/250, loss=0.0000, time=0.02.\n",
            "epoch 195/250, loss=0.0000, time=0.02.\n",
            "epoch 196/250, loss=0.0000, time=0.02.\n",
            "epoch 197/250, loss=0.0000, time=0.02.\n",
            "epoch 198/250, loss=0.0000, time=0.02.\n",
            "epoch 199/250, loss=0.0000, time=0.02.\n",
            "epoch 200/250, loss=0.0000, time=0.02.\n",
            "epoch 201/250, loss=0.0000, time=0.02.\n",
            "epoch 202/250, loss=0.0000, time=0.02.\n",
            "epoch 203/250, loss=0.0000, time=0.02.\n",
            "epoch 204/250, loss=0.0000, time=0.02.\n",
            "epoch 205/250, loss=0.0000, time=0.02.\n",
            "epoch 206/250, loss=0.0000, time=0.02.\n",
            "epoch 207/250, loss=0.0000, time=0.02.\n",
            "epoch 208/250, loss=0.0000, time=0.02.\n",
            "epoch 209/250, loss=0.0000, time=0.02.\n",
            "epoch 210/250, loss=0.0000, time=0.02.\n",
            "epoch 211/250, loss=0.0000, time=0.02.\n",
            "epoch 212/250, loss=0.0000, time=0.02.\n",
            "epoch 213/250, loss=0.0000, time=0.02.\n",
            "epoch 214/250, loss=0.0000, time=0.02.\n",
            "epoch 215/250, loss=0.0000, time=0.02.\n",
            "epoch 216/250, loss=0.0000, time=0.02.\n",
            "epoch 217/250, loss=0.0000, time=0.02.\n",
            "epoch 218/250, loss=0.0000, time=0.02.\n",
            "epoch 219/250, loss=0.0000, time=0.02.\n",
            "epoch 220/250, loss=0.0000, time=0.02.\n",
            "epoch 221/250, loss=0.0000, time=0.02.\n",
            "epoch 222/250, loss=0.0000, time=0.02.\n",
            "epoch 223/250, loss=0.0000, time=0.02.\n",
            "epoch 224/250, loss=0.0000, time=0.02.\n",
            "epoch 225/250, loss=0.0000, time=0.02.\n",
            "epoch 226/250, loss=0.0000, time=0.02.\n",
            "epoch 227/250, loss=0.0000, time=0.02.\n",
            "epoch 228/250, loss=0.0000, time=0.02.\n",
            "epoch 229/250, loss=0.0000, time=0.02.\n",
            "epoch 230/250, loss=0.0000, time=0.02.\n",
            "epoch 231/250, loss=0.0000, time=0.02.\n",
            "epoch 232/250, loss=0.0000, time=0.02.\n",
            "epoch 233/250, loss=0.0000, time=0.02.\n",
            "epoch 234/250, loss=0.0000, time=0.02.\n",
            "epoch 235/250, loss=0.0000, time=0.02.\n",
            "epoch 236/250, loss=0.0000, time=0.02.\n",
            "epoch 237/250, loss=0.0000, time=0.02.\n",
            "epoch 238/250, loss=0.0000, time=0.02.\n",
            "epoch 239/250, loss=0.0000, time=0.02.\n",
            "epoch 240/250, loss=0.0000, time=0.02.\n",
            "epoch 241/250, loss=0.0000, time=0.02.\n",
            "epoch 242/250, loss=0.0000, time=0.02.\n",
            "epoch 243/250, loss=0.0000, time=0.02.\n",
            "epoch 244/250, loss=0.0000, time=0.02.\n",
            "epoch 245/250, loss=0.0000, time=0.02.\n",
            "epoch 246/250, loss=0.0000, time=0.02.\n",
            "epoch 247/250, loss=0.0000, time=0.02.\n",
            "epoch 248/250, loss=0.0000, time=0.02.\n",
            "epoch 249/250, loss=0.0000, time=0.02.\n",
            "epoch 250/250, loss=0.0000, time=0.02.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0154.npz, 0.7778, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0179.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0159.npz, 0.7778, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0219.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0234.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0194.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0149.npz, 0.7778, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0144.npz, 0.7778, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0174.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0089.npz, 0.7407, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0229.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0044.npz, 0.7407, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0169.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0139.npz, 0.7778, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0034.npz, 0.7037, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0124.npz, 0.7778, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0049.npz, 0.7407, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0004.npz, 0.6296, 0.5185, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0134.npz, 0.7778, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0114.npz, 0.7778, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0209.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0009.npz, 0.7037, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0199.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0014.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0129.npz, 0.7778, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0184.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0119.npz, 0.7778, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0104.npz, 0.7778, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0094.npz, 0.7778, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0069.npz, 0.7407, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0079.npz, 0.7407, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0054.npz, 0.7407, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0059.npz, 0.7407, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0249.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0099.npz, 0.7778, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0019.npz, 0.7037, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0109.npz, 0.7778, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0244.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0084.npz, 0.7407, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0064.npz, 0.7407, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0239.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0214.npz, 0.7778, 0.7037, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0204.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0039.npz, 0.7407, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0024.npz, 0.6667, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0189.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0074.npz, 0.7407, 0.6667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0164.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0224.npz, 0.7778, 0.7037, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_InsectEPGSmallTrain/alst_sc_c_0000_0029.npz, 0.6667, 0.6667, 0.00\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=2.2353, time=0.27.\n",
            "epoch 2/250, loss=1.9059, time=0.18.\n",
            "epoch 3/250, loss=1.6282, time=0.17.\n",
            "epoch 4/250, loss=1.4068, time=0.17.\n",
            "epoch 5/250, loss=1.2431, time=0.17.\n",
            "epoch 6/250, loss=1.0994, time=0.16.\n",
            "epoch 7/250, loss=0.9663, time=0.17.\n",
            "epoch 8/250, loss=0.9410, time=0.17.\n",
            "epoch 9/250, loss=0.8813, time=0.17.\n",
            "epoch 10/250, loss=0.8138, time=0.17.\n",
            "epoch 11/250, loss=0.7727, time=0.17.\n",
            "epoch 12/250, loss=0.7510, time=0.17.\n",
            "epoch 13/250, loss=0.6713, time=0.17.\n",
            "epoch 14/250, loss=0.6485, time=0.17.\n",
            "epoch 15/250, loss=0.7154, time=0.17.\n",
            "epoch 16/250, loss=0.6095, time=0.16.\n",
            "epoch 17/250, loss=0.5219, time=0.17.\n",
            "epoch 18/250, loss=0.5260, time=0.17.\n",
            "epoch 19/250, loss=0.4737, time=0.17.\n",
            "epoch 20/250, loss=0.4805, time=0.17.\n",
            "epoch 21/250, loss=0.5014, time=0.17.\n",
            "epoch 22/250, loss=0.4103, time=0.17.\n",
            "epoch 23/250, loss=0.3815, time=0.17.\n",
            "epoch 24/250, loss=0.4172, time=0.17.\n",
            "epoch 25/250, loss=0.5068, time=0.17.\n",
            "epoch 26/250, loss=0.4560, time=0.16.\n",
            "epoch 27/250, loss=0.4557, time=0.17.\n",
            "epoch 28/250, loss=0.5348, time=0.17.\n",
            "epoch 29/250, loss=0.4826, time=0.17.\n",
            "epoch 30/250, loss=0.3802, time=0.17.\n",
            "epoch 31/250, loss=0.3983, time=0.16.\n",
            "epoch 32/250, loss=0.3152, time=0.17.\n",
            "epoch 33/250, loss=0.3184, time=0.17.\n",
            "epoch 34/250, loss=0.3145, time=0.17.\n",
            "epoch 35/250, loss=0.2522, time=0.17.\n",
            "epoch 36/250, loss=0.2326, time=0.16.\n",
            "epoch 37/250, loss=0.2080, time=0.17.\n",
            "epoch 38/250, loss=0.3174, time=0.17.\n",
            "epoch 39/250, loss=0.2452, time=0.17.\n",
            "epoch 40/250, loss=0.2992, time=0.17.\n",
            "epoch 41/250, loss=0.3188, time=0.17.\n",
            "epoch 42/250, loss=0.2937, time=0.17.\n",
            "epoch 43/250, loss=0.2301, time=0.17.\n",
            "epoch 44/250, loss=0.3280, time=0.17.\n",
            "epoch 45/250, loss=0.3117, time=0.17.\n",
            "epoch 46/250, loss=0.3307, time=0.16.\n",
            "epoch 47/250, loss=0.2419, time=0.17.\n",
            "epoch 48/250, loss=0.1778, time=0.17.\n",
            "epoch 49/250, loss=0.1951, time=0.17.\n",
            "epoch 50/250, loss=0.1924, time=0.17.\n",
            "epoch 51/250, loss=0.1592, time=0.16.\n",
            "epoch 52/250, loss=0.1354, time=0.17.\n",
            "epoch 53/250, loss=0.1619, time=0.17.\n",
            "epoch 54/250, loss=0.1416, time=0.17.\n",
            "epoch 55/250, loss=0.1225, time=0.17.\n",
            "epoch 56/250, loss=0.1056, time=0.16.\n",
            "epoch 57/250, loss=0.0794, time=0.17.\n",
            "epoch 58/250, loss=0.1180, time=0.17.\n",
            "epoch 59/250, loss=0.1255, time=0.17.\n",
            "epoch 60/250, loss=0.1265, time=0.17.\n",
            "epoch 61/250, loss=0.1520, time=0.17.\n",
            "epoch 62/250, loss=0.2950, time=0.17.\n",
            "epoch 63/250, loss=0.2776, time=0.17.\n",
            "epoch 64/250, loss=0.3836, time=0.17.\n",
            "epoch 65/250, loss=0.5865, time=0.17.\n",
            "epoch 66/250, loss=0.3553, time=0.17.\n",
            "epoch 67/250, loss=0.1784, time=0.17.\n",
            "epoch 68/250, loss=0.1359, time=0.17.\n",
            "epoch 69/250, loss=0.1358, time=0.17.\n",
            "epoch 70/250, loss=0.1387, time=0.17.\n",
            "epoch 71/250, loss=0.1305, time=0.17.\n",
            "epoch 72/250, loss=0.1011, time=0.17.\n",
            "epoch 73/250, loss=0.0656, time=0.17.\n",
            "epoch 74/250, loss=0.0715, time=0.17.\n",
            "epoch 75/250, loss=0.0578, time=0.17.\n",
            "epoch 76/250, loss=0.0477, time=0.16.\n",
            "epoch 77/250, loss=0.0322, time=0.17.\n",
            "epoch 78/250, loss=0.0186, time=0.17.\n",
            "epoch 79/250, loss=0.0122, time=0.17.\n",
            "epoch 80/250, loss=0.0094, time=0.17.\n",
            "epoch 81/250, loss=0.0082, time=0.17.\n",
            "epoch 82/250, loss=0.0060, time=0.17.\n",
            "epoch 83/250, loss=0.0063, time=0.17.\n",
            "epoch 84/250, loss=0.0059, time=0.17.\n",
            "epoch 85/250, loss=0.0048, time=0.17.\n",
            "epoch 86/250, loss=0.0046, time=0.17.\n",
            "epoch 87/250, loss=0.0039, time=0.17.\n",
            "epoch 88/250, loss=0.0034, time=0.17.\n",
            "epoch 89/250, loss=0.0032, time=0.17.\n",
            "epoch 90/250, loss=0.0031, time=0.17.\n",
            "epoch 91/250, loss=0.0029, time=0.17.\n",
            "epoch 92/250, loss=0.0030, time=0.18.\n",
            "epoch 93/250, loss=0.0029, time=0.17.\n",
            "epoch 94/250, loss=0.0030, time=0.18.\n",
            "epoch 95/250, loss=0.0031, time=0.17.\n",
            "epoch 96/250, loss=0.0024, time=0.17.\n",
            "epoch 97/250, loss=0.0020, time=0.17.\n",
            "epoch 98/250, loss=0.0018, time=0.17.\n",
            "epoch 99/250, loss=0.0018, time=0.17.\n",
            "epoch 100/250, loss=0.0017, time=0.17.\n",
            "epoch 101/250, loss=0.0016, time=0.17.\n",
            "epoch 102/250, loss=0.0016, time=0.17.\n",
            "epoch 103/250, loss=0.0014, time=0.17.\n",
            "epoch 104/250, loss=0.0014, time=0.17.\n",
            "epoch 105/250, loss=0.0013, time=0.17.\n",
            "epoch 106/250, loss=0.0013, time=0.16.\n",
            "epoch 107/250, loss=0.0012, time=0.17.\n",
            "epoch 108/250, loss=0.0012, time=0.17.\n",
            "epoch 109/250, loss=0.0012, time=0.17.\n",
            "epoch 110/250, loss=0.0011, time=0.17.\n",
            "epoch 111/250, loss=0.0011, time=0.17.\n",
            "epoch 112/250, loss=0.0011, time=0.17.\n",
            "epoch 113/250, loss=0.0010, time=0.17.\n",
            "epoch 114/250, loss=0.0010, time=0.17.\n",
            "epoch 115/250, loss=0.0010, time=0.17.\n",
            "epoch 116/250, loss=0.0009, time=0.16.\n",
            "epoch 117/250, loss=0.0009, time=0.17.\n",
            "epoch 118/250, loss=0.0009, time=0.17.\n",
            "epoch 119/250, loss=0.0008, time=0.17.\n",
            "epoch 120/250, loss=0.0009, time=0.17.\n",
            "epoch 121/250, loss=0.0008, time=0.17.\n",
            "epoch 122/250, loss=0.0008, time=0.17.\n",
            "epoch 123/250, loss=0.0008, time=0.17.\n",
            "epoch 124/250, loss=0.0008, time=0.18.\n",
            "epoch 125/250, loss=0.0008, time=0.17.\n",
            "epoch 126/250, loss=0.0007, time=0.17.\n",
            "epoch 127/250, loss=0.0007, time=0.17.\n",
            "epoch 128/250, loss=0.0007, time=0.18.\n",
            "epoch 129/250, loss=0.0007, time=0.17.\n",
            "epoch 130/250, loss=0.0006, time=0.17.\n",
            "epoch 131/250, loss=0.0006, time=0.17.\n",
            "epoch 132/250, loss=0.0006, time=0.17.\n",
            "epoch 133/250, loss=0.0006, time=0.17.\n",
            "epoch 134/250, loss=0.0006, time=0.17.\n",
            "epoch 135/250, loss=0.0006, time=0.17.\n",
            "epoch 136/250, loss=0.0006, time=0.16.\n",
            "epoch 137/250, loss=0.0005, time=0.17.\n",
            "epoch 138/250, loss=0.0005, time=0.17.\n",
            "epoch 139/250, loss=0.0005, time=0.17.\n",
            "epoch 140/250, loss=0.0005, time=0.17.\n",
            "epoch 141/250, loss=0.0005, time=0.16.\n",
            "epoch 142/250, loss=0.0005, time=0.17.\n",
            "epoch 143/250, loss=0.0005, time=0.17.\n",
            "epoch 144/250, loss=0.0005, time=0.17.\n",
            "epoch 145/250, loss=0.0005, time=0.17.\n",
            "epoch 146/250, loss=0.0005, time=0.17.\n",
            "epoch 147/250, loss=0.0004, time=0.17.\n",
            "epoch 148/250, loss=0.0004, time=0.17.\n",
            "epoch 149/250, loss=0.0004, time=0.17.\n",
            "epoch 150/250, loss=0.0004, time=0.17.\n",
            "epoch 151/250, loss=0.0004, time=0.17.\n",
            "epoch 152/250, loss=0.0004, time=0.17.\n",
            "epoch 153/250, loss=0.0004, time=0.17.\n",
            "epoch 154/250, loss=0.0004, time=0.17.\n",
            "epoch 155/250, loss=0.0004, time=0.18.\n",
            "epoch 156/250, loss=0.0004, time=0.17.\n",
            "epoch 157/250, loss=0.0004, time=0.17.\n",
            "epoch 158/250, loss=0.0004, time=0.17.\n",
            "epoch 159/250, loss=0.0004, time=0.17.\n",
            "epoch 160/250, loss=0.0004, time=0.18.\n",
            "epoch 161/250, loss=0.0004, time=0.17.\n",
            "epoch 162/250, loss=0.0003, time=0.17.\n",
            "epoch 163/250, loss=0.0003, time=0.17.\n",
            "epoch 164/250, loss=0.0003, time=0.17.\n",
            "epoch 165/250, loss=0.0003, time=0.17.\n",
            "epoch 166/250, loss=0.0003, time=0.17.\n",
            "epoch 167/250, loss=0.0003, time=0.17.\n",
            "epoch 168/250, loss=0.0003, time=0.17.\n",
            "epoch 169/250, loss=0.0003, time=0.17.\n",
            "epoch 170/250, loss=0.0003, time=0.17.\n",
            "epoch 171/250, loss=0.0003, time=0.17.\n",
            "epoch 172/250, loss=0.0003, time=0.17.\n",
            "epoch 173/250, loss=0.0003, time=0.17.\n",
            "epoch 174/250, loss=0.0003, time=0.17.\n",
            "epoch 175/250, loss=0.0003, time=0.17.\n",
            "epoch 176/250, loss=0.0003, time=0.17.\n",
            "epoch 177/250, loss=0.0003, time=0.17.\n",
            "epoch 178/250, loss=0.0003, time=0.18.\n",
            "epoch 179/250, loss=0.0003, time=0.17.\n",
            "epoch 180/250, loss=0.0003, time=0.17.\n",
            "epoch 181/250, loss=0.0003, time=0.17.\n",
            "epoch 182/250, loss=0.0003, time=0.17.\n",
            "epoch 183/250, loss=0.0002, time=0.17.\n",
            "epoch 184/250, loss=0.0002, time=0.17.\n",
            "epoch 185/250, loss=0.0002, time=0.17.\n",
            "epoch 186/250, loss=0.0002, time=0.16.\n",
            "epoch 187/250, loss=0.0002, time=0.17.\n",
            "epoch 188/250, loss=0.0002, time=0.17.\n",
            "epoch 189/250, loss=0.0002, time=0.17.\n",
            "epoch 190/250, loss=0.0002, time=0.17.\n",
            "epoch 191/250, loss=0.0002, time=0.16.\n",
            "epoch 192/250, loss=0.0002, time=0.18.\n",
            "epoch 193/250, loss=0.0002, time=0.17.\n",
            "epoch 194/250, loss=0.0002, time=0.18.\n",
            "epoch 195/250, loss=0.0002, time=0.17.\n",
            "epoch 196/250, loss=0.0002, time=0.17.\n",
            "epoch 197/250, loss=0.0002, time=0.17.\n",
            "epoch 198/250, loss=0.0002, time=0.17.\n",
            "epoch 199/250, loss=0.0002, time=0.17.\n",
            "epoch 200/250, loss=0.0002, time=0.17.\n",
            "epoch 201/250, loss=0.0002, time=0.17.\n",
            "epoch 202/250, loss=0.0002, time=0.17.\n",
            "epoch 203/250, loss=0.0002, time=0.17.\n",
            "epoch 204/250, loss=0.0002, time=0.17.\n",
            "epoch 205/250, loss=0.0002, time=0.17.\n",
            "epoch 206/250, loss=0.0002, time=0.17.\n",
            "epoch 207/250, loss=0.0002, time=0.17.\n",
            "epoch 208/250, loss=0.0002, time=0.17.\n",
            "epoch 209/250, loss=0.0002, time=0.17.\n",
            "epoch 210/250, loss=0.0002, time=0.17.\n",
            "epoch 211/250, loss=0.0002, time=0.16.\n",
            "epoch 212/250, loss=0.0002, time=0.17.\n",
            "epoch 213/250, loss=0.0002, time=0.17.\n",
            "epoch 214/250, loss=0.0002, time=0.17.\n",
            "epoch 215/250, loss=0.0002, time=0.17.\n",
            "epoch 216/250, loss=0.0002, time=0.17.\n",
            "epoch 217/250, loss=0.0002, time=0.17.\n",
            "epoch 218/250, loss=0.0002, time=0.18.\n",
            "epoch 219/250, loss=0.0002, time=0.17.\n",
            "epoch 220/250, loss=0.0001, time=0.17.\n",
            "epoch 221/250, loss=0.0001, time=0.17.\n",
            "epoch 222/250, loss=0.0001, time=0.17.\n",
            "epoch 223/250, loss=0.0001, time=0.17.\n",
            "epoch 224/250, loss=0.0001, time=0.17.\n",
            "epoch 225/250, loss=0.0001, time=0.17.\n",
            "epoch 226/250, loss=0.0001, time=0.17.\n",
            "epoch 227/250, loss=0.0001, time=0.17.\n",
            "epoch 228/250, loss=0.0001, time=0.17.\n",
            "epoch 229/250, loss=0.0001, time=0.17.\n",
            "epoch 230/250, loss=0.0001, time=0.17.\n",
            "epoch 231/250, loss=0.0001, time=0.17.\n",
            "epoch 232/250, loss=0.0001, time=0.17.\n",
            "epoch 233/250, loss=0.0001, time=0.17.\n",
            "epoch 234/250, loss=0.0001, time=0.17.\n",
            "epoch 235/250, loss=0.0001, time=0.17.\n",
            "epoch 236/250, loss=0.0001, time=0.17.\n",
            "epoch 237/250, loss=0.0001, time=0.17.\n",
            "epoch 238/250, loss=0.0001, time=0.17.\n",
            "epoch 239/250, loss=0.0001, time=0.17.\n",
            "epoch 240/250, loss=0.0001, time=0.17.\n",
            "epoch 241/250, loss=0.0001, time=0.17.\n",
            "epoch 242/250, loss=0.0001, time=0.17.\n",
            "epoch 243/250, loss=0.0001, time=0.17.\n",
            "epoch 244/250, loss=0.0001, time=0.17.\n",
            "epoch 245/250, loss=0.0001, time=0.17.\n",
            "epoch 246/250, loss=0.0001, time=0.17.\n",
            "epoch 247/250, loss=0.0001, time=0.17.\n",
            "epoch 248/250, loss=0.0001, time=0.17.\n",
            "epoch 249/250, loss=0.0001, time=0.18.\n",
            "epoch 250/250, loss=0.0001, time=0.17.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0249.npz, 0.7298, 0.6936, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0169.npz, 0.7326, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0079.npz, 0.7437, 0.6769, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0239.npz, 0.7298, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0119.npz, 0.7382, 0.6852, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0109.npz, 0.7354, 0.6852, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0054.npz, 0.6852, 0.6462, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0034.npz, 0.7103, 0.6435, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0074.npz, 0.7214, 0.6852, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0099.npz, 0.7354, 0.6797, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0134.npz, 0.7382, 0.6880, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0064.npz, 0.6128, 0.5850, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0124.npz, 0.7409, 0.6852, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0174.npz, 0.7298, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0219.npz, 0.7298, 0.6880, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0164.npz, 0.7326, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0039.npz, 0.6825, 0.6490, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0139.npz, 0.7354, 0.6880, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0224.npz, 0.7298, 0.6880, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0084.npz, 0.7437, 0.6797, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0214.npz, 0.7298, 0.6880, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0199.npz, 0.7298, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0194.npz, 0.7270, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0024.npz, 0.6657, 0.6212, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0209.npz, 0.7298, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0184.npz, 0.7298, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0234.npz, 0.7298, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0009.npz, 0.6407, 0.6128, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0029.npz, 0.6657, 0.6156, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0149.npz, 0.7354, 0.6880, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0179.npz, 0.7326, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0004.npz, 0.5655, 0.5487, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0104.npz, 0.7382, 0.6825, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0094.npz, 0.7465, 0.6797, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0049.npz, 0.6769, 0.6462, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0129.npz, 0.7409, 0.6852, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0069.npz, 0.6852, 0.6574, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0144.npz, 0.7354, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0014.npz, 0.6797, 0.6267, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0229.npz, 0.7298, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0204.npz, 0.7298, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0189.npz, 0.7326, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0244.npz, 0.7298, 0.6936, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0019.npz, 0.6657, 0.6769, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0114.npz, 0.7354, 0.6852, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0089.npz, 0.7382, 0.6769, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0159.npz, 0.7354, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0154.npz, 0.7354, 0.6908, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0059.npz, 0.6797, 0.6852, 0.04\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MelbournePedestrian/alst_sc_c_0000_0044.npz, 0.7242, 0.6685, 0.04\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=1.5095, time=0.20.\n",
            "epoch 2/250, loss=1.1618, time=0.15.\n",
            "epoch 3/250, loss=0.9234, time=0.13.\n",
            "epoch 4/250, loss=0.7949, time=0.13.\n",
            "epoch 5/250, loss=0.7114, time=0.13.\n",
            "epoch 6/250, loss=0.6699, time=0.13.\n",
            "epoch 7/250, loss=0.6301, time=0.13.\n",
            "epoch 8/250, loss=0.6016, time=0.13.\n",
            "epoch 9/250, loss=0.4852, time=0.13.\n",
            "epoch 10/250, loss=0.4341, time=0.14.\n",
            "epoch 11/250, loss=0.4047, time=0.13.\n",
            "epoch 12/250, loss=0.4034, time=0.13.\n",
            "epoch 13/250, loss=0.4086, time=0.13.\n",
            "epoch 14/250, loss=0.3802, time=0.13.\n",
            "epoch 15/250, loss=0.3536, time=0.13.\n",
            "epoch 16/250, loss=0.3383, time=0.13.\n",
            "epoch 17/250, loss=0.3323, time=0.13.\n",
            "epoch 18/250, loss=0.3519, time=0.14.\n",
            "epoch 19/250, loss=0.3345, time=0.13.\n",
            "epoch 20/250, loss=0.2764, time=0.13.\n",
            "epoch 21/250, loss=0.2542, time=0.12.\n",
            "epoch 22/250, loss=0.2561, time=0.13.\n",
            "epoch 23/250, loss=0.2334, time=0.13.\n",
            "epoch 24/250, loss=0.2300, time=0.13.\n",
            "epoch 25/250, loss=0.2782, time=0.14.\n",
            "epoch 26/250, loss=0.2530, time=0.12.\n",
            "epoch 27/250, loss=0.2319, time=0.14.\n",
            "epoch 28/250, loss=0.1998, time=0.13.\n",
            "epoch 29/250, loss=0.2732, time=0.13.\n",
            "epoch 30/250, loss=0.2484, time=0.13.\n",
            "epoch 31/250, loss=0.2271, time=0.12.\n",
            "epoch 32/250, loss=0.2230, time=0.13.\n",
            "epoch 33/250, loss=0.2035, time=0.13.\n",
            "epoch 34/250, loss=0.1866, time=0.13.\n",
            "epoch 35/250, loss=0.2882, time=0.13.\n",
            "epoch 36/250, loss=0.2179, time=0.13.\n",
            "epoch 37/250, loss=0.1545, time=0.14.\n",
            "epoch 38/250, loss=0.1135, time=0.13.\n",
            "epoch 39/250, loss=0.1316, time=0.13.\n",
            "epoch 40/250, loss=0.1768, time=0.13.\n",
            "epoch 41/250, loss=0.1569, time=0.13.\n",
            "epoch 42/250, loss=0.2639, time=0.13.\n",
            "epoch 43/250, loss=0.2835, time=0.13.\n",
            "epoch 44/250, loss=0.2003, time=0.13.\n",
            "epoch 45/250, loss=0.1253, time=0.13.\n",
            "epoch 46/250, loss=0.0838, time=0.13.\n",
            "epoch 47/250, loss=0.0601, time=0.13.\n",
            "epoch 48/250, loss=0.0475, time=0.13.\n",
            "epoch 49/250, loss=0.1022, time=0.13.\n",
            "epoch 50/250, loss=0.1041, time=0.13.\n",
            "epoch 51/250, loss=0.1086, time=0.13.\n",
            "epoch 52/250, loss=0.1437, time=0.13.\n",
            "epoch 53/250, loss=0.2453, time=0.13.\n",
            "epoch 54/250, loss=0.2304, time=0.13.\n",
            "epoch 55/250, loss=0.1265, time=0.13.\n",
            "epoch 56/250, loss=0.1449, time=0.13.\n",
            "epoch 57/250, loss=0.0975, time=0.14.\n",
            "epoch 58/250, loss=0.0884, time=0.13.\n",
            "epoch 59/250, loss=0.1270, time=0.13.\n",
            "epoch 60/250, loss=0.0740, time=0.13.\n",
            "epoch 61/250, loss=0.0595, time=0.13.\n",
            "epoch 62/250, loss=0.0610, time=0.14.\n",
            "epoch 63/250, loss=0.0700, time=0.13.\n",
            "epoch 64/250, loss=0.0493, time=0.13.\n",
            "epoch 65/250, loss=0.0714, time=0.13.\n",
            "epoch 66/250, loss=0.0552, time=0.12.\n",
            "epoch 67/250, loss=0.0462, time=0.13.\n",
            "epoch 68/250, loss=0.0278, time=0.13.\n",
            "epoch 69/250, loss=0.0197, time=0.13.\n",
            "epoch 70/250, loss=0.0163, time=0.13.\n",
            "epoch 71/250, loss=0.0192, time=0.13.\n",
            "epoch 72/250, loss=0.0139, time=0.14.\n",
            "epoch 73/250, loss=0.0154, time=0.13.\n",
            "epoch 74/250, loss=0.0556, time=0.13.\n",
            "epoch 75/250, loss=0.1249, time=0.13.\n",
            "epoch 76/250, loss=0.0946, time=0.13.\n",
            "epoch 77/250, loss=0.0878, time=0.13.\n",
            "epoch 78/250, loss=0.0821, time=0.13.\n",
            "epoch 79/250, loss=0.1792, time=0.13.\n",
            "epoch 80/250, loss=0.1614, time=0.13.\n",
            "epoch 81/250, loss=0.1074, time=0.13.\n",
            "epoch 82/250, loss=0.1014, time=0.13.\n",
            "epoch 83/250, loss=0.0928, time=0.13.\n",
            "epoch 84/250, loss=0.0645, time=0.14.\n",
            "epoch 85/250, loss=0.0349, time=0.13.\n",
            "epoch 86/250, loss=0.0336, time=0.13.\n",
            "epoch 87/250, loss=0.0244, time=0.13.\n",
            "epoch 88/250, loss=0.0488, time=0.13.\n",
            "epoch 89/250, loss=0.0771, time=0.13.\n",
            "epoch 90/250, loss=0.1109, time=0.13.\n",
            "epoch 91/250, loss=0.0792, time=0.13.\n",
            "epoch 92/250, loss=0.0478, time=0.13.\n",
            "epoch 93/250, loss=0.0284, time=0.13.\n",
            "epoch 94/250, loss=0.0222, time=0.13.\n",
            "epoch 95/250, loss=0.0142, time=0.13.\n",
            "epoch 96/250, loss=0.0290, time=0.13.\n",
            "epoch 97/250, loss=0.0288, time=0.14.\n",
            "epoch 98/250, loss=0.0426, time=0.14.\n",
            "epoch 99/250, loss=0.1125, time=0.13.\n",
            "epoch 100/250, loss=0.1308, time=0.13.\n",
            "epoch 101/250, loss=0.1609, time=0.13.\n",
            "epoch 102/250, loss=0.1347, time=0.14.\n",
            "epoch 103/250, loss=0.0689, time=0.13.\n",
            "epoch 104/250, loss=0.0279, time=0.13.\n",
            "epoch 105/250, loss=0.0178, time=0.13.\n",
            "epoch 106/250, loss=0.0159, time=0.12.\n",
            "epoch 107/250, loss=0.0285, time=0.13.\n",
            "epoch 108/250, loss=0.0235, time=0.13.\n",
            "epoch 109/250, loss=0.0555, time=0.13.\n",
            "epoch 110/250, loss=0.0558, time=0.13.\n",
            "epoch 111/250, loss=0.0679, time=0.13.\n",
            "epoch 112/250, loss=0.0388, time=0.14.\n",
            "epoch 113/250, loss=0.0306, time=0.13.\n",
            "epoch 114/250, loss=0.0212, time=0.13.\n",
            "epoch 115/250, loss=0.0127, time=0.13.\n",
            "epoch 116/250, loss=0.0049, time=0.13.\n",
            "epoch 117/250, loss=0.0029, time=0.13.\n",
            "epoch 118/250, loss=0.0034, time=0.13.\n",
            "epoch 119/250, loss=0.0013, time=0.14.\n",
            "epoch 120/250, loss=0.0009, time=0.13.\n",
            "epoch 121/250, loss=0.0008, time=0.13.\n",
            "epoch 122/250, loss=0.0007, time=0.13.\n",
            "epoch 123/250, loss=0.0006, time=0.14.\n",
            "epoch 124/250, loss=0.0006, time=0.13.\n",
            "epoch 125/250, loss=0.0005, time=0.13.\n",
            "epoch 126/250, loss=0.0005, time=0.12.\n",
            "epoch 127/250, loss=0.0005, time=0.13.\n",
            "epoch 128/250, loss=0.0004, time=0.13.\n",
            "epoch 129/250, loss=0.0004, time=0.13.\n",
            "epoch 130/250, loss=0.0004, time=0.13.\n",
            "epoch 131/250, loss=0.0004, time=0.13.\n",
            "epoch 132/250, loss=0.0003, time=0.14.\n",
            "epoch 133/250, loss=0.0003, time=0.13.\n",
            "epoch 134/250, loss=0.0003, time=0.13.\n",
            "epoch 135/250, loss=0.0003, time=0.13.\n",
            "epoch 136/250, loss=0.0003, time=0.12.\n",
            "epoch 137/250, loss=0.0003, time=0.14.\n",
            "epoch 138/250, loss=0.0003, time=0.13.\n",
            "epoch 139/250, loss=0.0003, time=0.14.\n",
            "epoch 140/250, loss=0.0003, time=0.14.\n",
            "epoch 141/250, loss=0.0002, time=0.13.\n",
            "epoch 142/250, loss=0.0002, time=0.13.\n",
            "epoch 143/250, loss=0.0002, time=0.13.\n",
            "epoch 144/250, loss=0.0002, time=0.13.\n",
            "epoch 145/250, loss=0.0002, time=0.13.\n",
            "epoch 146/250, loss=0.0002, time=0.13.\n",
            "epoch 147/250, loss=0.0002, time=0.13.\n",
            "epoch 148/250, loss=0.0002, time=0.13.\n",
            "epoch 149/250, loss=0.0002, time=0.13.\n",
            "epoch 150/250, loss=0.0002, time=0.13.\n",
            "epoch 151/250, loss=0.0002, time=0.13.\n",
            "epoch 152/250, loss=0.0002, time=0.13.\n",
            "epoch 153/250, loss=0.0002, time=0.13.\n",
            "epoch 154/250, loss=0.0002, time=0.13.\n",
            "epoch 155/250, loss=0.0002, time=0.13.\n",
            "epoch 156/250, loss=0.0002, time=0.13.\n",
            "epoch 157/250, loss=0.0002, time=0.14.\n",
            "epoch 158/250, loss=0.0002, time=0.13.\n",
            "epoch 159/250, loss=0.0002, time=0.13.\n",
            "epoch 160/250, loss=0.0001, time=0.13.\n",
            "epoch 161/250, loss=0.0001, time=0.13.\n",
            "epoch 162/250, loss=0.0001, time=0.14.\n",
            "epoch 163/250, loss=0.0001, time=0.13.\n",
            "epoch 164/250, loss=0.0001, time=0.13.\n",
            "epoch 165/250, loss=0.0001, time=0.13.\n",
            "epoch 166/250, loss=0.0001, time=0.13.\n",
            "epoch 167/250, loss=0.0001, time=0.13.\n",
            "epoch 168/250, loss=0.0001, time=0.13.\n",
            "epoch 169/250, loss=0.0001, time=0.13.\n",
            "epoch 170/250, loss=0.0001, time=0.13.\n",
            "epoch 171/250, loss=0.0001, time=0.13.\n",
            "epoch 172/250, loss=0.0001, time=0.13.\n",
            "epoch 173/250, loss=0.0001, time=0.13.\n",
            "epoch 174/250, loss=0.0001, time=0.13.\n",
            "epoch 175/250, loss=0.0001, time=0.13.\n",
            "epoch 176/250, loss=0.0001, time=0.13.\n",
            "epoch 177/250, loss=0.0001, time=0.13.\n",
            "epoch 178/250, loss=0.0001, time=0.13.\n",
            "epoch 179/250, loss=0.0001, time=0.13.\n",
            "epoch 180/250, loss=0.0001, time=0.13.\n",
            "epoch 181/250, loss=0.0001, time=0.13.\n",
            "epoch 182/250, loss=0.0001, time=0.14.\n",
            "epoch 183/250, loss=0.0001, time=0.13.\n",
            "epoch 184/250, loss=0.0001, time=0.13.\n",
            "epoch 185/250, loss=0.0001, time=0.13.\n",
            "epoch 186/250, loss=0.0001, time=0.13.\n",
            "epoch 187/250, loss=0.0001, time=0.13.\n",
            "epoch 188/250, loss=0.0001, time=0.13.\n",
            "epoch 189/250, loss=0.0001, time=0.13.\n",
            "epoch 190/250, loss=0.0001, time=0.13.\n",
            "epoch 191/250, loss=0.0001, time=0.13.\n",
            "epoch 192/250, loss=0.0001, time=0.13.\n",
            "epoch 193/250, loss=0.0001, time=0.13.\n",
            "epoch 194/250, loss=0.0001, time=0.13.\n",
            "epoch 195/250, loss=0.0001, time=0.13.\n",
            "epoch 196/250, loss=0.0001, time=0.13.\n",
            "epoch 197/250, loss=0.0001, time=0.13.\n",
            "epoch 198/250, loss=0.0001, time=0.13.\n",
            "epoch 199/250, loss=0.0001, time=0.13.\n",
            "epoch 200/250, loss=0.0001, time=0.13.\n",
            "epoch 201/250, loss=0.0001, time=0.13.\n",
            "epoch 202/250, loss=0.0001, time=0.14.\n",
            "epoch 203/250, loss=0.0001, time=0.13.\n",
            "epoch 204/250, loss=0.0001, time=0.13.\n",
            "epoch 205/250, loss=0.0001, time=0.13.\n",
            "epoch 206/250, loss=0.0001, time=0.13.\n",
            "epoch 207/250, loss=0.0001, time=0.13.\n",
            "epoch 208/250, loss=0.0001, time=0.13.\n",
            "epoch 209/250, loss=0.0001, time=0.14.\n",
            "epoch 210/250, loss=0.0001, time=0.13.\n",
            "epoch 211/250, loss=0.0001, time=0.13.\n",
            "epoch 212/250, loss=0.0001, time=0.13.\n",
            "epoch 213/250, loss=0.0001, time=0.13.\n",
            "epoch 214/250, loss=0.0001, time=0.13.\n",
            "epoch 215/250, loss=0.0001, time=0.13.\n",
            "epoch 216/250, loss=0.0001, time=0.13.\n",
            "epoch 217/250, loss=0.0001, time=0.14.\n",
            "epoch 218/250, loss=0.0001, time=0.13.\n",
            "epoch 219/250, loss=0.0001, time=0.13.\n",
            "epoch 220/250, loss=0.0001, time=0.13.\n",
            "epoch 221/250, loss=0.0001, time=0.13.\n",
            "epoch 222/250, loss=0.0001, time=0.13.\n",
            "epoch 223/250, loss=0.0001, time=0.13.\n",
            "epoch 224/250, loss=0.0001, time=0.14.\n",
            "epoch 225/250, loss=0.0001, time=0.13.\n",
            "epoch 226/250, loss=0.0001, time=0.13.\n",
            "epoch 227/250, loss=0.0001, time=0.13.\n",
            "epoch 228/250, loss=0.0001, time=0.13.\n",
            "epoch 229/250, loss=0.0001, time=0.13.\n",
            "epoch 230/250, loss=0.0001, time=0.13.\n",
            "epoch 231/250, loss=0.0001, time=0.13.\n",
            "epoch 232/250, loss=0.0001, time=0.13.\n",
            "epoch 233/250, loss=0.0001, time=0.13.\n",
            "epoch 234/250, loss=0.0001, time=0.13.\n",
            "epoch 235/250, loss=0.0001, time=0.13.\n",
            "epoch 236/250, loss=0.0001, time=0.13.\n",
            "epoch 237/250, loss=0.0001, time=0.14.\n",
            "epoch 238/250, loss=0.0001, time=0.13.\n",
            "epoch 239/250, loss=0.0001, time=0.13.\n",
            "epoch 240/250, loss=0.0001, time=0.13.\n",
            "epoch 241/250, loss=0.0000, time=0.13.\n",
            "epoch 242/250, loss=0.0000, time=0.14.\n",
            "epoch 243/250, loss=0.0000, time=0.13.\n",
            "epoch 244/250, loss=0.0000, time=0.13.\n",
            "epoch 245/250, loss=0.0000, time=0.13.\n",
            "epoch 246/250, loss=0.0000, time=0.13.\n",
            "epoch 247/250, loss=0.0000, time=0.14.\n",
            "epoch 248/250, loss=0.0000, time=0.13.\n",
            "epoch 249/250, loss=0.0000, time=0.13.\n",
            "epoch 250/250, loss=0.0000, time=0.14.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0179.npz, 0.8836, 0.8459, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0159.npz, 0.8801, 0.8425, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0174.npz, 0.8836, 0.8459, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0029.npz, 0.8699, 0.8082, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0094.npz, 0.8733, 0.8322, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0134.npz, 0.8836, 0.8425, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0034.npz, 0.8733, 0.8253, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0114.npz, 0.8801, 0.8562, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0069.npz, 0.8459, 0.8527, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0244.npz, 0.8836, 0.8356, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0214.npz, 0.8836, 0.8390, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0209.npz, 0.8836, 0.8390, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0009.npz, 0.8014, 0.7774, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0189.npz, 0.8836, 0.8459, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0234.npz, 0.8836, 0.8356, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0024.npz, 0.8356, 0.8116, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0054.npz, 0.8288, 0.8151, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0129.npz, 0.8836, 0.8425, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0204.npz, 0.8836, 0.8390, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0049.npz, 0.8322, 0.8116, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0124.npz, 0.8801, 0.8356, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0109.npz, 0.8185, 0.7945, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0059.npz, 0.8596, 0.8562, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0014.npz, 0.8082, 0.7842, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0199.npz, 0.8836, 0.8390, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0044.npz, 0.8562, 0.8493, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0004.npz, 0.7432, 0.7192, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0184.npz, 0.8836, 0.8459, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0169.npz, 0.8801, 0.8459, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0104.npz, 0.8801, 0.8425, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0099.npz, 0.8596, 0.8493, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0019.npz, 0.7945, 0.7877, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0224.npz, 0.8836, 0.8390, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0164.npz, 0.8801, 0.8459, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0139.npz, 0.8836, 0.8425, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0229.npz, 0.8836, 0.8390, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0239.npz, 0.8836, 0.8356, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0249.npz, 0.8836, 0.8356, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0149.npz, 0.8801, 0.8425, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0064.npz, 0.8562, 0.8288, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0089.npz, 0.8425, 0.8219, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0084.npz, 0.8801, 0.8151, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0079.npz, 0.8596, 0.8322, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0144.npz, 0.8801, 0.8459, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0039.npz, 0.8425, 0.7877, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0119.npz, 0.8801, 0.8459, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0219.npz, 0.8836, 0.8390, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0154.npz, 0.8801, 0.8425, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0194.npz, 0.8836, 0.8459, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesRegularTrain/alst_sc_c_0000_0074.npz, 0.8527, 0.8493, 0.03\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=1.5870, time=0.18.\n",
            "epoch 2/250, loss=1.3269, time=0.15.\n",
            "epoch 3/250, loss=1.0904, time=0.11.\n",
            "epoch 4/250, loss=0.9643, time=0.11.\n",
            "epoch 5/250, loss=0.7824, time=0.11.\n",
            "epoch 6/250, loss=0.6883, time=0.11.\n",
            "epoch 7/250, loss=0.6787, time=0.11.\n",
            "epoch 8/250, loss=0.6990, time=0.11.\n",
            "epoch 9/250, loss=0.6052, time=0.11.\n",
            "epoch 10/250, loss=0.5427, time=0.11.\n",
            "epoch 11/250, loss=0.5083, time=0.11.\n",
            "epoch 12/250, loss=0.4588, time=0.11.\n",
            "epoch 13/250, loss=0.4170, time=0.12.\n",
            "epoch 14/250, loss=0.3754, time=0.12.\n",
            "epoch 15/250, loss=0.4004, time=0.11.\n",
            "epoch 16/250, loss=0.3768, time=0.11.\n",
            "epoch 17/250, loss=0.3957, time=0.11.\n",
            "epoch 18/250, loss=0.3738, time=0.12.\n",
            "epoch 19/250, loss=0.4601, time=0.11.\n",
            "epoch 20/250, loss=0.4254, time=0.11.\n",
            "epoch 21/250, loss=0.3909, time=0.11.\n",
            "epoch 22/250, loss=0.3569, time=0.11.\n",
            "epoch 23/250, loss=0.4010, time=0.11.\n",
            "epoch 24/250, loss=0.3428, time=0.11.\n",
            "epoch 25/250, loss=0.2845, time=0.11.\n",
            "epoch 26/250, loss=0.2307, time=0.11.\n",
            "epoch 27/250, loss=0.2380, time=0.11.\n",
            "epoch 28/250, loss=0.2226, time=0.12.\n",
            "epoch 29/250, loss=0.2525, time=0.11.\n",
            "epoch 30/250, loss=0.1982, time=0.11.\n",
            "epoch 31/250, loss=0.2204, time=0.11.\n",
            "epoch 32/250, loss=0.2096, time=0.11.\n",
            "epoch 33/250, loss=0.1615, time=0.11.\n",
            "epoch 34/250, loss=0.1609, time=0.11.\n",
            "epoch 35/250, loss=0.1466, time=0.11.\n",
            "epoch 36/250, loss=0.1567, time=0.11.\n",
            "epoch 37/250, loss=0.2311, time=0.11.\n",
            "epoch 38/250, loss=0.2035, time=0.12.\n",
            "epoch 39/250, loss=0.1748, time=0.12.\n",
            "epoch 40/250, loss=0.2437, time=0.11.\n",
            "epoch 41/250, loss=0.2675, time=0.11.\n",
            "epoch 42/250, loss=0.1754, time=0.11.\n",
            "epoch 43/250, loss=0.1846, time=0.11.\n",
            "epoch 44/250, loss=0.1683, time=0.12.\n",
            "epoch 45/250, loss=0.1671, time=0.11.\n",
            "epoch 46/250, loss=0.1810, time=0.11.\n",
            "epoch 47/250, loss=0.1948, time=0.11.\n",
            "epoch 48/250, loss=0.1694, time=0.12.\n",
            "epoch 49/250, loss=0.1394, time=0.11.\n",
            "epoch 50/250, loss=0.1626, time=0.11.\n",
            "epoch 51/250, loss=0.1516, time=0.11.\n",
            "epoch 52/250, loss=0.1340, time=0.11.\n",
            "epoch 53/250, loss=0.1248, time=0.12.\n",
            "epoch 54/250, loss=0.0759, time=0.11.\n",
            "epoch 55/250, loss=0.1056, time=0.11.\n",
            "epoch 56/250, loss=0.1303, time=0.11.\n",
            "epoch 57/250, loss=0.2782, time=0.11.\n",
            "epoch 58/250, loss=0.3088, time=0.12.\n",
            "epoch 59/250, loss=0.2131, time=0.12.\n",
            "epoch 60/250, loss=0.1961, time=0.12.\n",
            "epoch 61/250, loss=0.1669, time=0.11.\n",
            "epoch 62/250, loss=0.1507, time=0.11.\n",
            "epoch 63/250, loss=0.1091, time=0.11.\n",
            "epoch 64/250, loss=0.0857, time=0.11.\n",
            "epoch 65/250, loss=0.0660, time=0.11.\n",
            "epoch 66/250, loss=0.0753, time=0.11.\n",
            "epoch 67/250, loss=0.0441, time=0.11.\n",
            "epoch 68/250, loss=0.0336, time=0.12.\n",
            "epoch 69/250, loss=0.0355, time=0.11.\n",
            "epoch 70/250, loss=0.0251, time=0.11.\n",
            "epoch 71/250, loss=0.0246, time=0.11.\n",
            "epoch 72/250, loss=0.0230, time=0.11.\n",
            "epoch 73/250, loss=0.0159, time=0.11.\n",
            "epoch 74/250, loss=0.0132, time=0.11.\n",
            "epoch 75/250, loss=0.0257, time=0.11.\n",
            "epoch 76/250, loss=0.0629, time=0.11.\n",
            "epoch 77/250, loss=0.1016, time=0.11.\n",
            "epoch 78/250, loss=0.2058, time=0.12.\n",
            "epoch 79/250, loss=0.3764, time=0.11.\n",
            "epoch 80/250, loss=0.2857, time=0.11.\n",
            "epoch 81/250, loss=0.1383, time=0.11.\n",
            "epoch 82/250, loss=0.0782, time=0.11.\n",
            "epoch 83/250, loss=0.0470, time=0.11.\n",
            "epoch 84/250, loss=0.0262, time=0.11.\n",
            "epoch 85/250, loss=0.0160, time=0.11.\n",
            "epoch 86/250, loss=0.0104, time=0.11.\n",
            "epoch 87/250, loss=0.0069, time=0.11.\n",
            "epoch 88/250, loss=0.0051, time=0.12.\n",
            "epoch 89/250, loss=0.0037, time=0.11.\n",
            "epoch 90/250, loss=0.0030, time=0.11.\n",
            "epoch 91/250, loss=0.0027, time=0.11.\n",
            "epoch 92/250, loss=0.0024, time=0.11.\n",
            "epoch 93/250, loss=0.0021, time=0.12.\n",
            "epoch 94/250, loss=0.0019, time=0.11.\n",
            "epoch 95/250, loss=0.0017, time=0.11.\n",
            "epoch 96/250, loss=0.0016, time=0.11.\n",
            "epoch 97/250, loss=0.0015, time=0.11.\n",
            "epoch 98/250, loss=0.0014, time=0.12.\n",
            "epoch 99/250, loss=0.0013, time=0.11.\n",
            "epoch 100/250, loss=0.0012, time=0.11.\n",
            "epoch 101/250, loss=0.0011, time=0.11.\n",
            "epoch 102/250, loss=0.0011, time=0.11.\n",
            "epoch 103/250, loss=0.0010, time=0.11.\n",
            "epoch 104/250, loss=0.0010, time=0.11.\n",
            "epoch 105/250, loss=0.0009, time=0.11.\n",
            "epoch 106/250, loss=0.0009, time=0.11.\n",
            "epoch 107/250, loss=0.0008, time=0.11.\n",
            "epoch 108/250, loss=0.0008, time=0.12.\n",
            "epoch 109/250, loss=0.0008, time=0.11.\n",
            "epoch 110/250, loss=0.0007, time=0.11.\n",
            "epoch 111/250, loss=0.0007, time=0.11.\n",
            "epoch 112/250, loss=0.0007, time=0.11.\n",
            "epoch 113/250, loss=0.0006, time=0.12.\n",
            "epoch 114/250, loss=0.0006, time=0.11.\n",
            "epoch 115/250, loss=0.0006, time=0.11.\n",
            "epoch 116/250, loss=0.0006, time=0.11.\n",
            "epoch 117/250, loss=0.0006, time=0.12.\n",
            "epoch 118/250, loss=0.0005, time=0.12.\n",
            "epoch 119/250, loss=0.0005, time=0.12.\n",
            "epoch 120/250, loss=0.0005, time=0.11.\n",
            "epoch 121/250, loss=0.0005, time=0.11.\n",
            "epoch 122/250, loss=0.0005, time=0.11.\n",
            "epoch 123/250, loss=0.0005, time=0.12.\n",
            "epoch 124/250, loss=0.0004, time=0.11.\n",
            "epoch 125/250, loss=0.0004, time=0.11.\n",
            "epoch 126/250, loss=0.0004, time=0.11.\n",
            "epoch 127/250, loss=0.0004, time=0.11.\n",
            "epoch 128/250, loss=0.0004, time=0.12.\n",
            "epoch 129/250, loss=0.0004, time=0.11.\n",
            "epoch 130/250, loss=0.0004, time=0.11.\n",
            "epoch 131/250, loss=0.0004, time=0.11.\n",
            "epoch 132/250, loss=0.0004, time=0.11.\n",
            "epoch 133/250, loss=0.0003, time=0.12.\n",
            "epoch 134/250, loss=0.0003, time=0.11.\n",
            "epoch 135/250, loss=0.0003, time=0.11.\n",
            "epoch 136/250, loss=0.0003, time=0.11.\n",
            "epoch 137/250, loss=0.0003, time=0.11.\n",
            "epoch 138/250, loss=0.0003, time=0.12.\n",
            "epoch 139/250, loss=0.0003, time=0.11.\n",
            "epoch 140/250, loss=0.0003, time=0.11.\n",
            "epoch 141/250, loss=0.0003, time=0.11.\n",
            "epoch 142/250, loss=0.0003, time=0.11.\n",
            "epoch 143/250, loss=0.0003, time=0.12.\n",
            "epoch 144/250, loss=0.0003, time=0.11.\n",
            "epoch 145/250, loss=0.0003, time=0.12.\n",
            "epoch 146/250, loss=0.0003, time=0.11.\n",
            "epoch 147/250, loss=0.0003, time=0.12.\n",
            "epoch 148/250, loss=0.0003, time=0.11.\n",
            "epoch 149/250, loss=0.0002, time=0.12.\n",
            "epoch 150/250, loss=0.0002, time=0.11.\n",
            "epoch 151/250, loss=0.0002, time=0.11.\n",
            "epoch 152/250, loss=0.0002, time=0.11.\n",
            "epoch 153/250, loss=0.0002, time=0.12.\n",
            "epoch 154/250, loss=0.0002, time=0.11.\n",
            "epoch 155/250, loss=0.0002, time=0.11.\n",
            "epoch 156/250, loss=0.0002, time=0.11.\n",
            "epoch 157/250, loss=0.0002, time=0.11.\n",
            "epoch 158/250, loss=0.0002, time=0.12.\n",
            "epoch 159/250, loss=0.0002, time=0.11.\n",
            "epoch 160/250, loss=0.0002, time=0.11.\n",
            "epoch 161/250, loss=0.0002, time=0.11.\n",
            "epoch 162/250, loss=0.0002, time=0.11.\n",
            "epoch 163/250, loss=0.0002, time=0.12.\n",
            "epoch 164/250, loss=0.0002, time=0.12.\n",
            "epoch 165/250, loss=0.0002, time=0.11.\n",
            "epoch 166/250, loss=0.0002, time=0.11.\n",
            "epoch 167/250, loss=0.0002, time=0.12.\n",
            "epoch 168/250, loss=0.0002, time=0.11.\n",
            "epoch 169/250, loss=0.0002, time=0.12.\n",
            "epoch 170/250, loss=0.0002, time=0.11.\n",
            "epoch 171/250, loss=0.0002, time=0.11.\n",
            "epoch 172/250, loss=0.0002, time=0.11.\n",
            "epoch 173/250, loss=0.0002, time=0.12.\n",
            "epoch 174/250, loss=0.0002, time=0.11.\n",
            "epoch 175/250, loss=0.0002, time=0.12.\n",
            "epoch 176/250, loss=0.0002, time=0.11.\n",
            "epoch 177/250, loss=0.0002, time=0.11.\n",
            "epoch 178/250, loss=0.0001, time=0.12.\n",
            "epoch 179/250, loss=0.0001, time=0.11.\n",
            "epoch 180/250, loss=0.0001, time=0.12.\n",
            "epoch 181/250, loss=0.0001, time=0.11.\n",
            "epoch 182/250, loss=0.0001, time=0.12.\n",
            "epoch 183/250, loss=0.0001, time=0.11.\n",
            "epoch 184/250, loss=0.0001, time=0.11.\n",
            "epoch 185/250, loss=0.0001, time=0.12.\n",
            "epoch 186/250, loss=0.0001, time=0.11.\n",
            "epoch 187/250, loss=0.0001, time=0.11.\n",
            "epoch 188/250, loss=0.0001, time=0.12.\n",
            "epoch 189/250, loss=0.0001, time=0.11.\n",
            "epoch 190/250, loss=0.0001, time=0.11.\n",
            "epoch 191/250, loss=0.0001, time=0.11.\n",
            "epoch 192/250, loss=0.0001, time=0.11.\n",
            "epoch 193/250, loss=0.0001, time=0.11.\n",
            "epoch 194/250, loss=0.0001, time=0.11.\n",
            "epoch 195/250, loss=0.0001, time=0.11.\n",
            "epoch 196/250, loss=0.0001, time=0.11.\n",
            "epoch 197/250, loss=0.0001, time=0.11.\n",
            "epoch 198/250, loss=0.0001, time=0.12.\n",
            "epoch 199/250, loss=0.0001, time=0.12.\n",
            "epoch 200/250, loss=0.0001, time=0.11.\n",
            "epoch 201/250, loss=0.0001, time=0.11.\n",
            "epoch 202/250, loss=0.0001, time=0.11.\n",
            "epoch 203/250, loss=0.0001, time=0.12.\n",
            "epoch 204/250, loss=0.0001, time=0.11.\n",
            "epoch 205/250, loss=0.0001, time=0.11.\n",
            "epoch 206/250, loss=0.0001, time=0.11.\n",
            "epoch 207/250, loss=0.0001, time=0.12.\n",
            "epoch 208/250, loss=0.0001, time=0.12.\n",
            "epoch 209/250, loss=0.0001, time=0.12.\n",
            "epoch 210/250, loss=0.0001, time=0.11.\n",
            "epoch 211/250, loss=0.0001, time=0.11.\n",
            "epoch 212/250, loss=0.0001, time=0.11.\n",
            "epoch 213/250, loss=0.0001, time=0.12.\n",
            "epoch 214/250, loss=0.0001, time=0.11.\n",
            "epoch 215/250, loss=0.0001, time=0.11.\n",
            "epoch 216/250, loss=0.0001, time=0.11.\n",
            "epoch 217/250, loss=0.0001, time=0.12.\n",
            "epoch 218/250, loss=0.0001, time=0.11.\n",
            "epoch 219/250, loss=0.0001, time=0.11.\n",
            "epoch 220/250, loss=0.0001, time=0.12.\n",
            "epoch 221/250, loss=0.0001, time=0.11.\n",
            "epoch 222/250, loss=0.0001, time=0.11.\n",
            "epoch 223/250, loss=0.0001, time=0.12.\n",
            "epoch 224/250, loss=0.0001, time=0.11.\n",
            "epoch 225/250, loss=0.0001, time=0.12.\n",
            "epoch 226/250, loss=0.0001, time=0.11.\n",
            "epoch 227/250, loss=0.0001, time=0.11.\n",
            "epoch 228/250, loss=0.0001, time=0.12.\n",
            "epoch 229/250, loss=0.0001, time=0.12.\n",
            "epoch 230/250, loss=0.0001, time=0.11.\n",
            "epoch 231/250, loss=0.0001, time=0.11.\n",
            "epoch 232/250, loss=0.0001, time=0.11.\n",
            "epoch 233/250, loss=0.0001, time=0.12.\n",
            "epoch 234/250, loss=0.0001, time=0.11.\n",
            "epoch 235/250, loss=0.0001, time=0.11.\n",
            "epoch 236/250, loss=0.0001, time=0.11.\n",
            "epoch 237/250, loss=0.0001, time=0.12.\n",
            "epoch 238/250, loss=0.0001, time=0.12.\n",
            "epoch 239/250, loss=0.0001, time=0.11.\n",
            "epoch 240/250, loss=0.0001, time=0.11.\n",
            "epoch 241/250, loss=0.0001, time=0.11.\n",
            "epoch 242/250, loss=0.0001, time=0.11.\n",
            "epoch 243/250, loss=0.0001, time=0.11.\n",
            "epoch 244/250, loss=0.0001, time=0.12.\n",
            "epoch 245/250, loss=0.0001, time=0.11.\n",
            "epoch 246/250, loss=0.0001, time=0.11.\n",
            "epoch 247/250, loss=0.0001, time=0.11.\n",
            "epoch 248/250, loss=0.0001, time=0.12.\n",
            "epoch 249/250, loss=0.0001, time=0.12.\n",
            "epoch 250/250, loss=0.0001, time=0.11.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0164.npz, 0.8214, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0029.npz, 0.8016, 0.7302, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0179.npz, 0.8254, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0124.npz, 0.8214, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0064.npz, 0.8095, 0.7976, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0109.npz, 0.8175, 0.7897, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0069.npz, 0.7976, 0.7976, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0159.npz, 0.8175, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0019.npz, 0.7619, 0.7579, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0224.npz, 0.8294, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0094.npz, 0.8175, 0.7857, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0214.npz, 0.8254, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0099.npz, 0.8175, 0.7857, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0104.npz, 0.8175, 0.7897, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0054.npz, 0.7778, 0.7659, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0169.npz, 0.8254, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0024.npz, 0.8056, 0.7500, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0234.npz, 0.8294, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0114.npz, 0.8214, 0.7897, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0134.npz, 0.8214, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0194.npz, 0.8254, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0249.npz, 0.8294, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0059.npz, 0.8333, 0.7619, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0244.npz, 0.8294, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0154.npz, 0.8175, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0014.npz, 0.7698, 0.7341, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0139.npz, 0.8214, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0074.npz, 0.8175, 0.7659, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0189.npz, 0.8254, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0084.npz, 0.8135, 0.7698, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0004.npz, 0.7063, 0.6548, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0144.npz, 0.8214, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0174.npz, 0.8254, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0184.npz, 0.8254, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0199.npz, 0.8254, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0204.npz, 0.8254, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0119.npz, 0.8175, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0079.npz, 0.7937, 0.7579, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0039.npz, 0.7500, 0.7024, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0049.npz, 0.7897, 0.7698, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0034.npz, 0.7817, 0.7500, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0219.npz, 0.8254, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0239.npz, 0.8294, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0009.npz, 0.6746, 0.6349, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0129.npz, 0.8214, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0209.npz, 0.8254, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0044.npz, 0.7976, 0.7817, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0089.npz, 0.8135, 0.7857, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0229.npz, 0.8294, 0.7937, 0.03\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_MixedShapesSmallTrain/alst_sc_c_0000_0149.npz, 0.8175, 0.7937, 0.03\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=2.4336, time=0.01.\n",
            "epoch 2/250, loss=2.2761, time=0.01.\n",
            "epoch 3/250, loss=2.1963, time=0.01.\n",
            "epoch 4/250, loss=2.0079, time=0.01.\n",
            "epoch 5/250, loss=1.8510, time=0.01.\n",
            "epoch 6/250, loss=1.6808, time=0.01.\n",
            "epoch 7/250, loss=1.5906, time=0.01.\n",
            "epoch 8/250, loss=1.4649, time=0.01.\n",
            "epoch 9/250, loss=1.3300, time=0.01.\n",
            "epoch 10/250, loss=1.1977, time=0.01.\n",
            "epoch 11/250, loss=1.1409, time=0.01.\n",
            "epoch 12/250, loss=0.9819, time=0.01.\n",
            "epoch 13/250, loss=0.8906, time=0.01.\n",
            "epoch 14/250, loss=0.8845, time=0.01.\n",
            "epoch 15/250, loss=0.7608, time=0.01.\n",
            "epoch 16/250, loss=0.6632, time=0.01.\n",
            "epoch 17/250, loss=0.5686, time=0.01.\n",
            "epoch 18/250, loss=0.4949, time=0.01.\n",
            "epoch 19/250, loss=0.4273, time=0.01.\n",
            "epoch 20/250, loss=0.3604, time=0.01.\n",
            "epoch 21/250, loss=0.2978, time=0.01.\n",
            "epoch 22/250, loss=0.2466, time=0.01.\n",
            "epoch 23/250, loss=0.1999, time=0.01.\n",
            "epoch 24/250, loss=0.1635, time=0.01.\n",
            "epoch 25/250, loss=0.1334, time=0.01.\n",
            "epoch 26/250, loss=0.1088, time=0.01.\n",
            "epoch 27/250, loss=0.0896, time=0.01.\n",
            "epoch 28/250, loss=0.0730, time=0.01.\n",
            "epoch 29/250, loss=0.0599, time=0.01.\n",
            "epoch 30/250, loss=0.0498, time=0.01.\n",
            "epoch 31/250, loss=0.0413, time=0.01.\n",
            "epoch 32/250, loss=0.0345, time=0.01.\n",
            "epoch 33/250, loss=0.0292, time=0.01.\n",
            "epoch 34/250, loss=0.0249, time=0.01.\n",
            "epoch 35/250, loss=0.0215, time=0.01.\n",
            "epoch 36/250, loss=0.0187, time=0.01.\n",
            "epoch 37/250, loss=0.0163, time=0.01.\n",
            "epoch 38/250, loss=0.0144, time=0.01.\n",
            "epoch 39/250, loss=0.0128, time=0.01.\n",
            "epoch 40/250, loss=0.0115, time=0.01.\n",
            "epoch 41/250, loss=0.0104, time=0.01.\n",
            "epoch 42/250, loss=0.0095, time=0.01.\n",
            "epoch 43/250, loss=0.0086, time=0.01.\n",
            "epoch 44/250, loss=0.0079, time=0.01.\n",
            "epoch 45/250, loss=0.0073, time=0.01.\n",
            "epoch 46/250, loss=0.0067, time=0.01.\n",
            "epoch 47/250, loss=0.0062, time=0.01.\n",
            "epoch 48/250, loss=0.0057, time=0.01.\n",
            "epoch 49/250, loss=0.0053, time=0.01.\n",
            "epoch 50/250, loss=0.0050, time=0.01.\n",
            "epoch 51/250, loss=0.0047, time=0.01.\n",
            "epoch 52/250, loss=0.0044, time=0.01.\n",
            "epoch 53/250, loss=0.0041, time=0.01.\n",
            "epoch 54/250, loss=0.0039, time=0.01.\n",
            "epoch 55/250, loss=0.0037, time=0.01.\n",
            "epoch 56/250, loss=0.0035, time=0.01.\n",
            "epoch 57/250, loss=0.0034, time=0.01.\n",
            "epoch 58/250, loss=0.0032, time=0.01.\n",
            "epoch 59/250, loss=0.0031, time=0.01.\n",
            "epoch 60/250, loss=0.0030, time=0.01.\n",
            "epoch 61/250, loss=0.0029, time=0.01.\n",
            "epoch 62/250, loss=0.0028, time=0.01.\n",
            "epoch 63/250, loss=0.0027, time=0.01.\n",
            "epoch 64/250, loss=0.0026, time=0.01.\n",
            "epoch 65/250, loss=0.0025, time=0.01.\n",
            "epoch 66/250, loss=0.0025, time=0.01.\n",
            "epoch 67/250, loss=0.0024, time=0.01.\n",
            "epoch 68/250, loss=0.0023, time=0.01.\n",
            "epoch 69/250, loss=0.0023, time=0.01.\n",
            "epoch 70/250, loss=0.0022, time=0.01.\n",
            "epoch 71/250, loss=0.0022, time=0.01.\n",
            "epoch 72/250, loss=0.0021, time=0.01.\n",
            "epoch 73/250, loss=0.0021, time=0.01.\n",
            "epoch 74/250, loss=0.0020, time=0.01.\n",
            "epoch 75/250, loss=0.0020, time=0.01.\n",
            "epoch 76/250, loss=0.0020, time=0.01.\n",
            "epoch 77/250, loss=0.0019, time=0.01.\n",
            "epoch 78/250, loss=0.0019, time=0.01.\n",
            "epoch 79/250, loss=0.0019, time=0.01.\n",
            "epoch 80/250, loss=0.0018, time=0.01.\n",
            "epoch 81/250, loss=0.0018, time=0.01.\n",
            "epoch 82/250, loss=0.0018, time=0.01.\n",
            "epoch 83/250, loss=0.0017, time=0.01.\n",
            "epoch 84/250, loss=0.0017, time=0.01.\n",
            "epoch 85/250, loss=0.0017, time=0.01.\n",
            "epoch 86/250, loss=0.0017, time=0.01.\n",
            "epoch 87/250, loss=0.0016, time=0.01.\n",
            "epoch 88/250, loss=0.0016, time=0.01.\n",
            "epoch 89/250, loss=0.0016, time=0.01.\n",
            "epoch 90/250, loss=0.0016, time=0.01.\n",
            "epoch 91/250, loss=0.0016, time=0.01.\n",
            "epoch 92/250, loss=0.0015, time=0.01.\n",
            "epoch 93/250, loss=0.0015, time=0.01.\n",
            "epoch 94/250, loss=0.0015, time=0.01.\n",
            "epoch 95/250, loss=0.0015, time=0.01.\n",
            "epoch 96/250, loss=0.0015, time=0.01.\n",
            "epoch 97/250, loss=0.0014, time=0.01.\n",
            "epoch 98/250, loss=0.0014, time=0.01.\n",
            "epoch 99/250, loss=0.0014, time=0.01.\n",
            "epoch 100/250, loss=0.0014, time=0.01.\n",
            "epoch 101/250, loss=0.0014, time=0.01.\n",
            "epoch 102/250, loss=0.0014, time=0.01.\n",
            "epoch 103/250, loss=0.0014, time=0.01.\n",
            "epoch 104/250, loss=0.0013, time=0.01.\n",
            "epoch 105/250, loss=0.0013, time=0.01.\n",
            "epoch 106/250, loss=0.0013, time=0.01.\n",
            "epoch 107/250, loss=0.0013, time=0.01.\n",
            "epoch 108/250, loss=0.0013, time=0.01.\n",
            "epoch 109/250, loss=0.0013, time=0.01.\n",
            "epoch 110/250, loss=0.0013, time=0.01.\n",
            "epoch 111/250, loss=0.0012, time=0.01.\n",
            "epoch 112/250, loss=0.0012, time=0.01.\n",
            "epoch 113/250, loss=0.0012, time=0.01.\n",
            "epoch 114/250, loss=0.0012, time=0.01.\n",
            "epoch 115/250, loss=0.0012, time=0.01.\n",
            "epoch 116/250, loss=0.0012, time=0.01.\n",
            "epoch 117/250, loss=0.0012, time=0.01.\n",
            "epoch 118/250, loss=0.0012, time=0.01.\n",
            "epoch 119/250, loss=0.0012, time=0.01.\n",
            "epoch 120/250, loss=0.0011, time=0.01.\n",
            "epoch 121/250, loss=0.0011, time=0.01.\n",
            "epoch 122/250, loss=0.0011, time=0.01.\n",
            "epoch 123/250, loss=0.0011, time=0.01.\n",
            "epoch 124/250, loss=0.0011, time=0.01.\n",
            "epoch 125/250, loss=0.0011, time=0.01.\n",
            "epoch 126/250, loss=0.0011, time=0.01.\n",
            "epoch 127/250, loss=0.0011, time=0.01.\n",
            "epoch 128/250, loss=0.0011, time=0.01.\n",
            "epoch 129/250, loss=0.0011, time=0.01.\n",
            "epoch 130/250, loss=0.0010, time=0.01.\n",
            "epoch 131/250, loss=0.0010, time=0.01.\n",
            "epoch 132/250, loss=0.0010, time=0.01.\n",
            "epoch 133/250, loss=0.0010, time=0.01.\n",
            "epoch 134/250, loss=0.0010, time=0.01.\n",
            "epoch 135/250, loss=0.0010, time=0.01.\n",
            "epoch 136/250, loss=0.0010, time=0.01.\n",
            "epoch 137/250, loss=0.0010, time=0.01.\n",
            "epoch 138/250, loss=0.0010, time=0.01.\n",
            "epoch 139/250, loss=0.0010, time=0.01.\n",
            "epoch 140/250, loss=0.0010, time=0.01.\n",
            "epoch 141/250, loss=0.0010, time=0.01.\n",
            "epoch 142/250, loss=0.0009, time=0.01.\n",
            "epoch 143/250, loss=0.0009, time=0.01.\n",
            "epoch 144/250, loss=0.0009, time=0.01.\n",
            "epoch 145/250, loss=0.0009, time=0.01.\n",
            "epoch 146/250, loss=0.0009, time=0.01.\n",
            "epoch 147/250, loss=0.0009, time=0.01.\n",
            "epoch 148/250, loss=0.0009, time=0.01.\n",
            "epoch 149/250, loss=0.0009, time=0.01.\n",
            "epoch 150/250, loss=0.0009, time=0.01.\n",
            "epoch 151/250, loss=0.0009, time=0.01.\n",
            "epoch 152/250, loss=0.0009, time=0.01.\n",
            "epoch 153/250, loss=0.0009, time=0.01.\n",
            "epoch 154/250, loss=0.0009, time=0.01.\n",
            "epoch 155/250, loss=0.0009, time=0.01.\n",
            "epoch 156/250, loss=0.0008, time=0.01.\n",
            "epoch 157/250, loss=0.0008, time=0.01.\n",
            "epoch 158/250, loss=0.0008, time=0.01.\n",
            "epoch 159/250, loss=0.0008, time=0.01.\n",
            "epoch 160/250, loss=0.0008, time=0.01.\n",
            "epoch 161/250, loss=0.0008, time=0.01.\n",
            "epoch 162/250, loss=0.0008, time=0.01.\n",
            "epoch 163/250, loss=0.0008, time=0.01.\n",
            "epoch 164/250, loss=0.0008, time=0.01.\n",
            "epoch 165/250, loss=0.0008, time=0.01.\n",
            "epoch 166/250, loss=0.0008, time=0.01.\n",
            "epoch 167/250, loss=0.0008, time=0.01.\n",
            "epoch 168/250, loss=0.0008, time=0.01.\n",
            "epoch 169/250, loss=0.0008, time=0.01.\n",
            "epoch 170/250, loss=0.0008, time=0.01.\n",
            "epoch 171/250, loss=0.0008, time=0.01.\n",
            "epoch 172/250, loss=0.0007, time=0.01.\n",
            "epoch 173/250, loss=0.0007, time=0.01.\n",
            "epoch 174/250, loss=0.0007, time=0.01.\n",
            "epoch 175/250, loss=0.0007, time=0.01.\n",
            "epoch 176/250, loss=0.0007, time=0.01.\n",
            "epoch 177/250, loss=0.0007, time=0.01.\n",
            "epoch 178/250, loss=0.0007, time=0.01.\n",
            "epoch 179/250, loss=0.0007, time=0.01.\n",
            "epoch 180/250, loss=0.0007, time=0.01.\n",
            "epoch 181/250, loss=0.0007, time=0.01.\n",
            "epoch 182/250, loss=0.0007, time=0.01.\n",
            "epoch 183/250, loss=0.0007, time=0.01.\n",
            "epoch 184/250, loss=0.0007, time=0.01.\n",
            "epoch 185/250, loss=0.0007, time=0.01.\n",
            "epoch 186/250, loss=0.0007, time=0.01.\n",
            "epoch 187/250, loss=0.0007, time=0.01.\n",
            "epoch 188/250, loss=0.0007, time=0.01.\n",
            "epoch 189/250, loss=0.0007, time=0.01.\n",
            "epoch 190/250, loss=0.0007, time=0.01.\n",
            "epoch 191/250, loss=0.0007, time=0.01.\n",
            "epoch 192/250, loss=0.0007, time=0.01.\n",
            "epoch 193/250, loss=0.0006, time=0.01.\n",
            "epoch 194/250, loss=0.0006, time=0.01.\n",
            "epoch 195/250, loss=0.0006, time=0.01.\n",
            "epoch 196/250, loss=0.0006, time=0.01.\n",
            "epoch 197/250, loss=0.0006, time=0.01.\n",
            "epoch 198/250, loss=0.0006, time=0.01.\n",
            "epoch 199/250, loss=0.0006, time=0.01.\n",
            "epoch 200/250, loss=0.0006, time=0.01.\n",
            "epoch 201/250, loss=0.0006, time=0.01.\n",
            "epoch 202/250, loss=0.0006, time=0.01.\n",
            "epoch 203/250, loss=0.0006, time=0.01.\n",
            "epoch 204/250, loss=0.0006, time=0.01.\n",
            "epoch 205/250, loss=0.0006, time=0.01.\n",
            "epoch 206/250, loss=0.0006, time=0.01.\n",
            "epoch 207/250, loss=0.0006, time=0.01.\n",
            "epoch 208/250, loss=0.0006, time=0.01.\n",
            "epoch 209/250, loss=0.0006, time=0.01.\n",
            "epoch 210/250, loss=0.0006, time=0.01.\n",
            "epoch 211/250, loss=0.0006, time=0.01.\n",
            "epoch 212/250, loss=0.0006, time=0.01.\n",
            "epoch 213/250, loss=0.0006, time=0.01.\n",
            "epoch 214/250, loss=0.0006, time=0.01.\n",
            "epoch 215/250, loss=0.0006, time=0.01.\n",
            "epoch 216/250, loss=0.0006, time=0.01.\n",
            "epoch 217/250, loss=0.0006, time=0.01.\n",
            "epoch 218/250, loss=0.0005, time=0.01.\n",
            "epoch 219/250, loss=0.0005, time=0.01.\n",
            "epoch 220/250, loss=0.0005, time=0.01.\n",
            "epoch 221/250, loss=0.0005, time=0.01.\n",
            "epoch 222/250, loss=0.0005, time=0.01.\n",
            "epoch 223/250, loss=0.0005, time=0.01.\n",
            "epoch 224/250, loss=0.0005, time=0.01.\n",
            "epoch 225/250, loss=0.0005, time=0.01.\n",
            "epoch 226/250, loss=0.0005, time=0.01.\n",
            "epoch 227/250, loss=0.0005, time=0.01.\n",
            "epoch 228/250, loss=0.0005, time=0.01.\n",
            "epoch 229/250, loss=0.0005, time=0.01.\n",
            "epoch 230/250, loss=0.0005, time=0.01.\n",
            "epoch 231/250, loss=0.0005, time=0.01.\n",
            "epoch 232/250, loss=0.0005, time=0.01.\n",
            "epoch 233/250, loss=0.0005, time=0.01.\n",
            "epoch 234/250, loss=0.0005, time=0.01.\n",
            "epoch 235/250, loss=0.0005, time=0.01.\n",
            "epoch 236/250, loss=0.0005, time=0.01.\n",
            "epoch 237/250, loss=0.0005, time=0.01.\n",
            "epoch 238/250, loss=0.0005, time=0.01.\n",
            "epoch 239/250, loss=0.0005, time=0.01.\n",
            "epoch 240/250, loss=0.0005, time=0.01.\n",
            "epoch 241/250, loss=0.0005, time=0.01.\n",
            "epoch 242/250, loss=0.0005, time=0.01.\n",
            "epoch 243/250, loss=0.0005, time=0.01.\n",
            "epoch 244/250, loss=0.0005, time=0.01.\n",
            "epoch 245/250, loss=0.0005, time=0.01.\n",
            "epoch 246/250, loss=0.0005, time=0.01.\n",
            "epoch 247/250, loss=0.0005, time=0.01.\n",
            "epoch 248/250, loss=0.0005, time=0.01.\n",
            "epoch 249/250, loss=0.0005, time=0.01.\n",
            "epoch 250/250, loss=0.0005, time=0.01.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0179.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0194.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0199.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0074.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0129.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0249.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0234.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0044.npz, 0.1000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0189.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0104.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0004.npz, 0.3000, 0.1000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0124.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0244.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0204.npz, 0.1000, 0.4000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0109.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0054.npz, 0.1000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0214.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0039.npz, 0.1000, 0.3000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0019.npz, 0.2000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0024.npz, 0.2000, 0.3000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0159.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0134.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0239.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0084.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0209.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0099.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0139.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0089.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0144.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0094.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0114.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0219.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0059.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0174.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0229.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0079.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0154.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0029.npz, 0.1000, 0.3000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0119.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0069.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0169.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0034.npz, 0.1000, 0.3000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0009.npz, 0.2000, 0.2000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0064.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0014.npz, 0.4000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0224.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0184.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0164.npz, 0.1000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0049.npz, 0.1000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PickupGestureWiimoteZ/alst_sc_c_0000_0149.npz, 0.1000, 0.4000, 0.00\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=3.9785, time=0.03.\n",
            "epoch 2/250, loss=3.8942, time=0.03.\n",
            "epoch 3/250, loss=3.8951, time=0.03.\n",
            "epoch 4/250, loss=3.7917, time=0.03.\n",
            "epoch 5/250, loss=3.7072, time=0.03.\n",
            "epoch 6/250, loss=3.6035, time=0.02.\n",
            "epoch 7/250, loss=3.4454, time=0.02.\n",
            "epoch 8/250, loss=3.3208, time=0.02.\n",
            "epoch 9/250, loss=3.1452, time=0.02.\n",
            "epoch 10/250, loss=3.0414, time=0.02.\n",
            "epoch 11/250, loss=2.8883, time=0.02.\n",
            "epoch 12/250, loss=2.7481, time=0.02.\n",
            "epoch 13/250, loss=2.6099, time=0.02.\n",
            "epoch 14/250, loss=2.4534, time=0.02.\n",
            "epoch 15/250, loss=2.3527, time=0.02.\n",
            "epoch 16/250, loss=2.0569, time=0.02.\n",
            "epoch 17/250, loss=1.8919, time=0.02.\n",
            "epoch 18/250, loss=1.7914, time=0.02.\n",
            "epoch 19/250, loss=1.5962, time=0.02.\n",
            "epoch 20/250, loss=1.5075, time=0.02.\n",
            "epoch 21/250, loss=1.3135, time=0.02.\n",
            "epoch 22/250, loss=1.2210, time=0.02.\n",
            "epoch 23/250, loss=1.2645, time=0.02.\n",
            "epoch 24/250, loss=1.0773, time=0.02.\n",
            "epoch 25/250, loss=0.9734, time=0.02.\n",
            "epoch 26/250, loss=0.8313, time=0.02.\n",
            "epoch 27/250, loss=0.6974, time=0.02.\n",
            "epoch 28/250, loss=0.6642, time=0.02.\n",
            "epoch 29/250, loss=0.5609, time=0.02.\n",
            "epoch 30/250, loss=0.6409, time=0.02.\n",
            "epoch 31/250, loss=0.5095, time=0.02.\n",
            "epoch 32/250, loss=0.4742, time=0.02.\n",
            "epoch 33/250, loss=0.4219, time=0.02.\n",
            "epoch 34/250, loss=0.7221, time=0.02.\n",
            "epoch 35/250, loss=1.0600, time=0.02.\n",
            "epoch 36/250, loss=1.1335, time=0.02.\n",
            "epoch 37/250, loss=0.7254, time=0.02.\n",
            "epoch 38/250, loss=0.7691, time=0.02.\n",
            "epoch 39/250, loss=0.6322, time=0.02.\n",
            "epoch 40/250, loss=0.5035, time=0.02.\n",
            "epoch 41/250, loss=0.5698, time=0.02.\n",
            "epoch 42/250, loss=0.6441, time=0.02.\n",
            "epoch 43/250, loss=0.6288, time=0.02.\n",
            "epoch 44/250, loss=0.9087, time=0.02.\n",
            "epoch 45/250, loss=0.7445, time=0.02.\n",
            "epoch 46/250, loss=0.7920, time=0.02.\n",
            "epoch 47/250, loss=0.7365, time=0.02.\n",
            "epoch 48/250, loss=0.7752, time=0.02.\n",
            "epoch 49/250, loss=0.5256, time=0.02.\n",
            "epoch 50/250, loss=0.4950, time=0.02.\n",
            "epoch 51/250, loss=0.2986, time=0.02.\n",
            "epoch 52/250, loss=0.2861, time=0.02.\n",
            "epoch 53/250, loss=0.2107, time=0.02.\n",
            "epoch 54/250, loss=0.1958, time=0.02.\n",
            "epoch 55/250, loss=0.1451, time=0.02.\n",
            "epoch 56/250, loss=0.1232, time=0.02.\n",
            "epoch 57/250, loss=0.1151, time=0.02.\n",
            "epoch 58/250, loss=0.0918, time=0.02.\n",
            "epoch 59/250, loss=0.0728, time=0.02.\n",
            "epoch 60/250, loss=0.0672, time=0.02.\n",
            "epoch 61/250, loss=0.0626, time=0.02.\n",
            "epoch 62/250, loss=0.0530, time=0.02.\n",
            "epoch 63/250, loss=0.0423, time=0.02.\n",
            "epoch 64/250, loss=0.0384, time=0.02.\n",
            "epoch 65/250, loss=0.0348, time=0.02.\n",
            "epoch 66/250, loss=0.0330, time=0.02.\n",
            "epoch 67/250, loss=0.0268, time=0.02.\n",
            "epoch 68/250, loss=0.0257, time=0.02.\n",
            "epoch 69/250, loss=0.0236, time=0.02.\n",
            "epoch 70/250, loss=0.0224, time=0.02.\n",
            "epoch 71/250, loss=0.0195, time=0.02.\n",
            "epoch 72/250, loss=0.0188, time=0.02.\n",
            "epoch 73/250, loss=0.0175, time=0.02.\n",
            "epoch 74/250, loss=0.0170, time=0.02.\n",
            "epoch 75/250, loss=0.0164, time=0.02.\n",
            "epoch 76/250, loss=0.0139, time=0.02.\n",
            "epoch 77/250, loss=0.0135, time=0.02.\n",
            "epoch 78/250, loss=0.0130, time=0.02.\n",
            "epoch 79/250, loss=0.0123, time=0.02.\n",
            "epoch 80/250, loss=0.0117, time=0.02.\n",
            "epoch 81/250, loss=0.0114, time=0.02.\n",
            "epoch 82/250, loss=0.0104, time=0.02.\n",
            "epoch 83/250, loss=0.0106, time=0.02.\n",
            "epoch 84/250, loss=0.0102, time=0.02.\n",
            "epoch 85/250, loss=0.0097, time=0.02.\n",
            "epoch 86/250, loss=0.0090, time=0.02.\n",
            "epoch 87/250, loss=0.0091, time=0.02.\n",
            "epoch 88/250, loss=0.0089, time=0.02.\n",
            "epoch 89/250, loss=0.0089, time=0.02.\n",
            "epoch 90/250, loss=0.0085, time=0.02.\n",
            "epoch 91/250, loss=0.0083, time=0.02.\n",
            "epoch 92/250, loss=0.0078, time=0.02.\n",
            "epoch 93/250, loss=0.0076, time=0.02.\n",
            "epoch 94/250, loss=0.0077, time=0.02.\n",
            "epoch 95/250, loss=0.0074, time=0.02.\n",
            "epoch 96/250, loss=0.0074, time=0.02.\n",
            "epoch 97/250, loss=0.0074, time=0.02.\n",
            "epoch 98/250, loss=0.0070, time=0.02.\n",
            "epoch 99/250, loss=0.0069, time=0.02.\n",
            "epoch 100/250, loss=0.0066, time=0.02.\n",
            "epoch 101/250, loss=0.0067, time=0.02.\n",
            "epoch 102/250, loss=0.0064, time=0.02.\n",
            "epoch 103/250, loss=0.0063, time=0.02.\n",
            "epoch 104/250, loss=0.0060, time=0.02.\n",
            "epoch 105/250, loss=0.0058, time=0.02.\n",
            "epoch 106/250, loss=0.0058, time=0.02.\n",
            "epoch 107/250, loss=0.0060, time=0.02.\n",
            "epoch 108/250, loss=0.0059, time=0.02.\n",
            "epoch 109/250, loss=0.0057, time=0.02.\n",
            "epoch 110/250, loss=0.0054, time=0.02.\n",
            "epoch 111/250, loss=0.0055, time=0.02.\n",
            "epoch 112/250, loss=0.0053, time=0.02.\n",
            "epoch 113/250, loss=0.0053, time=0.02.\n",
            "epoch 114/250, loss=0.0053, time=0.02.\n",
            "epoch 115/250, loss=0.0050, time=0.02.\n",
            "epoch 116/250, loss=0.0050, time=0.02.\n",
            "epoch 117/250, loss=0.0049, time=0.02.\n",
            "epoch 118/250, loss=0.0049, time=0.02.\n",
            "epoch 119/250, loss=0.0047, time=0.02.\n",
            "epoch 120/250, loss=0.0049, time=0.02.\n",
            "epoch 121/250, loss=0.0047, time=0.02.\n",
            "epoch 122/250, loss=0.0046, time=0.02.\n",
            "epoch 123/250, loss=0.0044, time=0.02.\n",
            "epoch 124/250, loss=0.0044, time=0.02.\n",
            "epoch 125/250, loss=0.0044, time=0.02.\n",
            "epoch 126/250, loss=0.0043, time=0.02.\n",
            "epoch 127/250, loss=0.0043, time=0.02.\n",
            "epoch 128/250, loss=0.0042, time=0.02.\n",
            "epoch 129/250, loss=0.0041, time=0.02.\n",
            "epoch 130/250, loss=0.0039, time=0.02.\n",
            "epoch 131/250, loss=0.0040, time=0.02.\n",
            "epoch 132/250, loss=0.0039, time=0.02.\n",
            "epoch 133/250, loss=0.0038, time=0.02.\n",
            "epoch 134/250, loss=0.0040, time=0.02.\n",
            "epoch 135/250, loss=0.0038, time=0.02.\n",
            "epoch 136/250, loss=0.0037, time=0.02.\n",
            "epoch 137/250, loss=0.0037, time=0.02.\n",
            "epoch 138/250, loss=0.0037, time=0.02.\n",
            "epoch 139/250, loss=0.0035, time=0.02.\n",
            "epoch 140/250, loss=0.0036, time=0.02.\n",
            "epoch 141/250, loss=0.0035, time=0.02.\n",
            "epoch 142/250, loss=0.0034, time=0.02.\n",
            "epoch 143/250, loss=0.0034, time=0.02.\n",
            "epoch 144/250, loss=0.0034, time=0.02.\n",
            "epoch 145/250, loss=0.0033, time=0.02.\n",
            "epoch 146/250, loss=0.0032, time=0.02.\n",
            "epoch 147/250, loss=0.0034, time=0.02.\n",
            "epoch 148/250, loss=0.0032, time=0.02.\n",
            "epoch 149/250, loss=0.0032, time=0.02.\n",
            "epoch 150/250, loss=0.0032, time=0.02.\n",
            "epoch 151/250, loss=0.0031, time=0.02.\n",
            "epoch 152/250, loss=0.0030, time=0.02.\n",
            "epoch 153/250, loss=0.0030, time=0.02.\n",
            "epoch 154/250, loss=0.0030, time=0.02.\n",
            "epoch 155/250, loss=0.0029, time=0.02.\n",
            "epoch 156/250, loss=0.0029, time=0.02.\n",
            "epoch 157/250, loss=0.0030, time=0.02.\n",
            "epoch 158/250, loss=0.0028, time=0.02.\n",
            "epoch 159/250, loss=0.0028, time=0.02.\n",
            "epoch 160/250, loss=0.0028, time=0.02.\n",
            "epoch 161/250, loss=0.0028, time=0.02.\n",
            "epoch 162/250, loss=0.0028, time=0.02.\n",
            "epoch 163/250, loss=0.0027, time=0.02.\n",
            "epoch 164/250, loss=0.0027, time=0.02.\n",
            "epoch 165/250, loss=0.0027, time=0.02.\n",
            "epoch 166/250, loss=0.0026, time=0.02.\n",
            "epoch 167/250, loss=0.0026, time=0.02.\n",
            "epoch 168/250, loss=0.0025, time=0.02.\n",
            "epoch 169/250, loss=0.0025, time=0.02.\n",
            "epoch 170/250, loss=0.0025, time=0.02.\n",
            "epoch 171/250, loss=0.0025, time=0.02.\n",
            "epoch 172/250, loss=0.0025, time=0.02.\n",
            "epoch 173/250, loss=0.0024, time=0.02.\n",
            "epoch 174/250, loss=0.0024, time=0.02.\n",
            "epoch 175/250, loss=0.0024, time=0.02.\n",
            "epoch 176/250, loss=0.0024, time=0.02.\n",
            "epoch 177/250, loss=0.0023, time=0.02.\n",
            "epoch 178/250, loss=0.0024, time=0.02.\n",
            "epoch 179/250, loss=0.0023, time=0.02.\n",
            "epoch 180/250, loss=0.0023, time=0.02.\n",
            "epoch 181/250, loss=0.0023, time=0.02.\n",
            "epoch 182/250, loss=0.0021, time=0.02.\n",
            "epoch 183/250, loss=0.0023, time=0.02.\n",
            "epoch 184/250, loss=0.0022, time=0.02.\n",
            "epoch 185/250, loss=0.0021, time=0.02.\n",
            "epoch 186/250, loss=0.0022, time=0.02.\n",
            "epoch 187/250, loss=0.0022, time=0.02.\n",
            "epoch 188/250, loss=0.0022, time=0.02.\n",
            "epoch 189/250, loss=0.0021, time=0.02.\n",
            "epoch 190/250, loss=0.0020, time=0.02.\n",
            "epoch 191/250, loss=0.0020, time=0.02.\n",
            "epoch 192/250, loss=0.0020, time=0.02.\n",
            "epoch 193/250, loss=0.0020, time=0.02.\n",
            "epoch 194/250, loss=0.0020, time=0.02.\n",
            "epoch 195/250, loss=0.0020, time=0.02.\n",
            "epoch 196/250, loss=0.0021, time=0.02.\n",
            "epoch 197/250, loss=0.0020, time=0.02.\n",
            "epoch 198/250, loss=0.0020, time=0.02.\n",
            "epoch 199/250, loss=0.0019, time=0.02.\n",
            "epoch 200/250, loss=0.0019, time=0.02.\n",
            "epoch 201/250, loss=0.0019, time=0.02.\n",
            "epoch 202/250, loss=0.0019, time=0.02.\n",
            "epoch 203/250, loss=0.0019, time=0.02.\n",
            "epoch 204/250, loss=0.0018, time=0.02.\n",
            "epoch 205/250, loss=0.0019, time=0.02.\n",
            "epoch 206/250, loss=0.0018, time=0.02.\n",
            "epoch 207/250, loss=0.0018, time=0.02.\n",
            "epoch 208/250, loss=0.0018, time=0.02.\n",
            "epoch 209/250, loss=0.0018, time=0.02.\n",
            "epoch 210/250, loss=0.0018, time=0.02.\n",
            "epoch 211/250, loss=0.0018, time=0.02.\n",
            "epoch 212/250, loss=0.0018, time=0.02.\n",
            "epoch 213/250, loss=0.0017, time=0.02.\n",
            "epoch 214/250, loss=0.0017, time=0.02.\n",
            "epoch 215/250, loss=0.0017, time=0.02.\n",
            "epoch 216/250, loss=0.0017, time=0.02.\n",
            "epoch 217/250, loss=0.0017, time=0.02.\n",
            "epoch 218/250, loss=0.0016, time=0.02.\n",
            "epoch 219/250, loss=0.0016, time=0.02.\n",
            "epoch 220/250, loss=0.0016, time=0.02.\n",
            "epoch 221/250, loss=0.0016, time=0.02.\n",
            "epoch 222/250, loss=0.0016, time=0.02.\n",
            "epoch 223/250, loss=0.0016, time=0.02.\n",
            "epoch 224/250, loss=0.0016, time=0.02.\n",
            "epoch 225/250, loss=0.0015, time=0.02.\n",
            "epoch 226/250, loss=0.0015, time=0.02.\n",
            "epoch 227/250, loss=0.0015, time=0.02.\n",
            "epoch 228/250, loss=0.0016, time=0.02.\n",
            "epoch 229/250, loss=0.0015, time=0.02.\n",
            "epoch 230/250, loss=0.0015, time=0.02.\n",
            "epoch 231/250, loss=0.0015, time=0.02.\n",
            "epoch 232/250, loss=0.0015, time=0.02.\n",
            "epoch 233/250, loss=0.0014, time=0.02.\n",
            "epoch 234/250, loss=0.0015, time=0.02.\n",
            "epoch 235/250, loss=0.0015, time=0.02.\n",
            "epoch 236/250, loss=0.0014, time=0.02.\n",
            "epoch 237/250, loss=0.0014, time=0.02.\n",
            "epoch 238/250, loss=0.0015, time=0.02.\n",
            "epoch 239/250, loss=0.0014, time=0.02.\n",
            "epoch 240/250, loss=0.0014, time=0.02.\n",
            "epoch 241/250, loss=0.0014, time=0.02.\n",
            "epoch 242/250, loss=0.0014, time=0.02.\n",
            "epoch 243/250, loss=0.0014, time=0.02.\n",
            "epoch 244/250, loss=0.0014, time=0.02.\n",
            "epoch 245/250, loss=0.0014, time=0.02.\n",
            "epoch 246/250, loss=0.0014, time=0.02.\n",
            "epoch 247/250, loss=0.0013, time=0.02.\n",
            "epoch 248/250, loss=0.0013, time=0.02.\n",
            "epoch 249/250, loss=0.0014, time=0.02.\n",
            "epoch 250/250, loss=0.0014, time=0.02.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0139.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0004.npz, 0.0192, 0.0000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0034.npz, 0.0577, 0.0577, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0164.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0049.npz, 0.0385, 0.0769, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0189.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0134.npz, 0.0385, 0.0577, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0109.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0219.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0159.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0194.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0064.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0244.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0044.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0234.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0239.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0114.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0204.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0009.npz, 0.0000, 0.0192, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0019.npz, 0.0577, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0094.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0209.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0104.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0039.npz, 0.0385, 0.0192, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0024.npz, 0.0385, 0.0192, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0079.npz, 0.0385, 0.0192, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0214.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0074.npz, 0.0385, 0.0192, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0059.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0184.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0084.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0249.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0054.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0014.npz, 0.0385, 0.0192, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0169.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0119.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0229.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0099.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0179.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0154.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0129.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0174.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0144.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0199.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0224.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0089.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0069.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0149.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0029.npz, 0.0385, 0.0192, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigAirwayPressure/alst_sc_c_0000_0124.npz, 0.0385, 0.0385, 0.01\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=3.9928, time=0.03.\n",
            "epoch 2/250, loss=3.9310, time=0.03.\n",
            "epoch 3/250, loss=3.8517, time=0.03.\n",
            "epoch 4/250, loss=3.7285, time=0.03.\n",
            "epoch 5/250, loss=3.5900, time=0.03.\n",
            "epoch 6/250, loss=3.4251, time=0.02.\n",
            "epoch 7/250, loss=3.3193, time=0.02.\n",
            "epoch 8/250, loss=3.1537, time=0.02.\n",
            "epoch 9/250, loss=2.9710, time=0.02.\n",
            "epoch 10/250, loss=2.7757, time=0.02.\n",
            "epoch 11/250, loss=2.5018, time=0.02.\n",
            "epoch 12/250, loss=2.3341, time=0.02.\n",
            "epoch 13/250, loss=2.1137, time=0.02.\n",
            "epoch 14/250, loss=1.9744, time=0.02.\n",
            "epoch 15/250, loss=1.7508, time=0.02.\n",
            "epoch 16/250, loss=1.5360, time=0.02.\n",
            "epoch 17/250, loss=1.3764, time=0.02.\n",
            "epoch 18/250, loss=1.2215, time=0.02.\n",
            "epoch 19/250, loss=1.0875, time=0.02.\n",
            "epoch 20/250, loss=0.9306, time=0.02.\n",
            "epoch 21/250, loss=0.7324, time=0.02.\n",
            "epoch 22/250, loss=0.6255, time=0.02.\n",
            "epoch 23/250, loss=0.5577, time=0.02.\n",
            "epoch 24/250, loss=0.4314, time=0.02.\n",
            "epoch 25/250, loss=0.4791, time=0.02.\n",
            "epoch 26/250, loss=0.4034, time=0.02.\n",
            "epoch 27/250, loss=0.3402, time=0.02.\n",
            "epoch 28/250, loss=0.2165, time=0.02.\n",
            "epoch 29/250, loss=0.2241, time=0.02.\n",
            "epoch 30/250, loss=0.1513, time=0.02.\n",
            "epoch 31/250, loss=0.1477, time=0.02.\n",
            "epoch 32/250, loss=0.1218, time=0.02.\n",
            "epoch 33/250, loss=0.0980, time=0.02.\n",
            "epoch 34/250, loss=0.0716, time=0.02.\n",
            "epoch 35/250, loss=0.0794, time=0.02.\n",
            "epoch 36/250, loss=0.0612, time=0.02.\n",
            "epoch 37/250, loss=0.0514, time=0.02.\n",
            "epoch 38/250, loss=0.0479, time=0.02.\n",
            "epoch 39/250, loss=0.0370, time=0.02.\n",
            "epoch 40/250, loss=0.0372, time=0.02.\n",
            "epoch 41/250, loss=0.0322, time=0.02.\n",
            "epoch 42/250, loss=0.0266, time=0.02.\n",
            "epoch 43/250, loss=0.0255, time=0.02.\n",
            "epoch 44/250, loss=0.0213, time=0.02.\n",
            "epoch 45/250, loss=0.0189, time=0.02.\n",
            "epoch 46/250, loss=0.0175, time=0.02.\n",
            "epoch 47/250, loss=0.0165, time=0.02.\n",
            "epoch 48/250, loss=0.0155, time=0.02.\n",
            "epoch 49/250, loss=0.0142, time=0.02.\n",
            "epoch 50/250, loss=0.0136, time=0.02.\n",
            "epoch 51/250, loss=0.0122, time=0.02.\n",
            "epoch 52/250, loss=0.0117, time=0.02.\n",
            "epoch 53/250, loss=0.0108, time=0.02.\n",
            "epoch 54/250, loss=0.0105, time=0.02.\n",
            "epoch 55/250, loss=0.0101, time=0.02.\n",
            "epoch 56/250, loss=0.0096, time=0.02.\n",
            "epoch 57/250, loss=0.0090, time=0.02.\n",
            "epoch 58/250, loss=0.0089, time=0.02.\n",
            "epoch 59/250, loss=0.0087, time=0.02.\n",
            "epoch 60/250, loss=0.0082, time=0.02.\n",
            "epoch 61/250, loss=0.0080, time=0.02.\n",
            "epoch 62/250, loss=0.0076, time=0.02.\n",
            "epoch 63/250, loss=0.0074, time=0.02.\n",
            "epoch 64/250, loss=0.0074, time=0.02.\n",
            "epoch 65/250, loss=0.0072, time=0.02.\n",
            "epoch 66/250, loss=0.0068, time=0.02.\n",
            "epoch 67/250, loss=0.0066, time=0.02.\n",
            "epoch 68/250, loss=0.0065, time=0.02.\n",
            "epoch 69/250, loss=0.0065, time=0.02.\n",
            "epoch 70/250, loss=0.0064, time=0.02.\n",
            "epoch 71/250, loss=0.0062, time=0.02.\n",
            "epoch 72/250, loss=0.0061, time=0.02.\n",
            "epoch 73/250, loss=0.0058, time=0.02.\n",
            "epoch 74/250, loss=0.0058, time=0.02.\n",
            "epoch 75/250, loss=0.0056, time=0.02.\n",
            "epoch 76/250, loss=0.0053, time=0.02.\n",
            "epoch 77/250, loss=0.0054, time=0.02.\n",
            "epoch 78/250, loss=0.0053, time=0.02.\n",
            "epoch 79/250, loss=0.0051, time=0.02.\n",
            "epoch 80/250, loss=0.0050, time=0.02.\n",
            "epoch 81/250, loss=0.0050, time=0.02.\n",
            "epoch 82/250, loss=0.0048, time=0.02.\n",
            "epoch 83/250, loss=0.0048, time=0.02.\n",
            "epoch 84/250, loss=0.0046, time=0.02.\n",
            "epoch 85/250, loss=0.0046, time=0.02.\n",
            "epoch 86/250, loss=0.0044, time=0.02.\n",
            "epoch 87/250, loss=0.0045, time=0.02.\n",
            "epoch 88/250, loss=0.0044, time=0.02.\n",
            "epoch 89/250, loss=0.0043, time=0.02.\n",
            "epoch 90/250, loss=0.0041, time=0.02.\n",
            "epoch 91/250, loss=0.0040, time=0.02.\n",
            "epoch 92/250, loss=0.0040, time=0.02.\n",
            "epoch 93/250, loss=0.0039, time=0.02.\n",
            "epoch 94/250, loss=0.0039, time=0.02.\n",
            "epoch 95/250, loss=0.0038, time=0.02.\n",
            "epoch 96/250, loss=0.0038, time=0.02.\n",
            "epoch 97/250, loss=0.0037, time=0.02.\n",
            "epoch 98/250, loss=0.0037, time=0.02.\n",
            "epoch 99/250, loss=0.0036, time=0.02.\n",
            "epoch 100/250, loss=0.0035, time=0.02.\n",
            "epoch 101/250, loss=0.0035, time=0.02.\n",
            "epoch 102/250, loss=0.0034, time=0.02.\n",
            "epoch 103/250, loss=0.0033, time=0.02.\n",
            "epoch 104/250, loss=0.0033, time=0.02.\n",
            "epoch 105/250, loss=0.0033, time=0.02.\n",
            "epoch 106/250, loss=0.0033, time=0.02.\n",
            "epoch 107/250, loss=0.0032, time=0.02.\n",
            "epoch 108/250, loss=0.0032, time=0.02.\n",
            "epoch 109/250, loss=0.0031, time=0.02.\n",
            "epoch 110/250, loss=0.0031, time=0.02.\n",
            "epoch 111/250, loss=0.0031, time=0.02.\n",
            "epoch 112/250, loss=0.0029, time=0.02.\n",
            "epoch 113/250, loss=0.0029, time=0.02.\n",
            "epoch 114/250, loss=0.0029, time=0.02.\n",
            "epoch 115/250, loss=0.0028, time=0.02.\n",
            "epoch 116/250, loss=0.0028, time=0.02.\n",
            "epoch 117/250, loss=0.0028, time=0.02.\n",
            "epoch 118/250, loss=0.0028, time=0.02.\n",
            "epoch 119/250, loss=0.0027, time=0.02.\n",
            "epoch 120/250, loss=0.0027, time=0.02.\n",
            "epoch 121/250, loss=0.0027, time=0.02.\n",
            "epoch 122/250, loss=0.0026, time=0.02.\n",
            "epoch 123/250, loss=0.0025, time=0.02.\n",
            "epoch 124/250, loss=0.0025, time=0.02.\n",
            "epoch 125/250, loss=0.0026, time=0.02.\n",
            "epoch 126/250, loss=0.0024, time=0.02.\n",
            "epoch 127/250, loss=0.0024, time=0.02.\n",
            "epoch 128/250, loss=0.0024, time=0.02.\n",
            "epoch 129/250, loss=0.0024, time=0.02.\n",
            "epoch 130/250, loss=0.0024, time=0.02.\n",
            "epoch 131/250, loss=0.0023, time=0.02.\n",
            "epoch 132/250, loss=0.0023, time=0.02.\n",
            "epoch 133/250, loss=0.0023, time=0.02.\n",
            "epoch 134/250, loss=0.0023, time=0.02.\n",
            "epoch 135/250, loss=0.0022, time=0.02.\n",
            "epoch 136/250, loss=0.0022, time=0.02.\n",
            "epoch 137/250, loss=0.0022, time=0.02.\n",
            "epoch 138/250, loss=0.0021, time=0.02.\n",
            "epoch 139/250, loss=0.0021, time=0.02.\n",
            "epoch 140/250, loss=0.0021, time=0.02.\n",
            "epoch 141/250, loss=0.0021, time=0.02.\n",
            "epoch 142/250, loss=0.0021, time=0.02.\n",
            "epoch 143/250, loss=0.0021, time=0.02.\n",
            "epoch 144/250, loss=0.0020, time=0.02.\n",
            "epoch 145/250, loss=0.0020, time=0.02.\n",
            "epoch 146/250, loss=0.0019, time=0.02.\n",
            "epoch 147/250, loss=0.0020, time=0.02.\n",
            "epoch 148/250, loss=0.0019, time=0.02.\n",
            "epoch 149/250, loss=0.0019, time=0.02.\n",
            "epoch 150/250, loss=0.0019, time=0.02.\n",
            "epoch 151/250, loss=0.0019, time=0.02.\n",
            "epoch 152/250, loss=0.0018, time=0.02.\n",
            "epoch 153/250, loss=0.0018, time=0.02.\n",
            "epoch 154/250, loss=0.0018, time=0.02.\n",
            "epoch 155/250, loss=0.0018, time=0.02.\n",
            "epoch 156/250, loss=0.0018, time=0.02.\n",
            "epoch 157/250, loss=0.0017, time=0.02.\n",
            "epoch 158/250, loss=0.0017, time=0.02.\n",
            "epoch 159/250, loss=0.0017, time=0.02.\n",
            "epoch 160/250, loss=0.0017, time=0.02.\n",
            "epoch 161/250, loss=0.0017, time=0.02.\n",
            "epoch 162/250, loss=0.0017, time=0.02.\n",
            "epoch 163/250, loss=0.0016, time=0.02.\n",
            "epoch 164/250, loss=0.0017, time=0.02.\n",
            "epoch 165/250, loss=0.0016, time=0.02.\n",
            "epoch 166/250, loss=0.0016, time=0.02.\n",
            "epoch 167/250, loss=0.0016, time=0.02.\n",
            "epoch 168/250, loss=0.0015, time=0.02.\n",
            "epoch 169/250, loss=0.0015, time=0.02.\n",
            "epoch 170/250, loss=0.0015, time=0.02.\n",
            "epoch 171/250, loss=0.0015, time=0.02.\n",
            "epoch 172/250, loss=0.0015, time=0.02.\n",
            "epoch 173/250, loss=0.0015, time=0.02.\n",
            "epoch 174/250, loss=0.0015, time=0.02.\n",
            "epoch 175/250, loss=0.0015, time=0.02.\n",
            "epoch 176/250, loss=0.0015, time=0.02.\n",
            "epoch 177/250, loss=0.0014, time=0.02.\n",
            "epoch 178/250, loss=0.0014, time=0.02.\n",
            "epoch 179/250, loss=0.0014, time=0.02.\n",
            "epoch 180/250, loss=0.0014, time=0.02.\n",
            "epoch 181/250, loss=0.0014, time=0.02.\n",
            "epoch 182/250, loss=0.0014, time=0.02.\n",
            "epoch 183/250, loss=0.0014, time=0.02.\n",
            "epoch 184/250, loss=0.0014, time=0.02.\n",
            "epoch 185/250, loss=0.0013, time=0.02.\n",
            "epoch 186/250, loss=0.0013, time=0.02.\n",
            "epoch 187/250, loss=0.0013, time=0.02.\n",
            "epoch 188/250, loss=0.0013, time=0.02.\n",
            "epoch 189/250, loss=0.0013, time=0.02.\n",
            "epoch 190/250, loss=0.0013, time=0.02.\n",
            "epoch 191/250, loss=0.0013, time=0.02.\n",
            "epoch 192/250, loss=0.0013, time=0.02.\n",
            "epoch 193/250, loss=0.0012, time=0.02.\n",
            "epoch 194/250, loss=0.0012, time=0.02.\n",
            "epoch 195/250, loss=0.0013, time=0.02.\n",
            "epoch 196/250, loss=0.0012, time=0.02.\n",
            "epoch 197/250, loss=0.0012, time=0.02.\n",
            "epoch 198/250, loss=0.0012, time=0.02.\n",
            "epoch 199/250, loss=0.0012, time=0.02.\n",
            "epoch 200/250, loss=0.0012, time=0.02.\n",
            "epoch 201/250, loss=0.0012, time=0.02.\n",
            "epoch 202/250, loss=0.0012, time=0.02.\n",
            "epoch 203/250, loss=0.0012, time=0.02.\n",
            "epoch 204/250, loss=0.0011, time=0.02.\n",
            "epoch 205/250, loss=0.0011, time=0.02.\n",
            "epoch 206/250, loss=0.0011, time=0.02.\n",
            "epoch 207/250, loss=0.0011, time=0.02.\n",
            "epoch 208/250, loss=0.0011, time=0.02.\n",
            "epoch 209/250, loss=0.0011, time=0.02.\n",
            "epoch 210/250, loss=0.0011, time=0.02.\n",
            "epoch 211/250, loss=0.0011, time=0.02.\n",
            "epoch 212/250, loss=0.0011, time=0.02.\n",
            "epoch 213/250, loss=0.0011, time=0.02.\n",
            "epoch 214/250, loss=0.0011, time=0.02.\n",
            "epoch 215/250, loss=0.0011, time=0.02.\n",
            "epoch 216/250, loss=0.0011, time=0.02.\n",
            "epoch 217/250, loss=0.0010, time=0.02.\n",
            "epoch 218/250, loss=0.0010, time=0.02.\n",
            "epoch 219/250, loss=0.0010, time=0.02.\n",
            "epoch 220/250, loss=0.0010, time=0.02.\n",
            "epoch 221/250, loss=0.0010, time=0.02.\n",
            "epoch 222/250, loss=0.0010, time=0.02.\n",
            "epoch 223/250, loss=0.0010, time=0.02.\n",
            "epoch 224/250, loss=0.0010, time=0.02.\n",
            "epoch 225/250, loss=0.0010, time=0.02.\n",
            "epoch 226/250, loss=0.0010, time=0.02.\n",
            "epoch 227/250, loss=0.0010, time=0.02.\n",
            "epoch 228/250, loss=0.0010, time=0.02.\n",
            "epoch 229/250, loss=0.0010, time=0.02.\n",
            "epoch 230/250, loss=0.0010, time=0.02.\n",
            "epoch 231/250, loss=0.0010, time=0.02.\n",
            "epoch 232/250, loss=0.0009, time=0.02.\n",
            "epoch 233/250, loss=0.0009, time=0.02.\n",
            "epoch 234/250, loss=0.0009, time=0.02.\n",
            "epoch 235/250, loss=0.0009, time=0.02.\n",
            "epoch 236/250, loss=0.0009, time=0.02.\n",
            "epoch 237/250, loss=0.0009, time=0.02.\n",
            "epoch 238/250, loss=0.0009, time=0.02.\n",
            "epoch 239/250, loss=0.0009, time=0.02.\n",
            "epoch 240/250, loss=0.0009, time=0.02.\n",
            "epoch 241/250, loss=0.0009, time=0.02.\n",
            "epoch 242/250, loss=0.0009, time=0.02.\n",
            "epoch 243/250, loss=0.0009, time=0.02.\n",
            "epoch 244/250, loss=0.0009, time=0.02.\n",
            "epoch 245/250, loss=0.0009, time=0.02.\n",
            "epoch 246/250, loss=0.0008, time=0.02.\n",
            "epoch 247/250, loss=0.0008, time=0.02.\n",
            "epoch 248/250, loss=0.0008, time=0.02.\n",
            "epoch 249/250, loss=0.0008, time=0.02.\n",
            "epoch 250/250, loss=0.0008, time=0.02.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0054.npz, 0.1154, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0069.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0244.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0114.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0049.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0229.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0099.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0089.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0014.npz, 0.1346, 0.1346, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0189.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0139.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0179.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0209.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0154.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0239.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0019.npz, 0.1154, 0.1154, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0219.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0164.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0039.npz, 0.1346, 0.1923, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0214.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0119.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0084.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0234.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0184.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0169.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0134.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0224.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0044.npz, 0.1154, 0.1923, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0174.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0149.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0144.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0124.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0104.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0004.npz, 0.0192, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0204.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0094.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0074.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0249.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0064.npz, 0.1346, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0079.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0199.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0034.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0194.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0029.npz, 0.1731, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0159.npz, 0.1538, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0009.npz, 0.1538, 0.0962, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0129.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0059.npz, 0.1346, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0024.npz, 0.1346, 0.1346, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigArtPressure/alst_sc_c_0000_0109.npz, 0.1538, 0.1538, 0.01\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=4.0618, time=0.03.\n",
            "epoch 2/250, loss=3.9038, time=0.03.\n",
            "epoch 3/250, loss=3.7352, time=0.03.\n",
            "epoch 4/250, loss=3.4752, time=0.03.\n",
            "epoch 5/250, loss=3.3289, time=0.03.\n",
            "epoch 6/250, loss=3.1202, time=0.03.\n",
            "epoch 7/250, loss=2.9110, time=0.03.\n",
            "epoch 8/250, loss=2.7071, time=0.03.\n",
            "epoch 9/250, loss=2.5140, time=0.02.\n",
            "epoch 10/250, loss=2.3311, time=0.02.\n",
            "epoch 11/250, loss=2.0696, time=0.02.\n",
            "epoch 12/250, loss=1.8475, time=0.02.\n",
            "epoch 13/250, loss=1.6366, time=0.02.\n",
            "epoch 14/250, loss=1.4557, time=0.02.\n",
            "epoch 15/250, loss=1.1602, time=0.02.\n",
            "epoch 16/250, loss=1.0070, time=0.02.\n",
            "epoch 17/250, loss=0.8711, time=0.02.\n",
            "epoch 18/250, loss=0.6657, time=0.02.\n",
            "epoch 19/250, loss=0.5406, time=0.02.\n",
            "epoch 20/250, loss=0.4289, time=0.02.\n",
            "epoch 21/250, loss=0.3401, time=0.02.\n",
            "epoch 22/250, loss=0.2579, time=0.02.\n",
            "epoch 23/250, loss=0.1973, time=0.02.\n",
            "epoch 24/250, loss=0.1449, time=0.02.\n",
            "epoch 25/250, loss=0.1172, time=0.02.\n",
            "epoch 26/250, loss=0.0987, time=0.02.\n",
            "epoch 27/250, loss=0.0754, time=0.02.\n",
            "epoch 28/250, loss=0.0589, time=0.02.\n",
            "epoch 29/250, loss=0.0524, time=0.02.\n",
            "epoch 30/250, loss=0.0432, time=0.02.\n",
            "epoch 31/250, loss=0.0361, time=0.02.\n",
            "epoch 32/250, loss=0.0307, time=0.02.\n",
            "epoch 33/250, loss=0.0278, time=0.02.\n",
            "epoch 34/250, loss=0.0234, time=0.02.\n",
            "epoch 35/250, loss=0.0210, time=0.02.\n",
            "epoch 36/250, loss=0.0199, time=0.02.\n",
            "epoch 37/250, loss=0.0178, time=0.02.\n",
            "epoch 38/250, loss=0.0157, time=0.02.\n",
            "epoch 39/250, loss=0.0142, time=0.02.\n",
            "epoch 40/250, loss=0.0137, time=0.02.\n",
            "epoch 41/250, loss=0.0126, time=0.02.\n",
            "epoch 42/250, loss=0.0117, time=0.02.\n",
            "epoch 43/250, loss=0.0110, time=0.02.\n",
            "epoch 44/250, loss=0.0106, time=0.02.\n",
            "epoch 45/250, loss=0.0098, time=0.02.\n",
            "epoch 46/250, loss=0.0093, time=0.02.\n",
            "epoch 47/250, loss=0.0089, time=0.02.\n",
            "epoch 48/250, loss=0.0086, time=0.02.\n",
            "epoch 49/250, loss=0.0086, time=0.02.\n",
            "epoch 50/250, loss=0.0082, time=0.02.\n",
            "epoch 51/250, loss=0.0081, time=0.02.\n",
            "epoch 52/250, loss=0.0077, time=0.02.\n",
            "epoch 53/250, loss=0.0076, time=0.02.\n",
            "epoch 54/250, loss=0.0072, time=0.02.\n",
            "epoch 55/250, loss=0.0071, time=0.02.\n",
            "epoch 56/250, loss=0.0068, time=0.02.\n",
            "epoch 57/250, loss=0.0065, time=0.02.\n",
            "epoch 58/250, loss=0.0065, time=0.02.\n",
            "epoch 59/250, loss=0.0061, time=0.02.\n",
            "epoch 60/250, loss=0.0062, time=0.02.\n",
            "epoch 61/250, loss=0.0060, time=0.02.\n",
            "epoch 62/250, loss=0.0058, time=0.02.\n",
            "epoch 63/250, loss=0.0059, time=0.02.\n",
            "epoch 64/250, loss=0.0055, time=0.02.\n",
            "epoch 65/250, loss=0.0053, time=0.02.\n",
            "epoch 66/250, loss=0.0051, time=0.02.\n",
            "epoch 67/250, loss=0.0050, time=0.02.\n",
            "epoch 68/250, loss=0.0050, time=0.02.\n",
            "epoch 69/250, loss=0.0049, time=0.02.\n",
            "epoch 70/250, loss=0.0048, time=0.02.\n",
            "epoch 71/250, loss=0.0048, time=0.02.\n",
            "epoch 72/250, loss=0.0046, time=0.02.\n",
            "epoch 73/250, loss=0.0046, time=0.02.\n",
            "epoch 74/250, loss=0.0044, time=0.02.\n",
            "epoch 75/250, loss=0.0044, time=0.02.\n",
            "epoch 76/250, loss=0.0043, time=0.02.\n",
            "epoch 77/250, loss=0.0042, time=0.02.\n",
            "epoch 78/250, loss=0.0042, time=0.02.\n",
            "epoch 79/250, loss=0.0040, time=0.02.\n",
            "epoch 80/250, loss=0.0039, time=0.02.\n",
            "epoch 81/250, loss=0.0039, time=0.02.\n",
            "epoch 82/250, loss=0.0038, time=0.02.\n",
            "epoch 83/250, loss=0.0038, time=0.02.\n",
            "epoch 84/250, loss=0.0037, time=0.02.\n",
            "epoch 85/250, loss=0.0037, time=0.02.\n",
            "epoch 86/250, loss=0.0035, time=0.02.\n",
            "epoch 87/250, loss=0.0034, time=0.02.\n",
            "epoch 88/250, loss=0.0034, time=0.02.\n",
            "epoch 89/250, loss=0.0033, time=0.02.\n",
            "epoch 90/250, loss=0.0033, time=0.02.\n",
            "epoch 91/250, loss=0.0033, time=0.02.\n",
            "epoch 92/250, loss=0.0033, time=0.02.\n",
            "epoch 93/250, loss=0.0032, time=0.02.\n",
            "epoch 94/250, loss=0.0032, time=0.02.\n",
            "epoch 95/250, loss=0.0031, time=0.02.\n",
            "epoch 96/250, loss=0.0031, time=0.02.\n",
            "epoch 97/250, loss=0.0030, time=0.02.\n",
            "epoch 98/250, loss=0.0029, time=0.02.\n",
            "epoch 99/250, loss=0.0029, time=0.02.\n",
            "epoch 100/250, loss=0.0029, time=0.02.\n",
            "epoch 101/250, loss=0.0028, time=0.02.\n",
            "epoch 102/250, loss=0.0028, time=0.02.\n",
            "epoch 103/250, loss=0.0028, time=0.02.\n",
            "epoch 104/250, loss=0.0027, time=0.02.\n",
            "epoch 105/250, loss=0.0027, time=0.02.\n",
            "epoch 106/250, loss=0.0026, time=0.02.\n",
            "epoch 107/250, loss=0.0026, time=0.02.\n",
            "epoch 108/250, loss=0.0025, time=0.02.\n",
            "epoch 109/250, loss=0.0025, time=0.02.\n",
            "epoch 110/250, loss=0.0025, time=0.02.\n",
            "epoch 111/250, loss=0.0024, time=0.02.\n",
            "epoch 112/250, loss=0.0024, time=0.02.\n",
            "epoch 113/250, loss=0.0023, time=0.02.\n",
            "epoch 114/250, loss=0.0023, time=0.02.\n",
            "epoch 115/250, loss=0.0023, time=0.02.\n",
            "epoch 116/250, loss=0.0023, time=0.02.\n",
            "epoch 117/250, loss=0.0022, time=0.02.\n",
            "epoch 118/250, loss=0.0022, time=0.02.\n",
            "epoch 119/250, loss=0.0022, time=0.02.\n",
            "epoch 120/250, loss=0.0022, time=0.02.\n",
            "epoch 121/250, loss=0.0021, time=0.02.\n",
            "epoch 122/250, loss=0.0021, time=0.02.\n",
            "epoch 123/250, loss=0.0021, time=0.02.\n",
            "epoch 124/250, loss=0.0021, time=0.02.\n",
            "epoch 125/250, loss=0.0020, time=0.02.\n",
            "epoch 126/250, loss=0.0020, time=0.02.\n",
            "epoch 127/250, loss=0.0019, time=0.02.\n",
            "epoch 128/250, loss=0.0019, time=0.02.\n",
            "epoch 129/250, loss=0.0019, time=0.02.\n",
            "epoch 130/250, loss=0.0019, time=0.02.\n",
            "epoch 131/250, loss=0.0019, time=0.02.\n",
            "epoch 132/250, loss=0.0019, time=0.02.\n",
            "epoch 133/250, loss=0.0019, time=0.02.\n",
            "epoch 134/250, loss=0.0018, time=0.02.\n",
            "epoch 135/250, loss=0.0018, time=0.02.\n",
            "epoch 136/250, loss=0.0017, time=0.02.\n",
            "epoch 137/250, loss=0.0017, time=0.02.\n",
            "epoch 138/250, loss=0.0018, time=0.02.\n",
            "epoch 139/250, loss=0.0017, time=0.02.\n",
            "epoch 140/250, loss=0.0017, time=0.02.\n",
            "epoch 141/250, loss=0.0017, time=0.02.\n",
            "epoch 142/250, loss=0.0017, time=0.02.\n",
            "epoch 143/250, loss=0.0016, time=0.02.\n",
            "epoch 144/250, loss=0.0016, time=0.02.\n",
            "epoch 145/250, loss=0.0016, time=0.02.\n",
            "epoch 146/250, loss=0.0016, time=0.02.\n",
            "epoch 147/250, loss=0.0016, time=0.02.\n",
            "epoch 148/250, loss=0.0016, time=0.02.\n",
            "epoch 149/250, loss=0.0015, time=0.02.\n",
            "epoch 150/250, loss=0.0015, time=0.02.\n",
            "epoch 151/250, loss=0.0015, time=0.02.\n",
            "epoch 152/250, loss=0.0015, time=0.02.\n",
            "epoch 153/250, loss=0.0015, time=0.02.\n",
            "epoch 154/250, loss=0.0014, time=0.02.\n",
            "epoch 155/250, loss=0.0014, time=0.02.\n",
            "epoch 156/250, loss=0.0014, time=0.02.\n",
            "epoch 157/250, loss=0.0014, time=0.02.\n",
            "epoch 158/250, loss=0.0014, time=0.02.\n",
            "epoch 159/250, loss=0.0014, time=0.02.\n",
            "epoch 160/250, loss=0.0014, time=0.02.\n",
            "epoch 161/250, loss=0.0013, time=0.02.\n",
            "epoch 162/250, loss=0.0013, time=0.02.\n",
            "epoch 163/250, loss=0.0013, time=0.02.\n",
            "epoch 164/250, loss=0.0013, time=0.02.\n",
            "epoch 165/250, loss=0.0013, time=0.02.\n",
            "epoch 166/250, loss=0.0013, time=0.02.\n",
            "epoch 167/250, loss=0.0013, time=0.02.\n",
            "epoch 168/250, loss=0.0013, time=0.02.\n",
            "epoch 169/250, loss=0.0013, time=0.02.\n",
            "epoch 170/250, loss=0.0012, time=0.02.\n",
            "epoch 171/250, loss=0.0012, time=0.02.\n",
            "epoch 172/250, loss=0.0012, time=0.02.\n",
            "epoch 173/250, loss=0.0012, time=0.02.\n",
            "epoch 174/250, loss=0.0012, time=0.02.\n",
            "epoch 175/250, loss=0.0012, time=0.02.\n",
            "epoch 176/250, loss=0.0012, time=0.02.\n",
            "epoch 177/250, loss=0.0012, time=0.02.\n",
            "epoch 178/250, loss=0.0011, time=0.02.\n",
            "epoch 179/250, loss=0.0011, time=0.02.\n",
            "epoch 180/250, loss=0.0011, time=0.02.\n",
            "epoch 181/250, loss=0.0011, time=0.02.\n",
            "epoch 182/250, loss=0.0011, time=0.02.\n",
            "epoch 183/250, loss=0.0011, time=0.02.\n",
            "epoch 184/250, loss=0.0011, time=0.02.\n",
            "epoch 185/250, loss=0.0011, time=0.02.\n",
            "epoch 186/250, loss=0.0011, time=0.02.\n",
            "epoch 187/250, loss=0.0011, time=0.02.\n",
            "epoch 188/250, loss=0.0011, time=0.02.\n",
            "epoch 189/250, loss=0.0010, time=0.02.\n",
            "epoch 190/250, loss=0.0011, time=0.02.\n",
            "epoch 191/250, loss=0.0010, time=0.02.\n",
            "epoch 192/250, loss=0.0010, time=0.02.\n",
            "epoch 193/250, loss=0.0010, time=0.02.\n",
            "epoch 194/250, loss=0.0010, time=0.02.\n",
            "epoch 195/250, loss=0.0010, time=0.02.\n",
            "epoch 196/250, loss=0.0010, time=0.02.\n",
            "epoch 197/250, loss=0.0010, time=0.02.\n",
            "epoch 198/250, loss=0.0009, time=0.02.\n",
            "epoch 199/250, loss=0.0010, time=0.02.\n",
            "epoch 200/250, loss=0.0010, time=0.02.\n",
            "epoch 201/250, loss=0.0009, time=0.02.\n",
            "epoch 202/250, loss=0.0010, time=0.02.\n",
            "epoch 203/250, loss=0.0009, time=0.02.\n",
            "epoch 204/250, loss=0.0009, time=0.02.\n",
            "epoch 205/250, loss=0.0009, time=0.02.\n",
            "epoch 206/250, loss=0.0009, time=0.02.\n",
            "epoch 207/250, loss=0.0009, time=0.02.\n",
            "epoch 208/250, loss=0.0009, time=0.02.\n",
            "epoch 209/250, loss=0.0009, time=0.02.\n",
            "epoch 210/250, loss=0.0009, time=0.02.\n",
            "epoch 211/250, loss=0.0009, time=0.02.\n",
            "epoch 212/250, loss=0.0009, time=0.02.\n",
            "epoch 213/250, loss=0.0009, time=0.02.\n",
            "epoch 214/250, loss=0.0009, time=0.02.\n",
            "epoch 215/250, loss=0.0009, time=0.02.\n",
            "epoch 216/250, loss=0.0008, time=0.02.\n",
            "epoch 217/250, loss=0.0009, time=0.02.\n",
            "epoch 218/250, loss=0.0008, time=0.02.\n",
            "epoch 219/250, loss=0.0008, time=0.02.\n",
            "epoch 220/250, loss=0.0008, time=0.02.\n",
            "epoch 221/250, loss=0.0008, time=0.02.\n",
            "epoch 222/250, loss=0.0008, time=0.02.\n",
            "epoch 223/250, loss=0.0008, time=0.02.\n",
            "epoch 224/250, loss=0.0008, time=0.02.\n",
            "epoch 225/250, loss=0.0008, time=0.02.\n",
            "epoch 226/250, loss=0.0008, time=0.02.\n",
            "epoch 227/250, loss=0.0008, time=0.02.\n",
            "epoch 228/250, loss=0.0008, time=0.02.\n",
            "epoch 229/250, loss=0.0008, time=0.02.\n",
            "epoch 230/250, loss=0.0008, time=0.02.\n",
            "epoch 231/250, loss=0.0007, time=0.02.\n",
            "epoch 232/250, loss=0.0007, time=0.02.\n",
            "epoch 233/250, loss=0.0007, time=0.02.\n",
            "epoch 234/250, loss=0.0007, time=0.02.\n",
            "epoch 235/250, loss=0.0007, time=0.02.\n",
            "epoch 236/250, loss=0.0007, time=0.02.\n",
            "epoch 237/250, loss=0.0007, time=0.02.\n",
            "epoch 238/250, loss=0.0007, time=0.02.\n",
            "epoch 239/250, loss=0.0007, time=0.02.\n",
            "epoch 240/250, loss=0.0007, time=0.02.\n",
            "epoch 241/250, loss=0.0007, time=0.02.\n",
            "epoch 242/250, loss=0.0007, time=0.02.\n",
            "epoch 243/250, loss=0.0007, time=0.02.\n",
            "epoch 244/250, loss=0.0007, time=0.02.\n",
            "epoch 245/250, loss=0.0007, time=0.02.\n",
            "epoch 246/250, loss=0.0007, time=0.02.\n",
            "epoch 247/250, loss=0.0007, time=0.02.\n",
            "epoch 248/250, loss=0.0007, time=0.02.\n",
            "epoch 249/250, loss=0.0007, time=0.02.\n",
            "epoch 250/250, loss=0.0007, time=0.02.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0189.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0224.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0054.npz, 0.1731, 0.1923, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0169.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0244.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0214.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0029.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0074.npz, 0.1731, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0144.npz, 0.1731, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0039.npz, 0.1923, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0164.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0249.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0104.npz, 0.1923, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0019.npz, 0.1538, 0.1346, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0184.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0139.npz, 0.1731, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0109.npz, 0.1923, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0004.npz, 0.0385, 0.0385, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0239.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0119.npz, 0.1923, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0049.npz, 0.1731, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0229.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0024.npz, 0.1923, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0059.npz, 0.1731, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0159.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0009.npz, 0.1154, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0114.npz, 0.1923, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0099.npz, 0.1923, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0094.npz, 0.1923, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0234.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0044.npz, 0.1731, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0204.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0209.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0064.npz, 0.1731, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0084.npz, 0.1923, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0149.npz, 0.1731, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0129.npz, 0.1923, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0069.npz, 0.1731, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0124.npz, 0.1923, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0034.npz, 0.1923, 0.1731, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0089.npz, 0.1923, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0219.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0014.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0154.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0134.npz, 0.1923, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0199.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0079.npz, 0.1923, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0174.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0179.npz, 0.1538, 0.1538, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PigCVP/alst_sc_c_0000_0194.npz, 0.1538, 0.1538, 0.01\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=2.3808, time=0.09.\n",
            "epoch 2/250, loss=2.1683, time=0.09.\n",
            "epoch 3/250, loss=1.9459, time=0.09.\n",
            "epoch 4/250, loss=1.6959, time=0.06.\n",
            "epoch 5/250, loss=1.4584, time=0.05.\n",
            "epoch 6/250, loss=1.3566, time=0.05.\n",
            "epoch 7/250, loss=1.1865, time=0.05.\n",
            "epoch 8/250, loss=1.0600, time=0.05.\n",
            "epoch 9/250, loss=0.9141, time=0.06.\n",
            "epoch 10/250, loss=0.7626, time=0.06.\n",
            "epoch 11/250, loss=0.6895, time=0.05.\n",
            "epoch 12/250, loss=0.6296, time=0.05.\n",
            "epoch 13/250, loss=0.5678, time=0.06.\n",
            "epoch 14/250, loss=0.5429, time=0.05.\n",
            "epoch 15/250, loss=0.4807, time=0.05.\n",
            "epoch 16/250, loss=0.4483, time=0.05.\n",
            "epoch 17/250, loss=0.4855, time=0.05.\n",
            "epoch 18/250, loss=0.4363, time=0.05.\n",
            "epoch 19/250, loss=0.4435, time=0.05.\n",
            "epoch 20/250, loss=0.4044, time=0.05.\n",
            "epoch 21/250, loss=0.4466, time=0.05.\n",
            "epoch 22/250, loss=0.3619, time=0.06.\n",
            "epoch 23/250, loss=0.4001, time=0.05.\n",
            "epoch 24/250, loss=0.4334, time=0.05.\n",
            "epoch 25/250, loss=0.4054, time=0.06.\n",
            "epoch 26/250, loss=0.3822, time=0.05.\n",
            "epoch 27/250, loss=0.3383, time=0.05.\n",
            "epoch 28/250, loss=0.2710, time=0.05.\n",
            "epoch 29/250, loss=0.2488, time=0.06.\n",
            "epoch 30/250, loss=0.2319, time=0.06.\n",
            "epoch 31/250, loss=0.2136, time=0.05.\n",
            "epoch 32/250, loss=0.2188, time=0.05.\n",
            "epoch 33/250, loss=0.1925, time=0.06.\n",
            "epoch 34/250, loss=0.2889, time=0.05.\n",
            "epoch 35/250, loss=0.2401, time=0.05.\n",
            "epoch 36/250, loss=0.2779, time=0.05.\n",
            "epoch 37/250, loss=0.2253, time=0.05.\n",
            "epoch 38/250, loss=0.2627, time=0.06.\n",
            "epoch 39/250, loss=0.3204, time=0.05.\n",
            "epoch 40/250, loss=0.2570, time=0.06.\n",
            "epoch 41/250, loss=0.2404, time=0.05.\n",
            "epoch 42/250, loss=0.1983, time=0.05.\n",
            "epoch 43/250, loss=0.1850, time=0.05.\n",
            "epoch 44/250, loss=0.2081, time=0.06.\n",
            "epoch 45/250, loss=0.1692, time=0.06.\n",
            "epoch 46/250, loss=0.2220, time=0.05.\n",
            "epoch 47/250, loss=0.2552, time=0.06.\n",
            "epoch 48/250, loss=0.1859, time=0.05.\n",
            "epoch 49/250, loss=0.2081, time=0.05.\n",
            "epoch 50/250, loss=0.1579, time=0.05.\n",
            "epoch 51/250, loss=0.1394, time=0.05.\n",
            "epoch 52/250, loss=0.1441, time=0.06.\n",
            "epoch 53/250, loss=0.2268, time=0.05.\n",
            "epoch 54/250, loss=0.3124, time=0.05.\n",
            "epoch 55/250, loss=0.4655, time=0.06.\n",
            "epoch 56/250, loss=0.3490, time=0.05.\n",
            "epoch 57/250, loss=0.2551, time=0.05.\n",
            "epoch 58/250, loss=0.2966, time=0.06.\n",
            "epoch 59/250, loss=0.2224, time=0.06.\n",
            "epoch 60/250, loss=0.1825, time=0.05.\n",
            "epoch 61/250, loss=0.1743, time=0.05.\n",
            "epoch 62/250, loss=0.1172, time=0.05.\n",
            "epoch 63/250, loss=0.1283, time=0.05.\n",
            "epoch 64/250, loss=0.1497, time=0.05.\n",
            "epoch 65/250, loss=0.1693, time=0.06.\n",
            "epoch 66/250, loss=0.1174, time=0.05.\n",
            "epoch 67/250, loss=0.1309, time=0.05.\n",
            "epoch 68/250, loss=0.1124, time=0.06.\n",
            "epoch 69/250, loss=0.1121, time=0.05.\n",
            "epoch 70/250, loss=0.1335, time=0.05.\n",
            "epoch 71/250, loss=0.1428, time=0.05.\n",
            "epoch 72/250, loss=0.1186, time=0.06.\n",
            "epoch 73/250, loss=0.0816, time=0.06.\n",
            "epoch 74/250, loss=0.0820, time=0.06.\n",
            "epoch 75/250, loss=0.0771, time=0.06.\n",
            "epoch 76/250, loss=0.0650, time=0.05.\n",
            "epoch 77/250, loss=0.0545, time=0.05.\n",
            "epoch 78/250, loss=0.0575, time=0.06.\n",
            "epoch 79/250, loss=0.0595, time=0.06.\n",
            "epoch 80/250, loss=0.0749, time=0.06.\n",
            "epoch 81/250, loss=0.0908, time=0.05.\n",
            "epoch 82/250, loss=0.0680, time=0.06.\n",
            "epoch 83/250, loss=0.0624, time=0.05.\n",
            "epoch 84/250, loss=0.0590, time=0.06.\n",
            "epoch 85/250, loss=0.0486, time=0.05.\n",
            "epoch 86/250, loss=0.0621, time=0.05.\n",
            "epoch 87/250, loss=0.0497, time=0.05.\n",
            "epoch 88/250, loss=0.0611, time=0.06.\n",
            "epoch 89/250, loss=0.0548, time=0.05.\n",
            "epoch 90/250, loss=0.0503, time=0.05.\n",
            "epoch 91/250, loss=0.0751, time=0.05.\n",
            "epoch 92/250, loss=0.0519, time=0.06.\n",
            "epoch 93/250, loss=0.0644, time=0.05.\n",
            "epoch 94/250, loss=0.0781, time=0.05.\n",
            "epoch 95/250, loss=0.1163, time=0.06.\n",
            "epoch 96/250, loss=0.2316, time=0.05.\n",
            "epoch 97/250, loss=0.2464, time=0.05.\n",
            "epoch 98/250, loss=0.1730, time=0.05.\n",
            "epoch 99/250, loss=0.3567, time=0.06.\n",
            "epoch 100/250, loss=0.3833, time=0.06.\n",
            "epoch 101/250, loss=0.2849, time=0.05.\n",
            "epoch 102/250, loss=0.1846, time=0.05.\n",
            "epoch 103/250, loss=0.1691, time=0.05.\n",
            "epoch 104/250, loss=0.0927, time=0.05.\n",
            "epoch 105/250, loss=0.1096, time=0.06.\n",
            "epoch 106/250, loss=0.0959, time=0.05.\n",
            "epoch 107/250, loss=0.0780, time=0.05.\n",
            "epoch 108/250, loss=0.0693, time=0.06.\n",
            "epoch 109/250, loss=0.0777, time=0.06.\n",
            "epoch 110/250, loss=0.0556, time=0.06.\n",
            "epoch 111/250, loss=0.0717, time=0.05.\n",
            "epoch 112/250, loss=0.0624, time=0.05.\n",
            "epoch 113/250, loss=0.0619, time=0.05.\n",
            "epoch 114/250, loss=0.0838, time=0.06.\n",
            "epoch 115/250, loss=0.0435, time=0.06.\n",
            "epoch 116/250, loss=0.0539, time=0.05.\n",
            "epoch 117/250, loss=0.0500, time=0.05.\n",
            "epoch 118/250, loss=0.0443, time=0.06.\n",
            "epoch 119/250, loss=0.0471, time=0.06.\n",
            "epoch 120/250, loss=0.0398, time=0.05.\n",
            "epoch 121/250, loss=0.0401, time=0.05.\n",
            "epoch 122/250, loss=0.0341, time=0.05.\n",
            "epoch 123/250, loss=0.0473, time=0.05.\n",
            "epoch 124/250, loss=0.0426, time=0.06.\n",
            "epoch 125/250, loss=0.0511, time=0.05.\n",
            "epoch 126/250, loss=0.0549, time=0.05.\n",
            "epoch 127/250, loss=0.0693, time=0.06.\n",
            "epoch 128/250, loss=0.1057, time=0.06.\n",
            "epoch 129/250, loss=0.1836, time=0.05.\n",
            "epoch 130/250, loss=0.1206, time=0.06.\n",
            "epoch 131/250, loss=0.2083, time=0.05.\n",
            "epoch 132/250, loss=0.1454, time=0.06.\n",
            "epoch 133/250, loss=0.0996, time=0.05.\n",
            "epoch 134/250, loss=0.0866, time=0.05.\n",
            "epoch 135/250, loss=0.0816, time=0.06.\n",
            "epoch 136/250, loss=0.0731, time=0.05.\n",
            "epoch 137/250, loss=0.0473, time=0.06.\n",
            "epoch 138/250, loss=0.0462, time=0.06.\n",
            "epoch 139/250, loss=0.0561, time=0.06.\n",
            "epoch 140/250, loss=0.0620, time=0.05.\n",
            "epoch 141/250, loss=0.0526, time=0.05.\n",
            "epoch 142/250, loss=0.0459, time=0.06.\n",
            "epoch 143/250, loss=0.0425, time=0.05.\n",
            "epoch 144/250, loss=0.0351, time=0.06.\n",
            "epoch 145/250, loss=0.0409, time=0.06.\n",
            "epoch 146/250, loss=0.0404, time=0.05.\n",
            "epoch 147/250, loss=0.0377, time=0.05.\n",
            "epoch 148/250, loss=0.0417, time=0.06.\n",
            "epoch 149/250, loss=0.0466, time=0.06.\n",
            "epoch 150/250, loss=0.0347, time=0.06.\n",
            "epoch 151/250, loss=0.0456, time=0.05.\n",
            "epoch 152/250, loss=0.0567, time=0.05.\n",
            "epoch 153/250, loss=0.0452, time=0.05.\n",
            "epoch 154/250, loss=0.0566, time=0.05.\n",
            "epoch 155/250, loss=0.0558, time=0.06.\n",
            "epoch 156/250, loss=0.0471, time=0.05.\n",
            "epoch 157/250, loss=0.0580, time=0.05.\n",
            "epoch 158/250, loss=0.0688, time=0.06.\n",
            "epoch 159/250, loss=0.0648, time=0.05.\n",
            "epoch 160/250, loss=0.0734, time=0.06.\n",
            "epoch 161/250, loss=0.0534, time=0.05.\n",
            "epoch 162/250, loss=0.0926, time=0.06.\n",
            "epoch 163/250, loss=0.0726, time=0.06.\n",
            "epoch 164/250, loss=0.0600, time=0.06.\n",
            "epoch 165/250, loss=0.0926, time=0.06.\n",
            "epoch 166/250, loss=0.1273, time=0.05.\n",
            "epoch 167/250, loss=0.2425, time=0.06.\n",
            "epoch 168/250, loss=0.2705, time=0.06.\n",
            "epoch 169/250, loss=0.2096, time=0.06.\n",
            "epoch 170/250, loss=0.2343, time=0.05.\n",
            "epoch 171/250, loss=0.2313, time=0.05.\n",
            "epoch 172/250, loss=0.2915, time=0.06.\n",
            "epoch 173/250, loss=0.2627, time=0.06.\n",
            "epoch 174/250, loss=0.2185, time=0.05.\n",
            "epoch 175/250, loss=0.2009, time=0.06.\n",
            "epoch 176/250, loss=0.2796, time=0.05.\n",
            "epoch 177/250, loss=0.2598, time=0.06.\n",
            "epoch 178/250, loss=0.2201, time=0.06.\n",
            "epoch 179/250, loss=0.1895, time=0.05.\n",
            "epoch 180/250, loss=0.1584, time=0.06.\n",
            "epoch 181/250, loss=0.0823, time=0.05.\n",
            "epoch 182/250, loss=0.0620, time=0.06.\n",
            "epoch 183/250, loss=0.0439, time=0.06.\n",
            "epoch 184/250, loss=0.0420, time=0.05.\n",
            "epoch 185/250, loss=0.2144, time=0.06.\n",
            "epoch 186/250, loss=0.1092, time=0.05.\n",
            "epoch 187/250, loss=0.0749, time=0.06.\n",
            "epoch 188/250, loss=0.0582, time=0.06.\n",
            "epoch 189/250, loss=0.0451, time=0.05.\n",
            "epoch 190/250, loss=0.0766, time=0.06.\n",
            "epoch 191/250, loss=0.1728, time=0.05.\n",
            "epoch 192/250, loss=0.1282, time=0.06.\n",
            "epoch 193/250, loss=0.1426, time=0.06.\n",
            "epoch 194/250, loss=0.1117, time=0.06.\n",
            "epoch 195/250, loss=0.0812, time=0.06.\n",
            "epoch 196/250, loss=0.0546, time=0.05.\n",
            "epoch 197/250, loss=0.0773, time=0.05.\n",
            "epoch 198/250, loss=0.1117, time=0.06.\n",
            "epoch 199/250, loss=0.1137, time=0.05.\n",
            "epoch 200/250, loss=0.0622, time=0.05.\n",
            "epoch 201/250, loss=0.0982, time=0.05.\n",
            "epoch 202/250, loss=0.1950, time=0.05.\n",
            "epoch 203/250, loss=0.1587, time=0.06.\n",
            "epoch 204/250, loss=0.1132, time=0.05.\n",
            "epoch 205/250, loss=0.1490, time=0.06.\n",
            "epoch 206/250, loss=0.1903, time=0.05.\n",
            "epoch 207/250, loss=0.1164, time=0.05.\n",
            "epoch 208/250, loss=0.0894, time=0.06.\n",
            "epoch 209/250, loss=0.0581, time=0.06.\n",
            "epoch 210/250, loss=0.0493, time=0.06.\n",
            "epoch 211/250, loss=0.0510, time=0.05.\n",
            "epoch 212/250, loss=0.0492, time=0.06.\n",
            "epoch 213/250, loss=0.0478, time=0.06.\n",
            "epoch 214/250, loss=0.0444, time=0.06.\n",
            "epoch 215/250, loss=0.0336, time=0.05.\n",
            "epoch 216/250, loss=0.0323, time=0.05.\n",
            "epoch 217/250, loss=0.0337, time=0.05.\n",
            "epoch 218/250, loss=0.0253, time=0.06.\n",
            "epoch 219/250, loss=0.0229, time=0.06.\n",
            "epoch 220/250, loss=0.0224, time=0.06.\n",
            "epoch 221/250, loss=0.0186, time=0.05.\n",
            "epoch 222/250, loss=0.0265, time=0.06.\n",
            "epoch 223/250, loss=0.0199, time=0.06.\n",
            "epoch 224/250, loss=0.0246, time=0.06.\n",
            "epoch 225/250, loss=0.0172, time=0.06.\n",
            "epoch 226/250, loss=0.0170, time=0.05.\n",
            "epoch 227/250, loss=0.0190, time=0.06.\n",
            "epoch 228/250, loss=0.0277, time=0.06.\n",
            "epoch 229/250, loss=0.0257, time=0.06.\n",
            "epoch 230/250, loss=0.0173, time=0.06.\n",
            "epoch 231/250, loss=0.0324, time=0.05.\n",
            "epoch 232/250, loss=0.0270, time=0.05.\n",
            "epoch 233/250, loss=0.0196, time=0.06.\n",
            "epoch 234/250, loss=0.0210, time=0.06.\n",
            "epoch 235/250, loss=0.0198, time=0.06.\n",
            "epoch 236/250, loss=0.0312, time=0.05.\n",
            "epoch 237/250, loss=0.0293, time=0.05.\n",
            "epoch 238/250, loss=0.0266, time=0.06.\n",
            "epoch 239/250, loss=0.0220, time=0.05.\n",
            "epoch 240/250, loss=0.0230, time=0.06.\n",
            "epoch 241/250, loss=0.0183, time=0.05.\n",
            "epoch 242/250, loss=0.0197, time=0.06.\n",
            "epoch 243/250, loss=0.0157, time=0.06.\n",
            "epoch 244/250, loss=0.0308, time=0.06.\n",
            "epoch 245/250, loss=0.0289, time=0.06.\n",
            "epoch 246/250, loss=0.0176, time=0.05.\n",
            "epoch 247/250, loss=0.0165, time=0.05.\n",
            "epoch 248/250, loss=0.0238, time=0.06.\n",
            "epoch 249/250, loss=0.0190, time=0.06.\n",
            "epoch 250/250, loss=0.0217, time=0.06.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0044.npz, 0.5273, 0.5091, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0079.npz, 0.5909, 0.5091, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0209.npz, 0.5455, 0.5000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0169.npz, 0.5455, 0.4455, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0114.npz, 0.5818, 0.5636, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0159.npz, 0.6000, 0.5182, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0224.npz, 0.5455, 0.5000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0039.npz, 0.5364, 0.5091, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0154.npz, 0.6091, 0.5364, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0249.npz, 0.5545, 0.4909, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0034.npz, 0.5545, 0.5182, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0084.npz, 0.5545, 0.5273, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0109.npz, 0.6000, 0.5273, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0234.npz, 0.5545, 0.4909, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0104.npz, 0.5909, 0.5545, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0149.npz, 0.5727, 0.5091, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0219.npz, 0.5545, 0.5091, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0199.npz, 0.5364, 0.5000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0244.npz, 0.5455, 0.5273, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0049.npz, 0.5636, 0.5091, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0074.npz, 0.5818, 0.5000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0239.npz, 0.5727, 0.4818, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0174.npz, 0.5455, 0.5455, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0094.npz, 0.5364, 0.5000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0089.npz, 0.5545, 0.5182, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0129.npz, 0.5909, 0.5364, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0019.npz, 0.5455, 0.5000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0134.npz, 0.5818, 0.5000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0069.npz, 0.5364, 0.5364, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0119.npz, 0.5909, 0.5182, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0139.npz, 0.5636, 0.5273, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0014.npz, 0.5636, 0.4909, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0064.npz, 0.5455, 0.5727, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0099.npz, 0.5818, 0.5909, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0184.npz, 0.4909, 0.4909, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0204.npz, 0.5818, 0.4909, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0164.npz, 0.6273, 0.5273, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0144.npz, 0.5818, 0.5182, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0059.npz, 0.5364, 0.6000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0029.npz, 0.5273, 0.5273, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0229.npz, 0.5545, 0.4909, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0189.npz, 0.5818, 0.5455, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0124.npz, 0.6000, 0.5000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0179.npz, 0.5091, 0.4909, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0004.npz, 0.4000, 0.4182, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0054.npz, 0.5455, 0.4364, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0194.npz, 0.5455, 0.5273, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0214.npz, 0.5455, 0.5000, 0.02\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0024.npz, 0.5000, 0.4727, 0.02\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PLAID/alst_sc_c_0000_0009.npz, 0.5091, 0.4545, 0.02\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=0.7085, time=0.03.\n",
            "epoch 2/250, loss=0.5977, time=0.03.\n",
            "epoch 3/250, loss=0.5262, time=0.03.\n",
            "epoch 4/250, loss=0.4721, time=0.03.\n",
            "epoch 5/250, loss=0.4622, time=0.03.\n",
            "epoch 6/250, loss=0.3990, time=0.03.\n",
            "epoch 7/250, loss=0.3200, time=0.03.\n",
            "epoch 8/250, loss=0.2758, time=0.03.\n",
            "epoch 9/250, loss=0.1890, time=0.02.\n",
            "epoch 10/250, loss=0.1181, time=0.02.\n",
            "epoch 11/250, loss=0.0701, time=0.02.\n",
            "epoch 12/250, loss=0.0513, time=0.02.\n",
            "epoch 13/250, loss=0.0348, time=0.02.\n",
            "epoch 14/250, loss=0.0194, time=0.02.\n",
            "epoch 15/250, loss=0.0116, time=0.02.\n",
            "epoch 16/250, loss=0.0068, time=0.02.\n",
            "epoch 17/250, loss=0.0048, time=0.02.\n",
            "epoch 18/250, loss=0.0036, time=0.02.\n",
            "epoch 19/250, loss=0.0022, time=0.02.\n",
            "epoch 20/250, loss=0.0015, time=0.02.\n",
            "epoch 21/250, loss=0.0012, time=0.02.\n",
            "epoch 22/250, loss=0.0009, time=0.02.\n",
            "epoch 23/250, loss=0.0006, time=0.02.\n",
            "epoch 24/250, loss=0.0005, time=0.02.\n",
            "epoch 25/250, loss=0.0004, time=0.02.\n",
            "epoch 26/250, loss=0.0003, time=0.02.\n",
            "epoch 27/250, loss=0.0003, time=0.02.\n",
            "epoch 28/250, loss=0.0003, time=0.02.\n",
            "epoch 29/250, loss=0.0002, time=0.02.\n",
            "epoch 30/250, loss=0.0002, time=0.02.\n",
            "epoch 31/250, loss=0.0002, time=0.02.\n",
            "epoch 32/250, loss=0.0002, time=0.02.\n",
            "epoch 33/250, loss=0.0002, time=0.02.\n",
            "epoch 34/250, loss=0.0002, time=0.02.\n",
            "epoch 35/250, loss=0.0001, time=0.02.\n",
            "epoch 36/250, loss=0.0002, time=0.02.\n",
            "epoch 37/250, loss=0.0001, time=0.02.\n",
            "epoch 38/250, loss=0.0001, time=0.02.\n",
            "epoch 39/250, loss=0.0001, time=0.02.\n",
            "epoch 40/250, loss=0.0001, time=0.02.\n",
            "epoch 41/250, loss=0.0001, time=0.02.\n",
            "epoch 42/250, loss=0.0001, time=0.02.\n",
            "epoch 43/250, loss=0.0001, time=0.02.\n",
            "epoch 44/250, loss=0.0001, time=0.02.\n",
            "epoch 45/250, loss=0.0001, time=0.02.\n",
            "epoch 46/250, loss=0.0001, time=0.02.\n",
            "epoch 47/250, loss=0.0001, time=0.02.\n",
            "epoch 48/250, loss=0.0001, time=0.02.\n",
            "epoch 49/250, loss=0.0001, time=0.02.\n",
            "epoch 50/250, loss=0.0001, time=0.02.\n",
            "epoch 51/250, loss=0.0001, time=0.02.\n",
            "epoch 52/250, loss=0.0001, time=0.02.\n",
            "epoch 53/250, loss=0.0001, time=0.02.\n",
            "epoch 54/250, loss=0.0001, time=0.02.\n",
            "epoch 55/250, loss=0.0001, time=0.02.\n",
            "epoch 56/250, loss=0.0001, time=0.02.\n",
            "epoch 57/250, loss=0.0001, time=0.02.\n",
            "epoch 58/250, loss=0.0001, time=0.02.\n",
            "epoch 59/250, loss=0.0001, time=0.02.\n",
            "epoch 60/250, loss=0.0001, time=0.02.\n",
            "epoch 61/250, loss=0.0001, time=0.02.\n",
            "epoch 62/250, loss=0.0001, time=0.02.\n",
            "epoch 63/250, loss=0.0001, time=0.02.\n",
            "epoch 64/250, loss=0.0001, time=0.02.\n",
            "epoch 65/250, loss=0.0001, time=0.02.\n",
            "epoch 66/250, loss=0.0001, time=0.02.\n",
            "epoch 67/250, loss=0.0001, time=0.02.\n",
            "epoch 68/250, loss=0.0001, time=0.02.\n",
            "epoch 69/250, loss=0.0001, time=0.02.\n",
            "epoch 70/250, loss=0.0001, time=0.02.\n",
            "epoch 71/250, loss=0.0001, time=0.02.\n",
            "epoch 72/250, loss=0.0001, time=0.02.\n",
            "epoch 73/250, loss=0.0001, time=0.02.\n",
            "epoch 74/250, loss=0.0001, time=0.02.\n",
            "epoch 75/250, loss=0.0001, time=0.02.\n",
            "epoch 76/250, loss=0.0001, time=0.02.\n",
            "epoch 77/250, loss=0.0001, time=0.02.\n",
            "epoch 78/250, loss=0.0001, time=0.02.\n",
            "epoch 79/250, loss=0.0001, time=0.02.\n",
            "epoch 80/250, loss=0.0001, time=0.02.\n",
            "epoch 81/250, loss=0.0001, time=0.02.\n",
            "epoch 82/250, loss=0.0001, time=0.02.\n",
            "epoch 83/250, loss=0.0001, time=0.02.\n",
            "epoch 84/250, loss=0.0001, time=0.02.\n",
            "epoch 85/250, loss=0.0001, time=0.02.\n",
            "epoch 86/250, loss=0.0001, time=0.02.\n",
            "epoch 87/250, loss=0.0001, time=0.02.\n",
            "epoch 88/250, loss=0.0001, time=0.02.\n",
            "epoch 89/250, loss=0.0001, time=0.02.\n",
            "epoch 90/250, loss=0.0001, time=0.02.\n",
            "epoch 91/250, loss=0.0001, time=0.02.\n",
            "epoch 92/250, loss=0.0001, time=0.02.\n",
            "epoch 93/250, loss=0.0001, time=0.02.\n",
            "epoch 94/250, loss=0.0001, time=0.02.\n",
            "epoch 95/250, loss=0.0001, time=0.02.\n",
            "epoch 96/250, loss=0.0001, time=0.02.\n",
            "epoch 97/250, loss=0.0001, time=0.02.\n",
            "epoch 98/250, loss=0.0001, time=0.02.\n",
            "epoch 99/250, loss=0.0001, time=0.02.\n",
            "epoch 100/250, loss=0.0001, time=0.02.\n",
            "epoch 101/250, loss=0.0001, time=0.02.\n",
            "epoch 102/250, loss=0.0001, time=0.02.\n",
            "epoch 103/250, loss=0.0000, time=0.02.\n",
            "epoch 104/250, loss=0.0000, time=0.02.\n",
            "epoch 105/250, loss=0.0000, time=0.02.\n",
            "epoch 106/250, loss=0.0000, time=0.02.\n",
            "epoch 107/250, loss=0.0000, time=0.02.\n",
            "epoch 108/250, loss=0.0001, time=0.02.\n",
            "epoch 109/250, loss=0.0000, time=0.02.\n",
            "epoch 110/250, loss=0.0000, time=0.02.\n",
            "epoch 111/250, loss=0.0000, time=0.02.\n",
            "epoch 112/250, loss=0.0000, time=0.02.\n",
            "epoch 113/250, loss=0.0000, time=0.02.\n",
            "epoch 114/250, loss=0.0000, time=0.02.\n",
            "epoch 115/250, loss=0.0000, time=0.02.\n",
            "epoch 116/250, loss=0.0000, time=0.02.\n",
            "epoch 117/250, loss=0.0000, time=0.02.\n",
            "epoch 118/250, loss=0.0000, time=0.02.\n",
            "epoch 119/250, loss=0.0000, time=0.02.\n",
            "epoch 120/250, loss=0.0000, time=0.02.\n",
            "epoch 121/250, loss=0.0000, time=0.02.\n",
            "epoch 122/250, loss=0.0000, time=0.02.\n",
            "epoch 123/250, loss=0.0000, time=0.02.\n",
            "epoch 124/250, loss=0.0000, time=0.02.\n",
            "epoch 125/250, loss=0.0000, time=0.02.\n",
            "epoch 126/250, loss=0.0000, time=0.02.\n",
            "epoch 127/250, loss=0.0000, time=0.02.\n",
            "epoch 128/250, loss=0.0000, time=0.02.\n",
            "epoch 129/250, loss=0.0000, time=0.02.\n",
            "epoch 130/250, loss=0.0000, time=0.02.\n",
            "epoch 131/250, loss=0.0000, time=0.02.\n",
            "epoch 132/250, loss=0.0000, time=0.02.\n",
            "epoch 133/250, loss=0.0000, time=0.02.\n",
            "epoch 134/250, loss=0.0000, time=0.02.\n",
            "epoch 135/250, loss=0.0000, time=0.02.\n",
            "epoch 136/250, loss=0.0000, time=0.02.\n",
            "epoch 137/250, loss=0.0000, time=0.02.\n",
            "epoch 138/250, loss=0.0000, time=0.02.\n",
            "epoch 139/250, loss=0.0000, time=0.02.\n",
            "epoch 140/250, loss=0.0000, time=0.02.\n",
            "epoch 141/250, loss=0.0000, time=0.02.\n",
            "epoch 142/250, loss=0.0000, time=0.02.\n",
            "epoch 143/250, loss=0.0000, time=0.02.\n",
            "epoch 144/250, loss=0.0000, time=0.02.\n",
            "epoch 145/250, loss=0.0000, time=0.02.\n",
            "epoch 146/250, loss=0.0000, time=0.02.\n",
            "epoch 147/250, loss=0.0000, time=0.02.\n",
            "epoch 148/250, loss=0.0000, time=0.02.\n",
            "epoch 149/250, loss=0.0000, time=0.02.\n",
            "epoch 150/250, loss=0.0000, time=0.02.\n",
            "epoch 151/250, loss=0.0000, time=0.02.\n",
            "epoch 152/250, loss=0.0000, time=0.02.\n",
            "epoch 153/250, loss=0.0000, time=0.02.\n",
            "epoch 154/250, loss=0.0000, time=0.02.\n",
            "epoch 155/250, loss=0.0000, time=0.02.\n",
            "epoch 156/250, loss=0.0000, time=0.02.\n",
            "epoch 157/250, loss=0.0000, time=0.02.\n",
            "epoch 158/250, loss=0.0000, time=0.02.\n",
            "epoch 159/250, loss=0.0000, time=0.02.\n",
            "epoch 160/250, loss=0.0000, time=0.02.\n",
            "epoch 161/250, loss=0.0000, time=0.02.\n",
            "epoch 162/250, loss=0.0000, time=0.02.\n",
            "epoch 163/250, loss=0.0000, time=0.02.\n",
            "epoch 164/250, loss=0.0000, time=0.02.\n",
            "epoch 165/250, loss=0.0000, time=0.02.\n",
            "epoch 166/250, loss=0.0000, time=0.02.\n",
            "epoch 167/250, loss=0.0000, time=0.02.\n",
            "epoch 168/250, loss=0.0000, time=0.02.\n",
            "epoch 169/250, loss=0.0000, time=0.02.\n",
            "epoch 170/250, loss=0.0000, time=0.02.\n",
            "epoch 171/250, loss=0.0000, time=0.02.\n",
            "epoch 172/250, loss=0.0000, time=0.02.\n",
            "epoch 173/250, loss=0.0000, time=0.02.\n",
            "epoch 174/250, loss=0.0000, time=0.02.\n",
            "epoch 175/250, loss=0.0000, time=0.02.\n",
            "epoch 176/250, loss=0.0000, time=0.02.\n",
            "epoch 177/250, loss=0.0000, time=0.02.\n",
            "epoch 178/250, loss=0.0000, time=0.02.\n",
            "epoch 179/250, loss=0.0000, time=0.02.\n",
            "epoch 180/250, loss=0.0000, time=0.02.\n",
            "epoch 181/250, loss=0.0000, time=0.02.\n",
            "epoch 182/250, loss=0.0000, time=0.02.\n",
            "epoch 183/250, loss=0.0000, time=0.02.\n",
            "epoch 184/250, loss=0.0000, time=0.02.\n",
            "epoch 185/250, loss=0.0000, time=0.02.\n",
            "epoch 186/250, loss=0.0000, time=0.02.\n",
            "epoch 187/250, loss=0.0000, time=0.02.\n",
            "epoch 188/250, loss=0.0000, time=0.02.\n",
            "epoch 189/250, loss=0.0000, time=0.02.\n",
            "epoch 190/250, loss=0.0000, time=0.02.\n",
            "epoch 191/250, loss=0.0000, time=0.02.\n",
            "epoch 192/250, loss=0.0000, time=0.02.\n",
            "epoch 193/250, loss=0.0000, time=0.02.\n",
            "epoch 194/250, loss=0.0000, time=0.02.\n",
            "epoch 195/250, loss=0.0000, time=0.02.\n",
            "epoch 196/250, loss=0.0000, time=0.02.\n",
            "epoch 197/250, loss=0.0000, time=0.02.\n",
            "epoch 198/250, loss=0.0000, time=0.02.\n",
            "epoch 199/250, loss=0.0000, time=0.02.\n",
            "epoch 200/250, loss=0.0000, time=0.02.\n",
            "epoch 201/250, loss=0.0000, time=0.02.\n",
            "epoch 202/250, loss=0.0000, time=0.02.\n",
            "epoch 203/250, loss=0.0000, time=0.02.\n",
            "epoch 204/250, loss=0.0000, time=0.02.\n",
            "epoch 205/250, loss=0.0000, time=0.02.\n",
            "epoch 206/250, loss=0.0000, time=0.02.\n",
            "epoch 207/250, loss=0.0000, time=0.02.\n",
            "epoch 208/250, loss=0.0000, time=0.02.\n",
            "epoch 209/250, loss=0.0000, time=0.02.\n",
            "epoch 210/250, loss=0.0000, time=0.02.\n",
            "epoch 211/250, loss=0.0000, time=0.02.\n",
            "epoch 212/250, loss=0.0000, time=0.02.\n",
            "epoch 213/250, loss=0.0000, time=0.02.\n",
            "epoch 214/250, loss=0.0000, time=0.02.\n",
            "epoch 215/250, loss=0.0000, time=0.02.\n",
            "epoch 216/250, loss=0.0000, time=0.02.\n",
            "epoch 217/250, loss=0.0000, time=0.02.\n",
            "epoch 218/250, loss=0.0000, time=0.02.\n",
            "epoch 219/250, loss=0.0000, time=0.02.\n",
            "epoch 220/250, loss=0.0000, time=0.02.\n",
            "epoch 221/250, loss=0.0000, time=0.02.\n",
            "epoch 222/250, loss=0.0000, time=0.02.\n",
            "epoch 223/250, loss=0.0000, time=0.02.\n",
            "epoch 224/250, loss=0.0000, time=0.02.\n",
            "epoch 225/250, loss=0.0000, time=0.02.\n",
            "epoch 226/250, loss=0.0000, time=0.02.\n",
            "epoch 227/250, loss=0.0000, time=0.02.\n",
            "epoch 228/250, loss=0.0000, time=0.02.\n",
            "epoch 229/250, loss=0.0000, time=0.02.\n",
            "epoch 230/250, loss=0.0000, time=0.02.\n",
            "epoch 231/250, loss=0.0000, time=0.02.\n",
            "epoch 232/250, loss=0.0000, time=0.02.\n",
            "epoch 233/250, loss=0.0000, time=0.02.\n",
            "epoch 234/250, loss=0.0000, time=0.02.\n",
            "epoch 235/250, loss=0.0000, time=0.02.\n",
            "epoch 236/250, loss=0.0000, time=0.02.\n",
            "epoch 237/250, loss=0.0000, time=0.02.\n",
            "epoch 238/250, loss=0.0000, time=0.02.\n",
            "epoch 239/250, loss=0.0000, time=0.02.\n",
            "epoch 240/250, loss=0.0000, time=0.02.\n",
            "epoch 241/250, loss=0.0000, time=0.02.\n",
            "epoch 242/250, loss=0.0000, time=0.02.\n",
            "epoch 243/250, loss=0.0000, time=0.02.\n",
            "epoch 244/250, loss=0.0000, time=0.02.\n",
            "epoch 245/250, loss=0.0000, time=0.02.\n",
            "epoch 246/250, loss=0.0000, time=0.02.\n",
            "epoch 247/250, loss=0.0000, time=0.02.\n",
            "epoch 248/250, loss=0.0000, time=0.02.\n",
            "epoch 249/250, loss=0.0000, time=0.02.\n",
            "epoch 250/250, loss=0.0000, time=0.02.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0114.npz, 0.6111, 0.7778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0014.npz, 0.6667, 0.7500, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0099.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0129.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0049.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0069.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0179.npz, 0.6111, 0.7778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0189.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0094.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0044.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0034.npz, 0.6111, 0.7500, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0249.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0164.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0119.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0089.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0199.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0019.npz, 0.6111, 0.7500, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0064.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0054.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0169.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0244.npz, 0.6111, 0.7778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0059.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0174.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0214.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0149.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0109.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0139.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0009.npz, 0.6111, 0.7500, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0219.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0159.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0154.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0084.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0209.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0204.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0004.npz, 0.6944, 0.6389, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0024.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0239.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0234.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0104.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0039.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0184.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0229.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0079.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0224.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0144.npz, 0.6111, 0.7778, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0029.npz, 0.6111, 0.7500, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0134.npz, 0.6111, 0.7778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0194.npz, 0.6111, 0.7778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0074.npz, 0.6111, 0.7778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_PowerCons/alst_sc_c_0000_0124.npz, 0.6111, 0.7778, 0.01\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=1.5175, time=0.01.\n",
            "epoch 2/250, loss=1.3829, time=0.01.\n",
            "epoch 3/250, loss=1.2488, time=0.01.\n",
            "epoch 4/250, loss=1.1510, time=0.01.\n",
            "epoch 5/250, loss=0.9709, time=0.01.\n",
            "epoch 6/250, loss=0.8039, time=0.01.\n",
            "epoch 7/250, loss=0.6638, time=0.01.\n",
            "epoch 8/250, loss=0.5572, time=0.01.\n",
            "epoch 9/250, loss=0.4870, time=0.01.\n",
            "epoch 10/250, loss=0.3979, time=0.01.\n",
            "epoch 11/250, loss=0.3162, time=0.01.\n",
            "epoch 12/250, loss=0.2528, time=0.01.\n",
            "epoch 13/250, loss=0.1980, time=0.01.\n",
            "epoch 14/250, loss=0.1546, time=0.01.\n",
            "epoch 15/250, loss=0.1222, time=0.01.\n",
            "epoch 16/250, loss=0.0946, time=0.01.\n",
            "epoch 17/250, loss=0.0722, time=0.01.\n",
            "epoch 18/250, loss=0.0557, time=0.01.\n",
            "epoch 19/250, loss=0.0438, time=0.01.\n",
            "epoch 20/250, loss=0.0347, time=0.01.\n",
            "epoch 21/250, loss=0.0280, time=0.01.\n",
            "epoch 22/250, loss=0.0229, time=0.01.\n",
            "epoch 23/250, loss=0.0188, time=0.01.\n",
            "epoch 24/250, loss=0.0155, time=0.01.\n",
            "epoch 25/250, loss=0.0130, time=0.01.\n",
            "epoch 26/250, loss=0.0109, time=0.01.\n",
            "epoch 27/250, loss=0.0093, time=0.01.\n",
            "epoch 28/250, loss=0.0079, time=0.01.\n",
            "epoch 29/250, loss=0.0068, time=0.01.\n",
            "epoch 30/250, loss=0.0060, time=0.01.\n",
            "epoch 31/250, loss=0.0052, time=0.01.\n",
            "epoch 32/250, loss=0.0046, time=0.01.\n",
            "epoch 33/250, loss=0.0041, time=0.01.\n",
            "epoch 34/250, loss=0.0037, time=0.01.\n",
            "epoch 35/250, loss=0.0033, time=0.01.\n",
            "epoch 36/250, loss=0.0030, time=0.01.\n",
            "epoch 37/250, loss=0.0027, time=0.01.\n",
            "epoch 38/250, loss=0.0025, time=0.01.\n",
            "epoch 39/250, loss=0.0023, time=0.01.\n",
            "epoch 40/250, loss=0.0021, time=0.01.\n",
            "epoch 41/250, loss=0.0020, time=0.01.\n",
            "epoch 42/250, loss=0.0018, time=0.01.\n",
            "epoch 43/250, loss=0.0017, time=0.01.\n",
            "epoch 44/250, loss=0.0016, time=0.01.\n",
            "epoch 45/250, loss=0.0015, time=0.01.\n",
            "epoch 46/250, loss=0.0014, time=0.01.\n",
            "epoch 47/250, loss=0.0014, time=0.01.\n",
            "epoch 48/250, loss=0.0013, time=0.01.\n",
            "epoch 49/250, loss=0.0012, time=0.01.\n",
            "epoch 50/250, loss=0.0012, time=0.01.\n",
            "epoch 51/250, loss=0.0011, time=0.01.\n",
            "epoch 52/250, loss=0.0011, time=0.01.\n",
            "epoch 53/250, loss=0.0011, time=0.01.\n",
            "epoch 54/250, loss=0.0010, time=0.01.\n",
            "epoch 55/250, loss=0.0010, time=0.01.\n",
            "epoch 56/250, loss=0.0010, time=0.01.\n",
            "epoch 57/250, loss=0.0009, time=0.01.\n",
            "epoch 58/250, loss=0.0009, time=0.01.\n",
            "epoch 59/250, loss=0.0009, time=0.01.\n",
            "epoch 60/250, loss=0.0009, time=0.01.\n",
            "epoch 61/250, loss=0.0008, time=0.01.\n",
            "epoch 62/250, loss=0.0008, time=0.01.\n",
            "epoch 63/250, loss=0.0008, time=0.01.\n",
            "epoch 64/250, loss=0.0008, time=0.01.\n",
            "epoch 65/250, loss=0.0008, time=0.01.\n",
            "epoch 66/250, loss=0.0008, time=0.01.\n",
            "epoch 67/250, loss=0.0007, time=0.01.\n",
            "epoch 68/250, loss=0.0007, time=0.01.\n",
            "epoch 69/250, loss=0.0007, time=0.01.\n",
            "epoch 70/250, loss=0.0007, time=0.01.\n",
            "epoch 71/250, loss=0.0007, time=0.01.\n",
            "epoch 72/250, loss=0.0007, time=0.01.\n",
            "epoch 73/250, loss=0.0007, time=0.01.\n",
            "epoch 74/250, loss=0.0007, time=0.01.\n",
            "epoch 75/250, loss=0.0006, time=0.01.\n",
            "epoch 76/250, loss=0.0006, time=0.01.\n",
            "epoch 77/250, loss=0.0006, time=0.01.\n",
            "epoch 78/250, loss=0.0006, time=0.01.\n",
            "epoch 79/250, loss=0.0006, time=0.01.\n",
            "epoch 80/250, loss=0.0006, time=0.01.\n",
            "epoch 81/250, loss=0.0006, time=0.01.\n",
            "epoch 82/250, loss=0.0006, time=0.01.\n",
            "epoch 83/250, loss=0.0006, time=0.01.\n",
            "epoch 84/250, loss=0.0006, time=0.01.\n",
            "epoch 85/250, loss=0.0006, time=0.01.\n",
            "epoch 86/250, loss=0.0006, time=0.01.\n",
            "epoch 87/250, loss=0.0006, time=0.01.\n",
            "epoch 88/250, loss=0.0006, time=0.01.\n",
            "epoch 89/250, loss=0.0005, time=0.01.\n",
            "epoch 90/250, loss=0.0005, time=0.01.\n",
            "epoch 91/250, loss=0.0005, time=0.01.\n",
            "epoch 92/250, loss=0.0005, time=0.01.\n",
            "epoch 93/250, loss=0.0005, time=0.01.\n",
            "epoch 94/250, loss=0.0005, time=0.01.\n",
            "epoch 95/250, loss=0.0005, time=0.01.\n",
            "epoch 96/250, loss=0.0005, time=0.01.\n",
            "epoch 97/250, loss=0.0005, time=0.01.\n",
            "epoch 98/250, loss=0.0005, time=0.01.\n",
            "epoch 99/250, loss=0.0005, time=0.01.\n",
            "epoch 100/250, loss=0.0005, time=0.01.\n",
            "epoch 101/250, loss=0.0005, time=0.01.\n",
            "epoch 102/250, loss=0.0005, time=0.01.\n",
            "epoch 103/250, loss=0.0005, time=0.01.\n",
            "epoch 104/250, loss=0.0005, time=0.01.\n",
            "epoch 105/250, loss=0.0005, time=0.01.\n",
            "epoch 106/250, loss=0.0005, time=0.01.\n",
            "epoch 107/250, loss=0.0005, time=0.01.\n",
            "epoch 108/250, loss=0.0005, time=0.01.\n",
            "epoch 109/250, loss=0.0004, time=0.01.\n",
            "epoch 110/250, loss=0.0004, time=0.01.\n",
            "epoch 111/250, loss=0.0004, time=0.01.\n",
            "epoch 112/250, loss=0.0004, time=0.01.\n",
            "epoch 113/250, loss=0.0004, time=0.01.\n",
            "epoch 114/250, loss=0.0004, time=0.01.\n",
            "epoch 115/250, loss=0.0004, time=0.01.\n",
            "epoch 116/250, loss=0.0004, time=0.01.\n",
            "epoch 117/250, loss=0.0004, time=0.01.\n",
            "epoch 118/250, loss=0.0004, time=0.01.\n",
            "epoch 119/250, loss=0.0004, time=0.01.\n",
            "epoch 120/250, loss=0.0004, time=0.01.\n",
            "epoch 121/250, loss=0.0004, time=0.01.\n",
            "epoch 122/250, loss=0.0004, time=0.01.\n",
            "epoch 123/250, loss=0.0004, time=0.01.\n",
            "epoch 124/250, loss=0.0004, time=0.01.\n",
            "epoch 125/250, loss=0.0004, time=0.01.\n",
            "epoch 126/250, loss=0.0004, time=0.01.\n",
            "epoch 127/250, loss=0.0004, time=0.01.\n",
            "epoch 128/250, loss=0.0004, time=0.01.\n",
            "epoch 129/250, loss=0.0004, time=0.01.\n",
            "epoch 130/250, loss=0.0004, time=0.01.\n",
            "epoch 131/250, loss=0.0004, time=0.01.\n",
            "epoch 132/250, loss=0.0004, time=0.01.\n",
            "epoch 133/250, loss=0.0004, time=0.01.\n",
            "epoch 134/250, loss=0.0004, time=0.01.\n",
            "epoch 135/250, loss=0.0004, time=0.01.\n",
            "epoch 136/250, loss=0.0004, time=0.01.\n",
            "epoch 137/250, loss=0.0004, time=0.01.\n",
            "epoch 138/250, loss=0.0004, time=0.01.\n",
            "epoch 139/250, loss=0.0003, time=0.01.\n",
            "epoch 140/250, loss=0.0003, time=0.01.\n",
            "epoch 141/250, loss=0.0003, time=0.01.\n",
            "epoch 142/250, loss=0.0003, time=0.01.\n",
            "epoch 143/250, loss=0.0003, time=0.01.\n",
            "epoch 144/250, loss=0.0003, time=0.01.\n",
            "epoch 145/250, loss=0.0003, time=0.01.\n",
            "epoch 146/250, loss=0.0003, time=0.01.\n",
            "epoch 147/250, loss=0.0003, time=0.01.\n",
            "epoch 148/250, loss=0.0003, time=0.01.\n",
            "epoch 149/250, loss=0.0003, time=0.01.\n",
            "epoch 150/250, loss=0.0003, time=0.01.\n",
            "epoch 151/250, loss=0.0003, time=0.01.\n",
            "epoch 152/250, loss=0.0003, time=0.01.\n",
            "epoch 153/250, loss=0.0003, time=0.01.\n",
            "epoch 154/250, loss=0.0003, time=0.01.\n",
            "epoch 155/250, loss=0.0003, time=0.01.\n",
            "epoch 156/250, loss=0.0003, time=0.01.\n",
            "epoch 157/250, loss=0.0003, time=0.01.\n",
            "epoch 158/250, loss=0.0003, time=0.01.\n",
            "epoch 159/250, loss=0.0003, time=0.01.\n",
            "epoch 160/250, loss=0.0003, time=0.01.\n",
            "epoch 161/250, loss=0.0003, time=0.01.\n",
            "epoch 162/250, loss=0.0003, time=0.01.\n",
            "epoch 163/250, loss=0.0003, time=0.01.\n",
            "epoch 164/250, loss=0.0003, time=0.01.\n",
            "epoch 165/250, loss=0.0003, time=0.01.\n",
            "epoch 166/250, loss=0.0003, time=0.01.\n",
            "epoch 167/250, loss=0.0003, time=0.01.\n",
            "epoch 168/250, loss=0.0003, time=0.01.\n",
            "epoch 169/250, loss=0.0003, time=0.01.\n",
            "epoch 170/250, loss=0.0003, time=0.01.\n",
            "epoch 171/250, loss=0.0003, time=0.01.\n",
            "epoch 172/250, loss=0.0003, time=0.01.\n",
            "epoch 173/250, loss=0.0003, time=0.01.\n",
            "epoch 174/250, loss=0.0003, time=0.01.\n",
            "epoch 175/250, loss=0.0003, time=0.01.\n",
            "epoch 176/250, loss=0.0003, time=0.01.\n",
            "epoch 177/250, loss=0.0003, time=0.01.\n",
            "epoch 178/250, loss=0.0003, time=0.01.\n",
            "epoch 179/250, loss=0.0003, time=0.01.\n",
            "epoch 180/250, loss=0.0003, time=0.01.\n",
            "epoch 181/250, loss=0.0003, time=0.01.\n",
            "epoch 182/250, loss=0.0003, time=0.01.\n",
            "epoch 183/250, loss=0.0003, time=0.01.\n",
            "epoch 184/250, loss=0.0003, time=0.01.\n",
            "epoch 185/250, loss=0.0003, time=0.01.\n",
            "epoch 186/250, loss=0.0002, time=0.01.\n",
            "epoch 187/250, loss=0.0002, time=0.01.\n",
            "epoch 188/250, loss=0.0002, time=0.01.\n",
            "epoch 189/250, loss=0.0002, time=0.01.\n",
            "epoch 190/250, loss=0.0002, time=0.01.\n",
            "epoch 191/250, loss=0.0002, time=0.01.\n",
            "epoch 192/250, loss=0.0002, time=0.01.\n",
            "epoch 193/250, loss=0.0002, time=0.01.\n",
            "epoch 194/250, loss=0.0002, time=0.01.\n",
            "epoch 195/250, loss=0.0002, time=0.01.\n",
            "epoch 196/250, loss=0.0002, time=0.01.\n",
            "epoch 197/250, loss=0.0002, time=0.01.\n",
            "epoch 198/250, loss=0.0002, time=0.01.\n",
            "epoch 199/250, loss=0.0002, time=0.01.\n",
            "epoch 200/250, loss=0.0002, time=0.01.\n",
            "epoch 201/250, loss=0.0002, time=0.01.\n",
            "epoch 202/250, loss=0.0002, time=0.01.\n",
            "epoch 203/250, loss=0.0002, time=0.01.\n",
            "epoch 204/250, loss=0.0002, time=0.01.\n",
            "epoch 205/250, loss=0.0002, time=0.01.\n",
            "epoch 206/250, loss=0.0002, time=0.01.\n",
            "epoch 207/250, loss=0.0002, time=0.01.\n",
            "epoch 208/250, loss=0.0002, time=0.01.\n",
            "epoch 209/250, loss=0.0002, time=0.01.\n",
            "epoch 210/250, loss=0.0002, time=0.01.\n",
            "epoch 211/250, loss=0.0002, time=0.01.\n",
            "epoch 212/250, loss=0.0002, time=0.01.\n",
            "epoch 213/250, loss=0.0002, time=0.01.\n",
            "epoch 214/250, loss=0.0002, time=0.01.\n",
            "epoch 215/250, loss=0.0002, time=0.01.\n",
            "epoch 216/250, loss=0.0002, time=0.01.\n",
            "epoch 217/250, loss=0.0002, time=0.01.\n",
            "epoch 218/250, loss=0.0002, time=0.01.\n",
            "epoch 219/250, loss=0.0002, time=0.01.\n",
            "epoch 220/250, loss=0.0002, time=0.01.\n",
            "epoch 221/250, loss=0.0002, time=0.01.\n",
            "epoch 222/250, loss=0.0002, time=0.01.\n",
            "epoch 223/250, loss=0.0002, time=0.01.\n",
            "epoch 224/250, loss=0.0002, time=0.01.\n",
            "epoch 225/250, loss=0.0002, time=0.01.\n",
            "epoch 226/250, loss=0.0002, time=0.01.\n",
            "epoch 227/250, loss=0.0002, time=0.01.\n",
            "epoch 228/250, loss=0.0002, time=0.01.\n",
            "epoch 229/250, loss=0.0002, time=0.01.\n",
            "epoch 230/250, loss=0.0002, time=0.01.\n",
            "epoch 231/250, loss=0.0002, time=0.01.\n",
            "epoch 232/250, loss=0.0002, time=0.01.\n",
            "epoch 233/250, loss=0.0002, time=0.01.\n",
            "epoch 234/250, loss=0.0002, time=0.01.\n",
            "epoch 235/250, loss=0.0002, time=0.01.\n",
            "epoch 236/250, loss=0.0002, time=0.01.\n",
            "epoch 237/250, loss=0.0002, time=0.01.\n",
            "epoch 238/250, loss=0.0002, time=0.01.\n",
            "epoch 239/250, loss=0.0002, time=0.01.\n",
            "epoch 240/250, loss=0.0002, time=0.01.\n",
            "epoch 241/250, loss=0.0002, time=0.01.\n",
            "epoch 242/250, loss=0.0002, time=0.01.\n",
            "epoch 243/250, loss=0.0002, time=0.01.\n",
            "epoch 244/250, loss=0.0002, time=0.01.\n",
            "epoch 245/250, loss=0.0002, time=0.01.\n",
            "epoch 246/250, loss=0.0002, time=0.01.\n",
            "epoch 247/250, loss=0.0002, time=0.01.\n",
            "epoch 248/250, loss=0.0002, time=0.01.\n",
            "epoch 249/250, loss=0.0002, time=0.01.\n",
            "epoch 250/250, loss=0.0002, time=0.01.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0159.npz, 0.5714, 0.4286, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0039.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0219.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0114.npz, 0.5714, 0.2857, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0069.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0004.npz, 0.2857, 0.2857, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0224.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0099.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0029.npz, 0.7143, 0.4286, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0044.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0189.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0104.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0184.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0229.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0079.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0054.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0154.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0234.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0134.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0139.npz, 0.5714, 0.4286, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0239.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0109.npz, 0.5714, 0.2857, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0249.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0024.npz, 0.5714, 0.2857, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0174.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0204.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0179.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0059.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0014.npz, 0.4286, 0.1429, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0244.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0169.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0064.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0124.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0214.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0019.npz, 0.4286, 0.1429, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0084.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0094.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0049.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0199.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0149.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0144.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0074.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0009.npz, 0.4286, 0.2857, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0129.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0089.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0209.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0119.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0164.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0194.npz, 0.5714, 0.4286, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_Rock/alst_sc_c_0000_0034.npz, 0.7143, 0.4286, 0.00\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=0.6821, time=0.08.\n",
            "epoch 2/250, loss=0.6247, time=0.08.\n",
            "epoch 3/250, loss=0.5388, time=0.05.\n",
            "epoch 4/250, loss=0.4404, time=0.05.\n",
            "epoch 5/250, loss=0.3621, time=0.05.\n",
            "epoch 6/250, loss=0.2621, time=0.05.\n",
            "epoch 7/250, loss=0.2025, time=0.04.\n",
            "epoch 8/250, loss=0.1601, time=0.05.\n",
            "epoch 9/250, loss=0.1234, time=0.05.\n",
            "epoch 10/250, loss=0.0797, time=0.05.\n",
            "epoch 11/250, loss=0.0886, time=0.04.\n",
            "epoch 12/250, loss=0.0719, time=0.05.\n",
            "epoch 13/250, loss=0.0996, time=0.05.\n",
            "epoch 14/250, loss=0.1423, time=0.05.\n",
            "epoch 15/250, loss=0.1249, time=0.05.\n",
            "epoch 16/250, loss=0.0714, time=0.04.\n",
            "epoch 17/250, loss=0.0899, time=0.04.\n",
            "epoch 18/250, loss=0.0823, time=0.05.\n",
            "epoch 19/250, loss=0.0924, time=0.05.\n",
            "epoch 20/250, loss=0.1203, time=0.05.\n",
            "epoch 21/250, loss=0.1521, time=0.04.\n",
            "epoch 22/250, loss=0.3693, time=0.04.\n",
            "epoch 23/250, loss=0.2343, time=0.05.\n",
            "epoch 24/250, loss=0.2646, time=0.05.\n",
            "epoch 25/250, loss=0.2139, time=0.05.\n",
            "epoch 26/250, loss=0.1669, time=0.04.\n",
            "epoch 27/250, loss=0.1023, time=0.04.\n",
            "epoch 28/250, loss=0.0598, time=0.05.\n",
            "epoch 29/250, loss=0.0286, time=0.05.\n",
            "epoch 30/250, loss=0.0192, time=0.05.\n",
            "epoch 31/250, loss=0.0095, time=0.04.\n",
            "epoch 32/250, loss=0.0059, time=0.04.\n",
            "epoch 33/250, loss=0.0037, time=0.05.\n",
            "epoch 34/250, loss=0.0028, time=0.05.\n",
            "epoch 35/250, loss=0.0022, time=0.05.\n",
            "epoch 36/250, loss=0.0018, time=0.04.\n",
            "epoch 37/250, loss=0.0016, time=0.04.\n",
            "epoch 38/250, loss=0.0013, time=0.05.\n",
            "epoch 39/250, loss=0.0012, time=0.05.\n",
            "epoch 40/250, loss=0.0011, time=0.05.\n",
            "epoch 41/250, loss=0.0010, time=0.04.\n",
            "epoch 42/250, loss=0.0009, time=0.04.\n",
            "epoch 43/250, loss=0.0008, time=0.05.\n",
            "epoch 44/250, loss=0.0008, time=0.05.\n",
            "epoch 45/250, loss=0.0007, time=0.05.\n",
            "epoch 46/250, loss=0.0007, time=0.04.\n",
            "epoch 47/250, loss=0.0007, time=0.04.\n",
            "epoch 48/250, loss=0.0007, time=0.05.\n",
            "epoch 49/250, loss=0.0006, time=0.05.\n",
            "epoch 50/250, loss=0.0006, time=0.05.\n",
            "epoch 51/250, loss=0.0006, time=0.04.\n",
            "epoch 52/250, loss=0.0005, time=0.05.\n",
            "epoch 53/250, loss=0.0005, time=0.05.\n",
            "epoch 54/250, loss=0.0005, time=0.05.\n",
            "epoch 55/250, loss=0.0004, time=0.05.\n",
            "epoch 56/250, loss=0.0004, time=0.04.\n",
            "epoch 57/250, loss=0.0004, time=0.04.\n",
            "epoch 58/250, loss=0.0004, time=0.05.\n",
            "epoch 59/250, loss=0.0004, time=0.05.\n",
            "epoch 60/250, loss=0.0004, time=0.05.\n",
            "epoch 61/250, loss=0.0004, time=0.04.\n",
            "epoch 62/250, loss=0.0004, time=0.04.\n",
            "epoch 63/250, loss=0.0003, time=0.05.\n",
            "epoch 64/250, loss=0.0003, time=0.05.\n",
            "epoch 65/250, loss=0.0003, time=0.05.\n",
            "epoch 66/250, loss=0.0003, time=0.04.\n",
            "epoch 67/250, loss=0.0003, time=0.05.\n",
            "epoch 68/250, loss=0.0003, time=0.05.\n",
            "epoch 69/250, loss=0.0003, time=0.05.\n",
            "epoch 70/250, loss=0.0003, time=0.05.\n",
            "epoch 71/250, loss=0.0003, time=0.04.\n",
            "epoch 72/250, loss=0.0002, time=0.05.\n",
            "epoch 73/250, loss=0.0003, time=0.05.\n",
            "epoch 74/250, loss=0.0002, time=0.05.\n",
            "epoch 75/250, loss=0.0002, time=0.05.\n",
            "epoch 76/250, loss=0.0002, time=0.04.\n",
            "epoch 77/250, loss=0.0002, time=0.04.\n",
            "epoch 78/250, loss=0.0002, time=0.05.\n",
            "epoch 79/250, loss=0.0002, time=0.05.\n",
            "epoch 80/250, loss=0.0002, time=0.05.\n",
            "epoch 81/250, loss=0.0002, time=0.04.\n",
            "epoch 82/250, loss=0.0002, time=0.05.\n",
            "epoch 83/250, loss=0.0002, time=0.04.\n",
            "epoch 84/250, loss=0.0002, time=0.05.\n",
            "epoch 85/250, loss=0.0002, time=0.05.\n",
            "epoch 86/250, loss=0.0002, time=0.04.\n",
            "epoch 87/250, loss=0.0002, time=0.04.\n",
            "epoch 88/250, loss=0.0002, time=0.05.\n",
            "epoch 89/250, loss=0.0002, time=0.05.\n",
            "epoch 90/250, loss=0.0002, time=0.05.\n",
            "epoch 91/250, loss=0.0002, time=0.04.\n",
            "epoch 92/250, loss=0.0002, time=0.04.\n",
            "epoch 93/250, loss=0.0002, time=0.05.\n",
            "epoch 94/250, loss=0.0001, time=0.05.\n",
            "epoch 95/250, loss=0.0001, time=0.05.\n",
            "epoch 96/250, loss=0.0001, time=0.04.\n",
            "epoch 97/250, loss=0.0001, time=0.05.\n",
            "epoch 98/250, loss=0.0001, time=0.05.\n",
            "epoch 99/250, loss=0.0001, time=0.05.\n",
            "epoch 100/250, loss=0.0001, time=0.05.\n",
            "epoch 101/250, loss=0.0001, time=0.04.\n",
            "epoch 102/250, loss=0.0001, time=0.05.\n",
            "epoch 103/250, loss=0.0001, time=0.05.\n",
            "epoch 104/250, loss=0.0001, time=0.05.\n",
            "epoch 105/250, loss=0.0001, time=0.05.\n",
            "epoch 106/250, loss=0.0001, time=0.04.\n",
            "epoch 107/250, loss=0.0001, time=0.05.\n",
            "epoch 108/250, loss=0.0001, time=0.05.\n",
            "epoch 109/250, loss=0.0001, time=0.05.\n",
            "epoch 110/250, loss=0.0001, time=0.05.\n",
            "epoch 111/250, loss=0.0001, time=0.04.\n",
            "epoch 112/250, loss=0.0001, time=0.05.\n",
            "epoch 113/250, loss=0.0001, time=0.05.\n",
            "epoch 114/250, loss=0.0001, time=0.05.\n",
            "epoch 115/250, loss=0.0001, time=0.05.\n",
            "epoch 116/250, loss=0.0001, time=0.04.\n",
            "epoch 117/250, loss=0.0001, time=0.05.\n",
            "epoch 118/250, loss=0.0001, time=0.05.\n",
            "epoch 119/250, loss=0.0001, time=0.05.\n",
            "epoch 120/250, loss=0.0001, time=0.05.\n",
            "epoch 121/250, loss=0.0001, time=0.04.\n",
            "epoch 122/250, loss=0.0001, time=0.04.\n",
            "epoch 123/250, loss=0.0001, time=0.05.\n",
            "epoch 124/250, loss=0.0001, time=0.05.\n",
            "epoch 125/250, loss=0.0001, time=0.05.\n",
            "epoch 126/250, loss=0.0001, time=0.04.\n",
            "epoch 127/250, loss=0.0001, time=0.04.\n",
            "epoch 128/250, loss=0.0001, time=0.05.\n",
            "epoch 129/250, loss=0.0001, time=0.05.\n",
            "epoch 130/250, loss=0.0001, time=0.05.\n",
            "epoch 131/250, loss=0.0001, time=0.04.\n",
            "epoch 132/250, loss=0.0001, time=0.05.\n",
            "epoch 133/250, loss=0.0001, time=0.05.\n",
            "epoch 134/250, loss=0.0001, time=0.05.\n",
            "epoch 135/250, loss=0.0001, time=0.05.\n",
            "epoch 136/250, loss=0.0001, time=0.04.\n",
            "epoch 137/250, loss=0.0001, time=0.05.\n",
            "epoch 138/250, loss=0.0001, time=0.05.\n",
            "epoch 139/250, loss=0.0001, time=0.05.\n",
            "epoch 140/250, loss=0.0001, time=0.05.\n",
            "epoch 141/250, loss=0.0001, time=0.04.\n",
            "epoch 142/250, loss=0.0001, time=0.04.\n",
            "epoch 143/250, loss=0.0001, time=0.05.\n",
            "epoch 144/250, loss=0.0001, time=0.05.\n",
            "epoch 145/250, loss=0.0001, time=0.05.\n",
            "epoch 146/250, loss=0.0001, time=0.04.\n",
            "epoch 147/250, loss=0.0001, time=0.04.\n",
            "epoch 148/250, loss=0.0001, time=0.05.\n",
            "epoch 149/250, loss=0.0001, time=0.05.\n",
            "epoch 150/250, loss=0.0001, time=0.05.\n",
            "epoch 151/250, loss=0.0001, time=0.04.\n",
            "epoch 152/250, loss=0.0001, time=0.05.\n",
            "epoch 153/250, loss=0.0001, time=0.05.\n",
            "epoch 154/250, loss=0.0001, time=0.05.\n",
            "epoch 155/250, loss=0.0001, time=0.05.\n",
            "epoch 156/250, loss=0.0001, time=0.04.\n",
            "epoch 157/250, loss=0.0001, time=0.05.\n",
            "epoch 158/250, loss=0.0001, time=0.05.\n",
            "epoch 159/250, loss=0.0001, time=0.05.\n",
            "epoch 160/250, loss=0.0001, time=0.05.\n",
            "epoch 161/250, loss=0.0001, time=0.04.\n",
            "epoch 162/250, loss=0.0001, time=0.04.\n",
            "epoch 163/250, loss=0.0001, time=0.05.\n",
            "epoch 164/250, loss=0.0001, time=0.05.\n",
            "epoch 165/250, loss=0.0001, time=0.05.\n",
            "epoch 166/250, loss=0.0001, time=0.04.\n",
            "epoch 167/250, loss=0.0000, time=0.05.\n",
            "epoch 168/250, loss=0.0001, time=0.05.\n",
            "epoch 169/250, loss=0.0001, time=0.05.\n",
            "epoch 170/250, loss=0.0001, time=0.05.\n",
            "epoch 171/250, loss=0.0001, time=0.04.\n",
            "epoch 172/250, loss=0.0000, time=0.04.\n",
            "epoch 173/250, loss=0.0000, time=0.05.\n",
            "epoch 174/250, loss=0.0000, time=0.05.\n",
            "epoch 175/250, loss=0.0000, time=0.05.\n",
            "epoch 176/250, loss=0.0000, time=0.04.\n",
            "epoch 177/250, loss=0.0000, time=0.05.\n",
            "epoch 178/250, loss=0.0000, time=0.05.\n",
            "epoch 179/250, loss=0.0000, time=0.05.\n",
            "epoch 180/250, loss=0.0000, time=0.05.\n",
            "epoch 181/250, loss=0.0000, time=0.04.\n",
            "epoch 182/250, loss=0.0000, time=0.05.\n",
            "epoch 183/250, loss=0.0000, time=0.05.\n",
            "epoch 184/250, loss=0.0000, time=0.05.\n",
            "epoch 185/250, loss=0.0000, time=0.05.\n",
            "epoch 186/250, loss=0.0000, time=0.04.\n",
            "epoch 187/250, loss=0.0000, time=0.04.\n",
            "epoch 188/250, loss=0.0000, time=0.05.\n",
            "epoch 189/250, loss=0.0000, time=0.05.\n",
            "epoch 190/250, loss=0.0000, time=0.05.\n",
            "epoch 191/250, loss=0.0000, time=0.04.\n",
            "epoch 192/250, loss=0.0000, time=0.05.\n",
            "epoch 193/250, loss=0.0000, time=0.05.\n",
            "epoch 194/250, loss=0.0000, time=0.05.\n",
            "epoch 195/250, loss=0.0000, time=0.05.\n",
            "epoch 196/250, loss=0.0000, time=0.04.\n",
            "epoch 197/250, loss=0.0000, time=0.05.\n",
            "epoch 198/250, loss=0.0000, time=0.05.\n",
            "epoch 199/250, loss=0.0000, time=0.05.\n",
            "epoch 200/250, loss=0.0000, time=0.05.\n",
            "epoch 201/250, loss=0.0000, time=0.04.\n",
            "epoch 202/250, loss=0.0000, time=0.04.\n",
            "epoch 203/250, loss=0.0000, time=0.05.\n",
            "epoch 204/250, loss=0.0000, time=0.05.\n",
            "epoch 205/250, loss=0.0000, time=0.05.\n",
            "epoch 206/250, loss=0.0000, time=0.04.\n",
            "epoch 207/250, loss=0.0000, time=0.05.\n",
            "epoch 208/250, loss=0.0000, time=0.05.\n",
            "epoch 209/250, loss=0.0000, time=0.05.\n",
            "epoch 210/250, loss=0.0000, time=0.05.\n",
            "epoch 211/250, loss=0.0000, time=0.04.\n",
            "epoch 212/250, loss=0.0000, time=0.05.\n",
            "epoch 213/250, loss=0.0000, time=0.05.\n",
            "epoch 214/250, loss=0.0000, time=0.05.\n",
            "epoch 215/250, loss=0.0000, time=0.05.\n",
            "epoch 216/250, loss=0.0000, time=0.04.\n",
            "epoch 217/250, loss=0.0000, time=0.05.\n",
            "epoch 218/250, loss=0.0000, time=0.05.\n",
            "epoch 219/250, loss=0.0000, time=0.05.\n",
            "epoch 220/250, loss=0.0000, time=0.05.\n",
            "epoch 221/250, loss=0.0000, time=0.04.\n",
            "epoch 222/250, loss=0.0000, time=0.05.\n",
            "epoch 223/250, loss=0.0000, time=0.05.\n",
            "epoch 224/250, loss=0.0000, time=0.05.\n",
            "epoch 225/250, loss=0.0000, time=0.05.\n",
            "epoch 226/250, loss=0.0000, time=0.04.\n",
            "epoch 227/250, loss=0.0000, time=0.04.\n",
            "epoch 228/250, loss=0.0000, time=0.05.\n",
            "epoch 229/250, loss=0.0000, time=0.05.\n",
            "epoch 230/250, loss=0.0000, time=0.05.\n",
            "epoch 231/250, loss=0.0000, time=0.04.\n",
            "epoch 232/250, loss=0.0000, time=0.05.\n",
            "epoch 233/250, loss=0.0000, time=0.05.\n",
            "epoch 234/250, loss=0.0000, time=0.05.\n",
            "epoch 235/250, loss=0.0000, time=0.05.\n",
            "epoch 236/250, loss=0.0000, time=0.04.\n",
            "epoch 237/250, loss=0.0000, time=0.05.\n",
            "epoch 238/250, loss=0.0000, time=0.05.\n",
            "epoch 239/250, loss=0.0000, time=0.05.\n",
            "epoch 240/250, loss=0.0000, time=0.05.\n",
            "epoch 241/250, loss=0.0000, time=0.04.\n",
            "epoch 242/250, loss=0.0000, time=0.05.\n",
            "epoch 243/250, loss=0.0000, time=0.05.\n",
            "epoch 244/250, loss=0.0000, time=0.05.\n",
            "epoch 245/250, loss=0.0000, time=0.05.\n",
            "epoch 246/250, loss=0.0000, time=0.04.\n",
            "epoch 247/250, loss=0.0000, time=0.05.\n",
            "epoch 248/250, loss=0.0000, time=0.05.\n",
            "epoch 249/250, loss=0.0000, time=0.05.\n",
            "epoch 250/250, loss=0.0000, time=0.05.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0129.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0094.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0079.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0009.npz, 0.6889, 0.6667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0169.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0164.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0139.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0144.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0154.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0244.npz, 0.7000, 0.7000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0249.npz, 0.7000, 0.7000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0084.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0214.npz, 0.7000, 0.7000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0184.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0039.npz, 0.6889, 0.7000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0019.npz, 0.6667, 0.6556, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0134.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0234.npz, 0.7000, 0.7000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0124.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0089.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0029.npz, 0.7111, 0.7111, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0159.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0204.npz, 0.7000, 0.7000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0179.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0044.npz, 0.7000, 0.7000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0014.npz, 0.6667, 0.7000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0229.npz, 0.7000, 0.7000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0209.npz, 0.7000, 0.7000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0024.npz, 0.6778, 0.7333, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0199.npz, 0.7000, 0.7000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0054.npz, 0.7000, 0.7111, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0069.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0114.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0174.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0219.npz, 0.7000, 0.7000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0194.npz, 0.7000, 0.7111, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0049.npz, 0.6889, 0.7111, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0239.npz, 0.7000, 0.7000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0004.npz, 0.6556, 0.6333, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0189.npz, 0.7000, 0.7111, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0034.npz, 0.6889, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0099.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0119.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0149.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0059.npz, 0.7000, 0.7111, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0064.npz, 0.7000, 0.7000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0109.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0104.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0074.npz, 0.7000, 0.7222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandGenderCh2/alst_sc_c_0000_0224.npz, 0.7000, 0.7000, 0.01\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=1.8232, time=0.08.\n",
            "epoch 2/250, loss=1.7162, time=0.08.\n",
            "epoch 3/250, loss=1.6433, time=0.06.\n",
            "epoch 4/250, loss=1.4873, time=0.05.\n",
            "epoch 5/250, loss=1.3438, time=0.05.\n",
            "epoch 6/250, loss=1.1993, time=0.04.\n",
            "epoch 7/250, loss=1.0289, time=0.05.\n",
            "epoch 8/250, loss=0.9463, time=0.05.\n",
            "epoch 9/250, loss=0.8250, time=0.05.\n",
            "epoch 10/250, loss=0.7438, time=0.05.\n",
            "epoch 11/250, loss=0.6445, time=0.04.\n",
            "epoch 12/250, loss=0.5525, time=0.05.\n",
            "epoch 13/250, loss=0.5553, time=0.05.\n",
            "epoch 14/250, loss=0.5143, time=0.05.\n",
            "epoch 15/250, loss=0.6198, time=0.05.\n",
            "epoch 16/250, loss=0.5119, time=0.04.\n",
            "epoch 17/250, loss=0.5760, time=0.05.\n",
            "epoch 18/250, loss=0.4547, time=0.05.\n",
            "epoch 19/250, loss=0.3576, time=0.05.\n",
            "epoch 20/250, loss=0.2934, time=0.05.\n",
            "epoch 21/250, loss=0.2469, time=0.04.\n",
            "epoch 22/250, loss=0.1709, time=0.05.\n",
            "epoch 23/250, loss=0.1652, time=0.05.\n",
            "epoch 24/250, loss=0.1389, time=0.05.\n",
            "epoch 25/250, loss=0.0995, time=0.05.\n",
            "epoch 26/250, loss=0.0695, time=0.04.\n",
            "epoch 27/250, loss=0.0386, time=0.04.\n",
            "epoch 28/250, loss=0.0288, time=0.05.\n",
            "epoch 29/250, loss=0.0179, time=0.05.\n",
            "epoch 30/250, loss=0.0118, time=0.05.\n",
            "epoch 31/250, loss=0.0098, time=0.04.\n",
            "epoch 32/250, loss=0.0082, time=0.04.\n",
            "epoch 33/250, loss=0.0066, time=0.05.\n",
            "epoch 34/250, loss=0.0057, time=0.05.\n",
            "epoch 35/250, loss=0.0051, time=0.05.\n",
            "epoch 36/250, loss=0.0046, time=0.04.\n",
            "epoch 37/250, loss=0.0041, time=0.04.\n",
            "epoch 38/250, loss=0.0038, time=0.05.\n",
            "epoch 39/250, loss=0.0036, time=0.05.\n",
            "epoch 40/250, loss=0.0033, time=0.05.\n",
            "epoch 41/250, loss=0.0030, time=0.04.\n",
            "epoch 42/250, loss=0.0029, time=0.05.\n",
            "epoch 43/250, loss=0.0027, time=0.05.\n",
            "epoch 44/250, loss=0.0026, time=0.05.\n",
            "epoch 45/250, loss=0.0025, time=0.05.\n",
            "epoch 46/250, loss=0.0024, time=0.04.\n",
            "epoch 47/250, loss=0.0022, time=0.05.\n",
            "epoch 48/250, loss=0.0022, time=0.05.\n",
            "epoch 49/250, loss=0.0020, time=0.05.\n",
            "epoch 50/250, loss=0.0020, time=0.05.\n",
            "epoch 51/250, loss=0.0019, time=0.04.\n",
            "epoch 52/250, loss=0.0018, time=0.04.\n",
            "epoch 53/250, loss=0.0017, time=0.05.\n",
            "epoch 54/250, loss=0.0017, time=0.05.\n",
            "epoch 55/250, loss=0.0016, time=0.05.\n",
            "epoch 56/250, loss=0.0015, time=0.04.\n",
            "epoch 57/250, loss=0.0015, time=0.04.\n",
            "epoch 58/250, loss=0.0015, time=0.05.\n",
            "epoch 59/250, loss=0.0014, time=0.05.\n",
            "epoch 60/250, loss=0.0014, time=0.05.\n",
            "epoch 61/250, loss=0.0013, time=0.04.\n",
            "epoch 62/250, loss=0.0013, time=0.05.\n",
            "epoch 63/250, loss=0.0012, time=0.05.\n",
            "epoch 64/250, loss=0.0012, time=0.05.\n",
            "epoch 65/250, loss=0.0012, time=0.05.\n",
            "epoch 66/250, loss=0.0012, time=0.04.\n",
            "epoch 67/250, loss=0.0011, time=0.05.\n",
            "epoch 68/250, loss=0.0011, time=0.05.\n",
            "epoch 69/250, loss=0.0010, time=0.05.\n",
            "epoch 70/250, loss=0.0010, time=0.05.\n",
            "epoch 71/250, loss=0.0010, time=0.04.\n",
            "epoch 72/250, loss=0.0009, time=0.05.\n",
            "epoch 73/250, loss=0.0009, time=0.05.\n",
            "epoch 74/250, loss=0.0009, time=0.05.\n",
            "epoch 75/250, loss=0.0009, time=0.05.\n",
            "epoch 76/250, loss=0.0009, time=0.04.\n",
            "epoch 77/250, loss=0.0009, time=0.05.\n",
            "epoch 78/250, loss=0.0008, time=0.05.\n",
            "epoch 79/250, loss=0.0008, time=0.05.\n",
            "epoch 80/250, loss=0.0008, time=0.05.\n",
            "epoch 81/250, loss=0.0008, time=0.04.\n",
            "epoch 82/250, loss=0.0008, time=0.05.\n",
            "epoch 83/250, loss=0.0007, time=0.05.\n",
            "epoch 84/250, loss=0.0007, time=0.05.\n",
            "epoch 85/250, loss=0.0007, time=0.05.\n",
            "epoch 86/250, loss=0.0007, time=0.04.\n",
            "epoch 87/250, loss=0.0007, time=0.04.\n",
            "epoch 88/250, loss=0.0006, time=0.05.\n",
            "epoch 89/250, loss=0.0007, time=0.05.\n",
            "epoch 90/250, loss=0.0006, time=0.05.\n",
            "epoch 91/250, loss=0.0006, time=0.04.\n",
            "epoch 92/250, loss=0.0006, time=0.04.\n",
            "epoch 93/250, loss=0.0006, time=0.05.\n",
            "epoch 94/250, loss=0.0006, time=0.05.\n",
            "epoch 95/250, loss=0.0006, time=0.05.\n",
            "epoch 96/250, loss=0.0006, time=0.04.\n",
            "epoch 97/250, loss=0.0006, time=0.04.\n",
            "epoch 98/250, loss=0.0006, time=0.05.\n",
            "epoch 99/250, loss=0.0005, time=0.05.\n",
            "epoch 100/250, loss=0.0005, time=0.05.\n",
            "epoch 101/250, loss=0.0005, time=0.04.\n",
            "epoch 102/250, loss=0.0005, time=0.04.\n",
            "epoch 103/250, loss=0.0005, time=0.05.\n",
            "epoch 104/250, loss=0.0005, time=0.05.\n",
            "epoch 105/250, loss=0.0005, time=0.05.\n",
            "epoch 106/250, loss=0.0005, time=0.04.\n",
            "epoch 107/250, loss=0.0005, time=0.05.\n",
            "epoch 108/250, loss=0.0005, time=0.05.\n",
            "epoch 109/250, loss=0.0005, time=0.05.\n",
            "epoch 110/250, loss=0.0004, time=0.05.\n",
            "epoch 111/250, loss=0.0004, time=0.04.\n",
            "epoch 112/250, loss=0.0004, time=0.05.\n",
            "epoch 113/250, loss=0.0004, time=0.05.\n",
            "epoch 114/250, loss=0.0004, time=0.05.\n",
            "epoch 115/250, loss=0.0004, time=0.05.\n",
            "epoch 116/250, loss=0.0004, time=0.04.\n",
            "epoch 117/250, loss=0.0004, time=0.05.\n",
            "epoch 118/250, loss=0.0004, time=0.05.\n",
            "epoch 119/250, loss=0.0004, time=0.05.\n",
            "epoch 120/250, loss=0.0004, time=0.05.\n",
            "epoch 121/250, loss=0.0004, time=0.04.\n",
            "epoch 122/250, loss=0.0004, time=0.05.\n",
            "epoch 123/250, loss=0.0004, time=0.05.\n",
            "epoch 124/250, loss=0.0004, time=0.05.\n",
            "epoch 125/250, loss=0.0004, time=0.05.\n",
            "epoch 126/250, loss=0.0004, time=0.04.\n",
            "epoch 127/250, loss=0.0003, time=0.04.\n",
            "epoch 128/250, loss=0.0003, time=0.05.\n",
            "epoch 129/250, loss=0.0003, time=0.05.\n",
            "epoch 130/250, loss=0.0003, time=0.05.\n",
            "epoch 131/250, loss=0.0003, time=0.04.\n",
            "epoch 132/250, loss=0.0003, time=0.05.\n",
            "epoch 133/250, loss=0.0003, time=0.05.\n",
            "epoch 134/250, loss=0.0003, time=0.05.\n",
            "epoch 135/250, loss=0.0003, time=0.05.\n",
            "epoch 136/250, loss=0.0003, time=0.04.\n",
            "epoch 137/250, loss=0.0003, time=0.05.\n",
            "epoch 138/250, loss=0.0003, time=0.05.\n",
            "epoch 139/250, loss=0.0003, time=0.05.\n",
            "epoch 140/250, loss=0.0003, time=0.05.\n",
            "epoch 141/250, loss=0.0003, time=0.04.\n",
            "epoch 142/250, loss=0.0003, time=0.04.\n",
            "epoch 143/250, loss=0.0003, time=0.05.\n",
            "epoch 144/250, loss=0.0003, time=0.05.\n",
            "epoch 145/250, loss=0.0003, time=0.05.\n",
            "epoch 146/250, loss=0.0003, time=0.04.\n",
            "epoch 147/250, loss=0.0003, time=0.05.\n",
            "epoch 148/250, loss=0.0003, time=0.05.\n",
            "epoch 149/250, loss=0.0003, time=0.05.\n",
            "epoch 150/250, loss=0.0003, time=0.05.\n",
            "epoch 151/250, loss=0.0003, time=0.04.\n",
            "epoch 152/250, loss=0.0003, time=0.04.\n",
            "epoch 153/250, loss=0.0003, time=0.05.\n",
            "epoch 154/250, loss=0.0003, time=0.05.\n",
            "epoch 155/250, loss=0.0002, time=0.05.\n",
            "epoch 156/250, loss=0.0002, time=0.04.\n",
            "epoch 157/250, loss=0.0002, time=0.05.\n",
            "epoch 158/250, loss=0.0002, time=0.05.\n",
            "epoch 159/250, loss=0.0002, time=0.05.\n",
            "epoch 160/250, loss=0.0002, time=0.05.\n",
            "epoch 161/250, loss=0.0002, time=0.04.\n",
            "epoch 162/250, loss=0.0002, time=0.05.\n",
            "epoch 163/250, loss=0.0002, time=0.05.\n",
            "epoch 164/250, loss=0.0002, time=0.05.\n",
            "epoch 165/250, loss=0.0002, time=0.05.\n",
            "epoch 166/250, loss=0.0002, time=0.04.\n",
            "epoch 167/250, loss=0.0002, time=0.05.\n",
            "epoch 168/250, loss=0.0002, time=0.05.\n",
            "epoch 169/250, loss=0.0002, time=0.05.\n",
            "epoch 170/250, loss=0.0002, time=0.05.\n",
            "epoch 171/250, loss=0.0002, time=0.04.\n",
            "epoch 172/250, loss=0.0002, time=0.05.\n",
            "epoch 173/250, loss=0.0002, time=0.05.\n",
            "epoch 174/250, loss=0.0002, time=0.05.\n",
            "epoch 175/250, loss=0.0002, time=0.05.\n",
            "epoch 176/250, loss=0.0002, time=0.04.\n",
            "epoch 177/250, loss=0.0002, time=0.05.\n",
            "epoch 178/250, loss=0.0002, time=0.05.\n",
            "epoch 179/250, loss=0.0002, time=0.04.\n",
            "epoch 180/250, loss=0.0002, time=0.05.\n",
            "epoch 181/250, loss=0.0002, time=0.04.\n",
            "epoch 182/250, loss=0.0002, time=0.04.\n",
            "epoch 183/250, loss=0.0002, time=0.05.\n",
            "epoch 184/250, loss=0.0002, time=0.05.\n",
            "epoch 185/250, loss=0.0002, time=0.05.\n",
            "epoch 186/250, loss=0.0002, time=0.04.\n",
            "epoch 187/250, loss=0.0002, time=0.05.\n",
            "epoch 188/250, loss=0.0002, time=0.05.\n",
            "epoch 189/250, loss=0.0002, time=0.05.\n",
            "epoch 190/250, loss=0.0002, time=0.05.\n",
            "epoch 191/250, loss=0.0002, time=0.04.\n",
            "epoch 192/250, loss=0.0002, time=0.05.\n",
            "epoch 193/250, loss=0.0002, time=0.05.\n",
            "epoch 194/250, loss=0.0002, time=0.05.\n",
            "epoch 195/250, loss=0.0002, time=0.05.\n",
            "epoch 196/250, loss=0.0002, time=0.04.\n",
            "epoch 197/250, loss=0.0002, time=0.04.\n",
            "epoch 198/250, loss=0.0002, time=0.05.\n",
            "epoch 199/250, loss=0.0002, time=0.05.\n",
            "epoch 200/250, loss=0.0002, time=0.05.\n",
            "epoch 201/250, loss=0.0002, time=0.04.\n",
            "epoch 202/250, loss=0.0002, time=0.05.\n",
            "epoch 203/250, loss=0.0002, time=0.05.\n",
            "epoch 204/250, loss=0.0002, time=0.05.\n",
            "epoch 205/250, loss=0.0002, time=0.05.\n",
            "epoch 206/250, loss=0.0001, time=0.04.\n",
            "epoch 207/250, loss=0.0002, time=0.04.\n",
            "epoch 208/250, loss=0.0001, time=0.05.\n",
            "epoch 209/250, loss=0.0001, time=0.05.\n",
            "epoch 210/250, loss=0.0001, time=0.05.\n",
            "epoch 211/250, loss=0.0001, time=0.04.\n",
            "epoch 212/250, loss=0.0001, time=0.04.\n",
            "epoch 213/250, loss=0.0001, time=0.05.\n",
            "epoch 214/250, loss=0.0001, time=0.05.\n",
            "epoch 215/250, loss=0.0001, time=0.05.\n",
            "epoch 216/250, loss=0.0001, time=0.04.\n",
            "epoch 217/250, loss=0.0001, time=0.04.\n",
            "epoch 218/250, loss=0.0001, time=0.05.\n",
            "epoch 219/250, loss=0.0001, time=0.05.\n",
            "epoch 220/250, loss=0.0001, time=0.05.\n",
            "epoch 221/250, loss=0.0001, time=0.04.\n",
            "epoch 222/250, loss=0.0001, time=0.04.\n",
            "epoch 223/250, loss=0.0001, time=0.05.\n",
            "epoch 224/250, loss=0.0001, time=0.05.\n",
            "epoch 225/250, loss=0.0001, time=0.05.\n",
            "epoch 226/250, loss=0.0001, time=0.04.\n",
            "epoch 227/250, loss=0.0001, time=0.05.\n",
            "epoch 228/250, loss=0.0001, time=0.05.\n",
            "epoch 229/250, loss=0.0001, time=0.05.\n",
            "epoch 230/250, loss=0.0001, time=0.05.\n",
            "epoch 231/250, loss=0.0001, time=0.04.\n",
            "epoch 232/250, loss=0.0001, time=0.04.\n",
            "epoch 233/250, loss=0.0001, time=0.05.\n",
            "epoch 234/250, loss=0.0001, time=0.05.\n",
            "epoch 235/250, loss=0.0001, time=0.05.\n",
            "epoch 236/250, loss=0.0001, time=0.04.\n",
            "epoch 237/250, loss=0.0001, time=0.05.\n",
            "epoch 238/250, loss=0.0001, time=0.05.\n",
            "epoch 239/250, loss=0.0001, time=0.05.\n",
            "epoch 240/250, loss=0.0001, time=0.05.\n",
            "epoch 241/250, loss=0.0001, time=0.04.\n",
            "epoch 242/250, loss=0.0001, time=0.05.\n",
            "epoch 243/250, loss=0.0001, time=0.05.\n",
            "epoch 244/250, loss=0.0001, time=0.05.\n",
            "epoch 245/250, loss=0.0001, time=0.05.\n",
            "epoch 246/250, loss=0.0001, time=0.04.\n",
            "epoch 247/250, loss=0.0001, time=0.05.\n",
            "epoch 248/250, loss=0.0001, time=0.05.\n",
            "epoch 249/250, loss=0.0001, time=0.05.\n",
            "epoch 250/250, loss=0.0001, time=0.05.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0229.npz, 0.2667, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0104.npz, 0.2667, 0.2667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0004.npz, 0.2889, 0.2556, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0204.npz, 0.2556, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0154.npz, 0.2556, 0.2778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0169.npz, 0.2556, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0124.npz, 0.2556, 0.2667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0019.npz, 0.3556, 0.3111, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0069.npz, 0.2778, 0.2778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0134.npz, 0.2556, 0.2667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0034.npz, 0.2889, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0249.npz, 0.2667, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0129.npz, 0.2556, 0.2667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0159.npz, 0.2556, 0.2778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0174.npz, 0.2556, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0214.npz, 0.2556, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0024.npz, 0.3556, 0.2778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0109.npz, 0.2667, 0.2667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0089.npz, 0.2667, 0.2667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0054.npz, 0.2778, 0.2778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0064.npz, 0.2778, 0.2778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0199.npz, 0.2556, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0044.npz, 0.2889, 0.2778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0149.npz, 0.2556, 0.2778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0094.npz, 0.2667, 0.2667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0234.npz, 0.2667, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0009.npz, 0.3444, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0029.npz, 0.3444, 0.3111, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0244.npz, 0.2667, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0039.npz, 0.2889, 0.2778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0139.npz, 0.2556, 0.2778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0049.npz, 0.2778, 0.2778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0084.npz, 0.2667, 0.2667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0114.npz, 0.2556, 0.2667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0014.npz, 0.3333, 0.2444, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0059.npz, 0.2778, 0.2778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0239.npz, 0.2667, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0074.npz, 0.2778, 0.2667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0099.npz, 0.2667, 0.2667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0184.npz, 0.2556, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0209.npz, 0.2556, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0164.npz, 0.2556, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0224.npz, 0.2667, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0194.npz, 0.2556, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0219.npz, 0.2556, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0079.npz, 0.2778, 0.2667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0119.npz, 0.2556, 0.2667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0189.npz, 0.2556, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0179.npz, 0.2556, 0.2889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandMovementCh2/alst_sc_c_0000_0144.npz, 0.2556, 0.2778, 0.01\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=1.6616, time=0.08.\n",
            "epoch 2/250, loss=1.5396, time=0.07.\n",
            "epoch 3/250, loss=1.4494, time=0.06.\n",
            "epoch 4/250, loss=1.3019, time=0.06.\n",
            "epoch 5/250, loss=1.2531, time=0.06.\n",
            "epoch 6/250, loss=1.1462, time=0.04.\n",
            "epoch 7/250, loss=1.0029, time=0.04.\n",
            "epoch 8/250, loss=0.8502, time=0.05.\n",
            "epoch 9/250, loss=0.8086, time=0.05.\n",
            "epoch 10/250, loss=0.7636, time=0.05.\n",
            "epoch 11/250, loss=0.7075, time=0.04.\n",
            "epoch 12/250, loss=0.6086, time=0.05.\n",
            "epoch 13/250, loss=0.5077, time=0.05.\n",
            "epoch 14/250, loss=0.4155, time=0.05.\n",
            "epoch 15/250, loss=0.3159, time=0.05.\n",
            "epoch 16/250, loss=0.2564, time=0.04.\n",
            "epoch 17/250, loss=0.2248, time=0.05.\n",
            "epoch 18/250, loss=0.1693, time=0.05.\n",
            "epoch 19/250, loss=0.1629, time=0.05.\n",
            "epoch 20/250, loss=0.3060, time=0.05.\n",
            "epoch 21/250, loss=0.3031, time=0.04.\n",
            "epoch 22/250, loss=0.5661, time=0.05.\n",
            "epoch 23/250, loss=0.7558, time=0.05.\n",
            "epoch 24/250, loss=0.5279, time=0.05.\n",
            "epoch 25/250, loss=0.5285, time=0.05.\n",
            "epoch 26/250, loss=0.3632, time=0.04.\n",
            "epoch 27/250, loss=0.2581, time=0.05.\n",
            "epoch 28/250, loss=0.1881, time=0.05.\n",
            "epoch 29/250, loss=0.1726, time=0.05.\n",
            "epoch 30/250, loss=0.1185, time=0.05.\n",
            "epoch 31/250, loss=0.1820, time=0.04.\n",
            "epoch 32/250, loss=0.2497, time=0.04.\n",
            "epoch 33/250, loss=0.2109, time=0.05.\n",
            "epoch 34/250, loss=0.1603, time=0.05.\n",
            "epoch 35/250, loss=0.1194, time=0.05.\n",
            "epoch 36/250, loss=0.0719, time=0.04.\n",
            "epoch 37/250, loss=0.0354, time=0.04.\n",
            "epoch 38/250, loss=0.0285, time=0.05.\n",
            "epoch 39/250, loss=0.0385, time=0.05.\n",
            "epoch 40/250, loss=0.0280, time=0.05.\n",
            "epoch 41/250, loss=0.0174, time=0.04.\n",
            "epoch 42/250, loss=0.0211, time=0.04.\n",
            "epoch 43/250, loss=0.0121, time=0.05.\n",
            "epoch 44/250, loss=0.0080, time=0.05.\n",
            "epoch 45/250, loss=0.0059, time=0.05.\n",
            "epoch 46/250, loss=0.0043, time=0.04.\n",
            "epoch 47/250, loss=0.0035, time=0.04.\n",
            "epoch 48/250, loss=0.0027, time=0.05.\n",
            "epoch 49/250, loss=0.0026, time=0.05.\n",
            "epoch 50/250, loss=0.0022, time=0.05.\n",
            "epoch 51/250, loss=0.0021, time=0.04.\n",
            "epoch 52/250, loss=0.0020, time=0.05.\n",
            "epoch 53/250, loss=0.0017, time=0.05.\n",
            "epoch 54/250, loss=0.0017, time=0.05.\n",
            "epoch 55/250, loss=0.0016, time=0.05.\n",
            "epoch 56/250, loss=0.0015, time=0.04.\n",
            "epoch 57/250, loss=0.0014, time=0.04.\n",
            "epoch 58/250, loss=0.0013, time=0.05.\n",
            "epoch 59/250, loss=0.0013, time=0.05.\n",
            "epoch 60/250, loss=0.0013, time=0.05.\n",
            "epoch 61/250, loss=0.0012, time=0.04.\n",
            "epoch 62/250, loss=0.0012, time=0.04.\n",
            "epoch 63/250, loss=0.0012, time=0.05.\n",
            "epoch 64/250, loss=0.0011, time=0.05.\n",
            "epoch 65/250, loss=0.0010, time=0.05.\n",
            "epoch 66/250, loss=0.0010, time=0.04.\n",
            "epoch 67/250, loss=0.0010, time=0.04.\n",
            "epoch 68/250, loss=0.0009, time=0.05.\n",
            "epoch 69/250, loss=0.0009, time=0.05.\n",
            "epoch 70/250, loss=0.0009, time=0.05.\n",
            "epoch 71/250, loss=0.0009, time=0.04.\n",
            "epoch 72/250, loss=0.0008, time=0.05.\n",
            "epoch 73/250, loss=0.0008, time=0.05.\n",
            "epoch 74/250, loss=0.0008, time=0.05.\n",
            "epoch 75/250, loss=0.0008, time=0.05.\n",
            "epoch 76/250, loss=0.0007, time=0.04.\n",
            "epoch 77/250, loss=0.0007, time=0.05.\n",
            "epoch 78/250, loss=0.0007, time=0.05.\n",
            "epoch 79/250, loss=0.0007, time=0.05.\n",
            "epoch 80/250, loss=0.0006, time=0.05.\n",
            "epoch 81/250, loss=0.0006, time=0.04.\n",
            "epoch 82/250, loss=0.0006, time=0.05.\n",
            "epoch 83/250, loss=0.0006, time=0.05.\n",
            "epoch 84/250, loss=0.0006, time=0.05.\n",
            "epoch 85/250, loss=0.0006, time=0.05.\n",
            "epoch 86/250, loss=0.0006, time=0.04.\n",
            "epoch 87/250, loss=0.0006, time=0.05.\n",
            "epoch 88/250, loss=0.0006, time=0.05.\n",
            "epoch 89/250, loss=0.0006, time=0.05.\n",
            "epoch 90/250, loss=0.0005, time=0.05.\n",
            "epoch 91/250, loss=0.0005, time=0.04.\n",
            "epoch 92/250, loss=0.0005, time=0.05.\n",
            "epoch 93/250, loss=0.0005, time=0.05.\n",
            "epoch 94/250, loss=0.0005, time=0.05.\n",
            "epoch 95/250, loss=0.0005, time=0.05.\n",
            "epoch 96/250, loss=0.0004, time=0.04.\n",
            "epoch 97/250, loss=0.0005, time=0.05.\n",
            "epoch 98/250, loss=0.0005, time=0.05.\n",
            "epoch 99/250, loss=0.0004, time=0.05.\n",
            "epoch 100/250, loss=0.0004, time=0.05.\n",
            "epoch 101/250, loss=0.0004, time=0.04.\n",
            "epoch 102/250, loss=0.0004, time=0.05.\n",
            "epoch 103/250, loss=0.0004, time=0.05.\n",
            "epoch 104/250, loss=0.0004, time=0.05.\n",
            "epoch 105/250, loss=0.0004, time=0.05.\n",
            "epoch 106/250, loss=0.0004, time=0.04.\n",
            "epoch 107/250, loss=0.0004, time=0.05.\n",
            "epoch 108/250, loss=0.0004, time=0.05.\n",
            "epoch 109/250, loss=0.0004, time=0.05.\n",
            "epoch 110/250, loss=0.0004, time=0.05.\n",
            "epoch 111/250, loss=0.0004, time=0.04.\n",
            "epoch 112/250, loss=0.0003, time=0.05.\n",
            "epoch 113/250, loss=0.0003, time=0.05.\n",
            "epoch 114/250, loss=0.0003, time=0.05.\n",
            "epoch 115/250, loss=0.0004, time=0.05.\n",
            "epoch 116/250, loss=0.0003, time=0.04.\n",
            "epoch 117/250, loss=0.0003, time=0.05.\n",
            "epoch 118/250, loss=0.0003, time=0.05.\n",
            "epoch 119/250, loss=0.0003, time=0.05.\n",
            "epoch 120/250, loss=0.0003, time=0.05.\n",
            "epoch 121/250, loss=0.0003, time=0.04.\n",
            "epoch 122/250, loss=0.0003, time=0.05.\n",
            "epoch 123/250, loss=0.0003, time=0.05.\n",
            "epoch 124/250, loss=0.0003, time=0.05.\n",
            "epoch 125/250, loss=0.0003, time=0.05.\n",
            "epoch 126/250, loss=0.0003, time=0.04.\n",
            "epoch 127/250, loss=0.0003, time=0.04.\n",
            "epoch 128/250, loss=0.0003, time=0.05.\n",
            "epoch 129/250, loss=0.0003, time=0.05.\n",
            "epoch 130/250, loss=0.0003, time=0.05.\n",
            "epoch 131/250, loss=0.0003, time=0.04.\n",
            "epoch 132/250, loss=0.0002, time=0.05.\n",
            "epoch 133/250, loss=0.0002, time=0.05.\n",
            "epoch 134/250, loss=0.0002, time=0.05.\n",
            "epoch 135/250, loss=0.0002, time=0.05.\n",
            "epoch 136/250, loss=0.0002, time=0.04.\n",
            "epoch 137/250, loss=0.0002, time=0.05.\n",
            "epoch 138/250, loss=0.0002, time=0.05.\n",
            "epoch 139/250, loss=0.0002, time=0.05.\n",
            "epoch 140/250, loss=0.0002, time=0.05.\n",
            "epoch 141/250, loss=0.0002, time=0.04.\n",
            "epoch 142/250, loss=0.0002, time=0.05.\n",
            "epoch 143/250, loss=0.0002, time=0.05.\n",
            "epoch 144/250, loss=0.0002, time=0.05.\n",
            "epoch 145/250, loss=0.0002, time=0.05.\n",
            "epoch 146/250, loss=0.0002, time=0.04.\n",
            "epoch 147/250, loss=0.0002, time=0.04.\n",
            "epoch 148/250, loss=0.0002, time=0.05.\n",
            "epoch 149/250, loss=0.0002, time=0.05.\n",
            "epoch 150/250, loss=0.0002, time=0.05.\n",
            "epoch 151/250, loss=0.0002, time=0.04.\n",
            "epoch 152/250, loss=0.0002, time=0.05.\n",
            "epoch 153/250, loss=0.0002, time=0.05.\n",
            "epoch 154/250, loss=0.0002, time=0.05.\n",
            "epoch 155/250, loss=0.0002, time=0.05.\n",
            "epoch 156/250, loss=0.0002, time=0.04.\n",
            "epoch 157/250, loss=0.0002, time=0.04.\n",
            "epoch 158/250, loss=0.0002, time=0.05.\n",
            "epoch 159/250, loss=0.0002, time=0.05.\n",
            "epoch 160/250, loss=0.0002, time=0.05.\n",
            "epoch 161/250, loss=0.0002, time=0.04.\n",
            "epoch 162/250, loss=0.0002, time=0.04.\n",
            "epoch 163/250, loss=0.0002, time=0.05.\n",
            "epoch 164/250, loss=0.0002, time=0.05.\n",
            "epoch 165/250, loss=0.0002, time=0.05.\n",
            "epoch 166/250, loss=0.0002, time=0.04.\n",
            "epoch 167/250, loss=0.0002, time=0.05.\n",
            "epoch 168/250, loss=0.0002, time=0.05.\n",
            "epoch 169/250, loss=0.0002, time=0.05.\n",
            "epoch 170/250, loss=0.0002, time=0.05.\n",
            "epoch 171/250, loss=0.0002, time=0.04.\n",
            "epoch 172/250, loss=0.0002, time=0.05.\n",
            "epoch 173/250, loss=0.0001, time=0.05.\n",
            "epoch 174/250, loss=0.0002, time=0.05.\n",
            "epoch 175/250, loss=0.0002, time=0.05.\n",
            "epoch 176/250, loss=0.0002, time=0.04.\n",
            "epoch 177/250, loss=0.0001, time=0.05.\n",
            "epoch 178/250, loss=0.0001, time=0.05.\n",
            "epoch 179/250, loss=0.0002, time=0.05.\n",
            "epoch 180/250, loss=0.0001, time=0.05.\n",
            "epoch 181/250, loss=0.0001, time=0.04.\n",
            "epoch 182/250, loss=0.0001, time=0.05.\n",
            "epoch 183/250, loss=0.0001, time=0.05.\n",
            "epoch 184/250, loss=0.0001, time=0.05.\n",
            "epoch 185/250, loss=0.0001, time=0.05.\n",
            "epoch 186/250, loss=0.0001, time=0.04.\n",
            "epoch 187/250, loss=0.0001, time=0.05.\n",
            "epoch 188/250, loss=0.0001, time=0.05.\n",
            "epoch 189/250, loss=0.0001, time=0.05.\n",
            "epoch 190/250, loss=0.0001, time=0.05.\n",
            "epoch 191/250, loss=0.0001, time=0.04.\n",
            "epoch 192/250, loss=0.0001, time=0.05.\n",
            "epoch 193/250, loss=0.0001, time=0.05.\n",
            "epoch 194/250, loss=0.0001, time=0.05.\n",
            "epoch 195/250, loss=0.0001, time=0.05.\n",
            "epoch 196/250, loss=0.0001, time=0.04.\n",
            "epoch 197/250, loss=0.0001, time=0.05.\n",
            "epoch 198/250, loss=0.0001, time=0.05.\n",
            "epoch 199/250, loss=0.0001, time=0.05.\n",
            "epoch 200/250, loss=0.0001, time=0.05.\n",
            "epoch 201/250, loss=0.0001, time=0.04.\n",
            "epoch 202/250, loss=0.0001, time=0.05.\n",
            "epoch 203/250, loss=0.0001, time=0.05.\n",
            "epoch 204/250, loss=0.0001, time=0.05.\n",
            "epoch 205/250, loss=0.0001, time=0.05.\n",
            "epoch 206/250, loss=0.0001, time=0.04.\n",
            "epoch 207/250, loss=0.0001, time=0.05.\n",
            "epoch 208/250, loss=0.0001, time=0.05.\n",
            "epoch 209/250, loss=0.0001, time=0.05.\n",
            "epoch 210/250, loss=0.0001, time=0.05.\n",
            "epoch 211/250, loss=0.0001, time=0.04.\n",
            "epoch 212/250, loss=0.0001, time=0.04.\n",
            "epoch 213/250, loss=0.0001, time=0.05.\n",
            "epoch 214/250, loss=0.0001, time=0.05.\n",
            "epoch 215/250, loss=0.0001, time=0.05.\n",
            "epoch 216/250, loss=0.0001, time=0.04.\n",
            "epoch 217/250, loss=0.0001, time=0.05.\n",
            "epoch 218/250, loss=0.0001, time=0.05.\n",
            "epoch 219/250, loss=0.0001, time=0.05.\n",
            "epoch 220/250, loss=0.0001, time=0.05.\n",
            "epoch 221/250, loss=0.0001, time=0.04.\n",
            "epoch 222/250, loss=0.0001, time=0.05.\n",
            "epoch 223/250, loss=0.0001, time=0.05.\n",
            "epoch 224/250, loss=0.0001, time=0.05.\n",
            "epoch 225/250, loss=0.0001, time=0.05.\n",
            "epoch 226/250, loss=0.0001, time=0.04.\n",
            "epoch 227/250, loss=0.0001, time=0.05.\n",
            "epoch 228/250, loss=0.0001, time=0.05.\n",
            "epoch 229/250, loss=0.0001, time=0.05.\n",
            "epoch 230/250, loss=0.0001, time=0.05.\n",
            "epoch 231/250, loss=0.0001, time=0.04.\n",
            "epoch 232/250, loss=0.0001, time=0.05.\n",
            "epoch 233/250, loss=0.0001, time=0.05.\n",
            "epoch 234/250, loss=0.0001, time=0.05.\n",
            "epoch 235/250, loss=0.0001, time=0.05.\n",
            "epoch 236/250, loss=0.0001, time=0.04.\n",
            "epoch 237/250, loss=0.0001, time=0.05.\n",
            "epoch 238/250, loss=0.0001, time=0.05.\n",
            "epoch 239/250, loss=0.0001, time=0.05.\n",
            "epoch 240/250, loss=0.0001, time=0.05.\n",
            "epoch 241/250, loss=0.0001, time=0.04.\n",
            "epoch 242/250, loss=0.0001, time=0.04.\n",
            "epoch 243/250, loss=0.0001, time=0.05.\n",
            "epoch 244/250, loss=0.0001, time=0.05.\n",
            "epoch 245/250, loss=0.0001, time=0.05.\n",
            "epoch 246/250, loss=0.0001, time=0.04.\n",
            "epoch 247/250, loss=0.0001, time=0.05.\n",
            "epoch 248/250, loss=0.0001, time=0.05.\n",
            "epoch 249/250, loss=0.0001, time=0.05.\n",
            "epoch 250/250, loss=0.0001, time=0.05.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0164.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0249.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0114.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0199.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0224.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0039.npz, 0.5556, 0.5111, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0179.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0229.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0234.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0054.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0149.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0214.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0034.npz, 0.5333, 0.5222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0074.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0069.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0154.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0219.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0204.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0129.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0239.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0194.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0029.npz, 0.5111, 0.5000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0099.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0209.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0189.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0059.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0044.npz, 0.6000, 0.5556, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0079.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0124.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0009.npz, 0.5333, 0.4667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0084.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0109.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0049.npz, 0.6222, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0064.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0094.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0134.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0244.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0119.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0024.npz, 0.4889, 0.5556, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0169.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0159.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0144.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0174.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0139.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0019.npz, 0.4556, 0.4889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0089.npz, 0.6111, 0.5778, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0014.npz, 0.5111, 0.5000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0004.npz, 0.5000, 0.3222, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0104.npz, 0.6111, 0.5889, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SemgHandSubjectCh2/alst_sc_c_0000_0184.npz, 0.6111, 0.5889, 0.01\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=2.3545, time=0.01.\n",
            "epoch 2/250, loss=2.0366, time=0.01.\n",
            "epoch 3/250, loss=1.8299, time=0.01.\n",
            "epoch 4/250, loss=1.7008, time=0.01.\n",
            "epoch 5/250, loss=1.6416, time=0.01.\n",
            "epoch 6/250, loss=1.5336, time=0.01.\n",
            "epoch 7/250, loss=1.4471, time=0.01.\n",
            "epoch 8/250, loss=1.3191, time=0.01.\n",
            "epoch 9/250, loss=1.2420, time=0.01.\n",
            "epoch 10/250, loss=1.1839, time=0.01.\n",
            "epoch 11/250, loss=1.1132, time=0.01.\n",
            "epoch 12/250, loss=1.0137, time=0.01.\n",
            "epoch 13/250, loss=0.8683, time=0.01.\n",
            "epoch 14/250, loss=0.9073, time=0.01.\n",
            "epoch 15/250, loss=0.8909, time=0.01.\n",
            "epoch 16/250, loss=0.8411, time=0.01.\n",
            "epoch 17/250, loss=0.7589, time=0.01.\n",
            "epoch 18/250, loss=0.7252, time=0.01.\n",
            "epoch 19/250, loss=0.6136, time=0.01.\n",
            "epoch 20/250, loss=0.4933, time=0.01.\n",
            "epoch 21/250, loss=0.3973, time=0.01.\n",
            "epoch 22/250, loss=0.3428, time=0.01.\n",
            "epoch 23/250, loss=0.2890, time=0.01.\n",
            "epoch 24/250, loss=0.2313, time=0.01.\n",
            "epoch 25/250, loss=0.1909, time=0.01.\n",
            "epoch 26/250, loss=0.1561, time=0.01.\n",
            "epoch 27/250, loss=0.1288, time=0.01.\n",
            "epoch 28/250, loss=0.1048, time=0.01.\n",
            "epoch 29/250, loss=0.0853, time=0.01.\n",
            "epoch 30/250, loss=0.0714, time=0.01.\n",
            "epoch 31/250, loss=0.0561, time=0.01.\n",
            "epoch 32/250, loss=0.0464, time=0.01.\n",
            "epoch 33/250, loss=0.0396, time=0.01.\n",
            "epoch 34/250, loss=0.0341, time=0.01.\n",
            "epoch 35/250, loss=0.0292, time=0.01.\n",
            "epoch 36/250, loss=0.0247, time=0.01.\n",
            "epoch 37/250, loss=0.0208, time=0.01.\n",
            "epoch 38/250, loss=0.0175, time=0.01.\n",
            "epoch 39/250, loss=0.0147, time=0.01.\n",
            "epoch 40/250, loss=0.0127, time=0.01.\n",
            "epoch 41/250, loss=0.0109, time=0.01.\n",
            "epoch 42/250, loss=0.0095, time=0.01.\n",
            "epoch 43/250, loss=0.0085, time=0.01.\n",
            "epoch 44/250, loss=0.0076, time=0.01.\n",
            "epoch 45/250, loss=0.0069, time=0.01.\n",
            "epoch 46/250, loss=0.0062, time=0.01.\n",
            "epoch 47/250, loss=0.0057, time=0.01.\n",
            "epoch 48/250, loss=0.0052, time=0.01.\n",
            "epoch 49/250, loss=0.0048, time=0.01.\n",
            "epoch 50/250, loss=0.0044, time=0.01.\n",
            "epoch 51/250, loss=0.0041, time=0.01.\n",
            "epoch 52/250, loss=0.0038, time=0.01.\n",
            "epoch 53/250, loss=0.0036, time=0.01.\n",
            "epoch 54/250, loss=0.0034, time=0.01.\n",
            "epoch 55/250, loss=0.0032, time=0.01.\n",
            "epoch 56/250, loss=0.0030, time=0.01.\n",
            "epoch 57/250, loss=0.0028, time=0.01.\n",
            "epoch 58/250, loss=0.0027, time=0.01.\n",
            "epoch 59/250, loss=0.0026, time=0.01.\n",
            "epoch 60/250, loss=0.0025, time=0.01.\n",
            "epoch 61/250, loss=0.0024, time=0.01.\n",
            "epoch 62/250, loss=0.0023, time=0.01.\n",
            "epoch 63/250, loss=0.0022, time=0.01.\n",
            "epoch 64/250, loss=0.0021, time=0.01.\n",
            "epoch 65/250, loss=0.0020, time=0.01.\n",
            "epoch 66/250, loss=0.0020, time=0.01.\n",
            "epoch 67/250, loss=0.0019, time=0.01.\n",
            "epoch 68/250, loss=0.0019, time=0.01.\n",
            "epoch 69/250, loss=0.0018, time=0.01.\n",
            "epoch 70/250, loss=0.0018, time=0.01.\n",
            "epoch 71/250, loss=0.0017, time=0.01.\n",
            "epoch 72/250, loss=0.0017, time=0.01.\n",
            "epoch 73/250, loss=0.0016, time=0.01.\n",
            "epoch 74/250, loss=0.0016, time=0.01.\n",
            "epoch 75/250, loss=0.0016, time=0.01.\n",
            "epoch 76/250, loss=0.0015, time=0.01.\n",
            "epoch 77/250, loss=0.0015, time=0.01.\n",
            "epoch 78/250, loss=0.0015, time=0.01.\n",
            "epoch 79/250, loss=0.0014, time=0.01.\n",
            "epoch 80/250, loss=0.0014, time=0.01.\n",
            "epoch 81/250, loss=0.0014, time=0.01.\n",
            "epoch 82/250, loss=0.0014, time=0.01.\n",
            "epoch 83/250, loss=0.0013, time=0.01.\n",
            "epoch 84/250, loss=0.0013, time=0.01.\n",
            "epoch 85/250, loss=0.0013, time=0.01.\n",
            "epoch 86/250, loss=0.0013, time=0.01.\n",
            "epoch 87/250, loss=0.0013, time=0.01.\n",
            "epoch 88/250, loss=0.0012, time=0.01.\n",
            "epoch 89/250, loss=0.0012, time=0.01.\n",
            "epoch 90/250, loss=0.0012, time=0.01.\n",
            "epoch 91/250, loss=0.0012, time=0.01.\n",
            "epoch 92/250, loss=0.0012, time=0.01.\n",
            "epoch 93/250, loss=0.0012, time=0.01.\n",
            "epoch 94/250, loss=0.0012, time=0.01.\n",
            "epoch 95/250, loss=0.0011, time=0.01.\n",
            "epoch 96/250, loss=0.0011, time=0.01.\n",
            "epoch 97/250, loss=0.0011, time=0.01.\n",
            "epoch 98/250, loss=0.0011, time=0.01.\n",
            "epoch 99/250, loss=0.0011, time=0.01.\n",
            "epoch 100/250, loss=0.0011, time=0.01.\n",
            "epoch 101/250, loss=0.0011, time=0.01.\n",
            "epoch 102/250, loss=0.0011, time=0.01.\n",
            "epoch 103/250, loss=0.0010, time=0.01.\n",
            "epoch 104/250, loss=0.0010, time=0.01.\n",
            "epoch 105/250, loss=0.0010, time=0.01.\n",
            "epoch 106/250, loss=0.0010, time=0.01.\n",
            "epoch 107/250, loss=0.0010, time=0.01.\n",
            "epoch 108/250, loss=0.0010, time=0.01.\n",
            "epoch 109/250, loss=0.0010, time=0.01.\n",
            "epoch 110/250, loss=0.0010, time=0.01.\n",
            "epoch 111/250, loss=0.0010, time=0.01.\n",
            "epoch 112/250, loss=0.0009, time=0.01.\n",
            "epoch 113/250, loss=0.0009, time=0.01.\n",
            "epoch 114/250, loss=0.0009, time=0.01.\n",
            "epoch 115/250, loss=0.0009, time=0.01.\n",
            "epoch 116/250, loss=0.0009, time=0.01.\n",
            "epoch 117/250, loss=0.0009, time=0.01.\n",
            "epoch 118/250, loss=0.0009, time=0.01.\n",
            "epoch 119/250, loss=0.0009, time=0.01.\n",
            "epoch 120/250, loss=0.0009, time=0.01.\n",
            "epoch 121/250, loss=0.0009, time=0.01.\n",
            "epoch 122/250, loss=0.0009, time=0.01.\n",
            "epoch 123/250, loss=0.0009, time=0.01.\n",
            "epoch 124/250, loss=0.0009, time=0.01.\n",
            "epoch 125/250, loss=0.0008, time=0.01.\n",
            "epoch 126/250, loss=0.0008, time=0.01.\n",
            "epoch 127/250, loss=0.0008, time=0.01.\n",
            "epoch 128/250, loss=0.0008, time=0.01.\n",
            "epoch 129/250, loss=0.0008, time=0.01.\n",
            "epoch 130/250, loss=0.0008, time=0.01.\n",
            "epoch 131/250, loss=0.0008, time=0.01.\n",
            "epoch 132/250, loss=0.0008, time=0.01.\n",
            "epoch 133/250, loss=0.0008, time=0.01.\n",
            "epoch 134/250, loss=0.0008, time=0.01.\n",
            "epoch 135/250, loss=0.0008, time=0.01.\n",
            "epoch 136/250, loss=0.0008, time=0.01.\n",
            "epoch 137/250, loss=0.0008, time=0.01.\n",
            "epoch 138/250, loss=0.0008, time=0.01.\n",
            "epoch 139/250, loss=0.0007, time=0.01.\n",
            "epoch 140/250, loss=0.0007, time=0.01.\n",
            "epoch 141/250, loss=0.0007, time=0.01.\n",
            "epoch 142/250, loss=0.0007, time=0.01.\n",
            "epoch 143/250, loss=0.0007, time=0.01.\n",
            "epoch 144/250, loss=0.0007, time=0.01.\n",
            "epoch 145/250, loss=0.0007, time=0.01.\n",
            "epoch 146/250, loss=0.0007, time=0.01.\n",
            "epoch 147/250, loss=0.0007, time=0.01.\n",
            "epoch 148/250, loss=0.0007, time=0.01.\n",
            "epoch 149/250, loss=0.0007, time=0.01.\n",
            "epoch 150/250, loss=0.0007, time=0.01.\n",
            "epoch 151/250, loss=0.0007, time=0.01.\n",
            "epoch 152/250, loss=0.0007, time=0.01.\n",
            "epoch 153/250, loss=0.0007, time=0.01.\n",
            "epoch 154/250, loss=0.0007, time=0.01.\n",
            "epoch 155/250, loss=0.0007, time=0.01.\n",
            "epoch 156/250, loss=0.0007, time=0.01.\n",
            "epoch 157/250, loss=0.0007, time=0.01.\n",
            "epoch 158/250, loss=0.0006, time=0.01.\n",
            "epoch 159/250, loss=0.0006, time=0.01.\n",
            "epoch 160/250, loss=0.0006, time=0.01.\n",
            "epoch 161/250, loss=0.0006, time=0.01.\n",
            "epoch 162/250, loss=0.0006, time=0.01.\n",
            "epoch 163/250, loss=0.0006, time=0.01.\n",
            "epoch 164/250, loss=0.0006, time=0.01.\n",
            "epoch 165/250, loss=0.0006, time=0.01.\n",
            "epoch 166/250, loss=0.0006, time=0.01.\n",
            "epoch 167/250, loss=0.0006, time=0.01.\n",
            "epoch 168/250, loss=0.0006, time=0.01.\n",
            "epoch 169/250, loss=0.0006, time=0.01.\n",
            "epoch 170/250, loss=0.0006, time=0.01.\n",
            "epoch 171/250, loss=0.0006, time=0.01.\n",
            "epoch 172/250, loss=0.0006, time=0.01.\n",
            "epoch 173/250, loss=0.0006, time=0.01.\n",
            "epoch 174/250, loss=0.0006, time=0.01.\n",
            "epoch 175/250, loss=0.0006, time=0.01.\n",
            "epoch 176/250, loss=0.0006, time=0.01.\n",
            "epoch 177/250, loss=0.0006, time=0.01.\n",
            "epoch 178/250, loss=0.0006, time=0.01.\n",
            "epoch 179/250, loss=0.0006, time=0.01.\n",
            "epoch 180/250, loss=0.0006, time=0.01.\n",
            "epoch 181/250, loss=0.0006, time=0.01.\n",
            "epoch 182/250, loss=0.0005, time=0.01.\n",
            "epoch 183/250, loss=0.0005, time=0.01.\n",
            "epoch 184/250, loss=0.0005, time=0.01.\n",
            "epoch 185/250, loss=0.0005, time=0.01.\n",
            "epoch 186/250, loss=0.0005, time=0.01.\n",
            "epoch 187/250, loss=0.0005, time=0.01.\n",
            "epoch 188/250, loss=0.0005, time=0.01.\n",
            "epoch 189/250, loss=0.0005, time=0.01.\n",
            "epoch 190/250, loss=0.0005, time=0.01.\n",
            "epoch 191/250, loss=0.0005, time=0.01.\n",
            "epoch 192/250, loss=0.0005, time=0.01.\n",
            "epoch 193/250, loss=0.0005, time=0.01.\n",
            "epoch 194/250, loss=0.0005, time=0.01.\n",
            "epoch 195/250, loss=0.0005, time=0.01.\n",
            "epoch 196/250, loss=0.0005, time=0.01.\n",
            "epoch 197/250, loss=0.0005, time=0.01.\n",
            "epoch 198/250, loss=0.0005, time=0.01.\n",
            "epoch 199/250, loss=0.0005, time=0.01.\n",
            "epoch 200/250, loss=0.0005, time=0.01.\n",
            "epoch 201/250, loss=0.0005, time=0.01.\n",
            "epoch 202/250, loss=0.0005, time=0.01.\n",
            "epoch 203/250, loss=0.0005, time=0.01.\n",
            "epoch 204/250, loss=0.0005, time=0.01.\n",
            "epoch 205/250, loss=0.0005, time=0.01.\n",
            "epoch 206/250, loss=0.0005, time=0.01.\n",
            "epoch 207/250, loss=0.0005, time=0.01.\n",
            "epoch 208/250, loss=0.0005, time=0.01.\n",
            "epoch 209/250, loss=0.0005, time=0.01.\n",
            "epoch 210/250, loss=0.0005, time=0.01.\n",
            "epoch 211/250, loss=0.0005, time=0.01.\n",
            "epoch 212/250, loss=0.0005, time=0.01.\n",
            "epoch 213/250, loss=0.0005, time=0.01.\n",
            "epoch 214/250, loss=0.0004, time=0.01.\n",
            "epoch 215/250, loss=0.0004, time=0.01.\n",
            "epoch 216/250, loss=0.0004, time=0.01.\n",
            "epoch 217/250, loss=0.0004, time=0.01.\n",
            "epoch 218/250, loss=0.0004, time=0.01.\n",
            "epoch 219/250, loss=0.0004, time=0.01.\n",
            "epoch 220/250, loss=0.0004, time=0.01.\n",
            "epoch 221/250, loss=0.0004, time=0.01.\n",
            "epoch 222/250, loss=0.0004, time=0.01.\n",
            "epoch 223/250, loss=0.0004, time=0.01.\n",
            "epoch 224/250, loss=0.0004, time=0.01.\n",
            "epoch 225/250, loss=0.0004, time=0.01.\n",
            "epoch 226/250, loss=0.0004, time=0.01.\n",
            "epoch 227/250, loss=0.0004, time=0.01.\n",
            "epoch 228/250, loss=0.0004, time=0.01.\n",
            "epoch 229/250, loss=0.0004, time=0.01.\n",
            "epoch 230/250, loss=0.0004, time=0.01.\n",
            "epoch 231/250, loss=0.0004, time=0.01.\n",
            "epoch 232/250, loss=0.0004, time=0.01.\n",
            "epoch 233/250, loss=0.0004, time=0.01.\n",
            "epoch 234/250, loss=0.0004, time=0.01.\n",
            "epoch 235/250, loss=0.0004, time=0.01.\n",
            "epoch 236/250, loss=0.0004, time=0.01.\n",
            "epoch 237/250, loss=0.0004, time=0.01.\n",
            "epoch 238/250, loss=0.0004, time=0.01.\n",
            "epoch 239/250, loss=0.0004, time=0.01.\n",
            "epoch 240/250, loss=0.0004, time=0.01.\n",
            "epoch 241/250, loss=0.0004, time=0.01.\n",
            "epoch 242/250, loss=0.0004, time=0.01.\n",
            "epoch 243/250, loss=0.0004, time=0.01.\n",
            "epoch 244/250, loss=0.0004, time=0.01.\n",
            "epoch 245/250, loss=0.0004, time=0.01.\n",
            "epoch 246/250, loss=0.0004, time=0.01.\n",
            "epoch 247/250, loss=0.0004, time=0.01.\n",
            "epoch 248/250, loss=0.0004, time=0.01.\n",
            "epoch 249/250, loss=0.0004, time=0.01.\n",
            "epoch 250/250, loss=0.0004, time=0.01.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0249.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0029.npz, 0.3000, 0.6000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0239.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0094.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0189.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0069.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0049.npz, 0.4000, 0.6000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0014.npz, 0.1000, 0.2000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0199.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0109.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0034.npz, 0.3000, 0.6000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0214.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0204.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0009.npz, 0.2000, 0.1000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0119.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0059.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0159.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0174.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0164.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0184.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0149.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0194.npz, 0.4000, 0.5000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0039.npz, 0.4000, 0.6000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0219.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0134.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0209.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0089.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0234.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0244.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0144.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0154.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0019.npz, 0.3000, 0.4000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0229.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0104.npz, 0.4000, 0.5000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0084.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0064.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0124.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0024.npz, 0.3000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0054.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0099.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0004.npz, 0.1000, 0.2000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0079.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0074.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0114.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0129.npz, 0.4000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0179.npz, 0.4000, 0.5000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0044.npz, 0.4000, 0.6000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0224.npz, 0.4000, 0.5000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0139.npz, 0.4000, 0.5000, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_ShakeGestureWiimoteZ/alst_sc_c_0000_0169.npz, 0.4000, 0.5000, 0.01\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=1.1048, time=0.03.\n",
            "epoch 2/250, loss=1.0117, time=0.03.\n",
            "epoch 3/250, loss=1.0037, time=0.03.\n",
            "epoch 4/250, loss=0.9489, time=0.03.\n",
            "epoch 5/250, loss=0.8913, time=0.03.\n",
            "epoch 6/250, loss=0.7909, time=0.03.\n",
            "epoch 7/250, loss=0.6274, time=0.03.\n",
            "epoch 8/250, loss=0.5203, time=0.03.\n",
            "epoch 9/250, loss=0.4007, time=0.02.\n",
            "epoch 10/250, loss=0.2982, time=0.02.\n",
            "epoch 11/250, loss=0.2265, time=0.02.\n",
            "epoch 12/250, loss=0.1605, time=0.02.\n",
            "epoch 13/250, loss=0.1131, time=0.02.\n",
            "epoch 14/250, loss=0.0744, time=0.02.\n",
            "epoch 15/250, loss=0.0468, time=0.02.\n",
            "epoch 16/250, loss=0.0348, time=0.02.\n",
            "epoch 17/250, loss=0.0233, time=0.02.\n",
            "epoch 18/250, loss=0.0153, time=0.02.\n",
            "epoch 19/250, loss=0.0109, time=0.02.\n",
            "epoch 20/250, loss=0.0080, time=0.02.\n",
            "epoch 21/250, loss=0.0057, time=0.02.\n",
            "epoch 22/250, loss=0.0040, time=0.02.\n",
            "epoch 23/250, loss=0.0035, time=0.02.\n",
            "epoch 24/250, loss=0.0026, time=0.02.\n",
            "epoch 25/250, loss=0.0020, time=0.02.\n",
            "epoch 26/250, loss=0.0017, time=0.02.\n",
            "epoch 27/250, loss=0.0016, time=0.02.\n",
            "epoch 28/250, loss=0.0013, time=0.02.\n",
            "epoch 29/250, loss=0.0012, time=0.02.\n",
            "epoch 30/250, loss=0.0010, time=0.02.\n",
            "epoch 31/250, loss=0.0009, time=0.02.\n",
            "epoch 32/250, loss=0.0008, time=0.02.\n",
            "epoch 33/250, loss=0.0007, time=0.02.\n",
            "epoch 34/250, loss=0.0007, time=0.02.\n",
            "epoch 35/250, loss=0.0006, time=0.02.\n",
            "epoch 36/250, loss=0.0006, time=0.02.\n",
            "epoch 37/250, loss=0.0006, time=0.02.\n",
            "epoch 38/250, loss=0.0006, time=0.02.\n",
            "epoch 39/250, loss=0.0005, time=0.02.\n",
            "epoch 40/250, loss=0.0005, time=0.02.\n",
            "epoch 41/250, loss=0.0005, time=0.02.\n",
            "epoch 42/250, loss=0.0005, time=0.02.\n",
            "epoch 43/250, loss=0.0005, time=0.02.\n",
            "epoch 44/250, loss=0.0005, time=0.02.\n",
            "epoch 45/250, loss=0.0005, time=0.02.\n",
            "epoch 46/250, loss=0.0004, time=0.02.\n",
            "epoch 47/250, loss=0.0004, time=0.02.\n",
            "epoch 48/250, loss=0.0004, time=0.02.\n",
            "epoch 49/250, loss=0.0004, time=0.02.\n",
            "epoch 50/250, loss=0.0004, time=0.02.\n",
            "epoch 51/250, loss=0.0004, time=0.02.\n",
            "epoch 52/250, loss=0.0004, time=0.02.\n",
            "epoch 53/250, loss=0.0004, time=0.02.\n",
            "epoch 54/250, loss=0.0003, time=0.02.\n",
            "epoch 55/250, loss=0.0004, time=0.02.\n",
            "epoch 56/250, loss=0.0003, time=0.02.\n",
            "epoch 57/250, loss=0.0004, time=0.02.\n",
            "epoch 58/250, loss=0.0003, time=0.02.\n",
            "epoch 59/250, loss=0.0003, time=0.02.\n",
            "epoch 60/250, loss=0.0003, time=0.02.\n",
            "epoch 61/250, loss=0.0003, time=0.02.\n",
            "epoch 62/250, loss=0.0003, time=0.02.\n",
            "epoch 63/250, loss=0.0003, time=0.02.\n",
            "epoch 64/250, loss=0.0003, time=0.02.\n",
            "epoch 65/250, loss=0.0003, time=0.02.\n",
            "epoch 66/250, loss=0.0003, time=0.02.\n",
            "epoch 67/250, loss=0.0003, time=0.02.\n",
            "epoch 68/250, loss=0.0003, time=0.02.\n",
            "epoch 69/250, loss=0.0003, time=0.02.\n",
            "epoch 70/250, loss=0.0003, time=0.02.\n",
            "epoch 71/250, loss=0.0003, time=0.02.\n",
            "epoch 72/250, loss=0.0003, time=0.02.\n",
            "epoch 73/250, loss=0.0003, time=0.02.\n",
            "epoch 74/250, loss=0.0003, time=0.02.\n",
            "epoch 75/250, loss=0.0003, time=0.02.\n",
            "epoch 76/250, loss=0.0003, time=0.02.\n",
            "epoch 77/250, loss=0.0002, time=0.02.\n",
            "epoch 78/250, loss=0.0003, time=0.02.\n",
            "epoch 79/250, loss=0.0002, time=0.02.\n",
            "epoch 80/250, loss=0.0002, time=0.02.\n",
            "epoch 81/250, loss=0.0002, time=0.02.\n",
            "epoch 82/250, loss=0.0002, time=0.02.\n",
            "epoch 83/250, loss=0.0002, time=0.02.\n",
            "epoch 84/250, loss=0.0002, time=0.02.\n",
            "epoch 85/250, loss=0.0002, time=0.02.\n",
            "epoch 86/250, loss=0.0002, time=0.02.\n",
            "epoch 87/250, loss=0.0002, time=0.02.\n",
            "epoch 88/250, loss=0.0002, time=0.02.\n",
            "epoch 89/250, loss=0.0002, time=0.02.\n",
            "epoch 90/250, loss=0.0002, time=0.02.\n",
            "epoch 91/250, loss=0.0002, time=0.02.\n",
            "epoch 92/250, loss=0.0002, time=0.02.\n",
            "epoch 93/250, loss=0.0002, time=0.02.\n",
            "epoch 94/250, loss=0.0002, time=0.02.\n",
            "epoch 95/250, loss=0.0002, time=0.02.\n",
            "epoch 96/250, loss=0.0002, time=0.02.\n",
            "epoch 97/250, loss=0.0002, time=0.02.\n",
            "epoch 98/250, loss=0.0002, time=0.02.\n",
            "epoch 99/250, loss=0.0002, time=0.02.\n",
            "epoch 100/250, loss=0.0002, time=0.02.\n",
            "epoch 101/250, loss=0.0002, time=0.02.\n",
            "epoch 102/250, loss=0.0002, time=0.02.\n",
            "epoch 103/250, loss=0.0002, time=0.02.\n",
            "epoch 104/250, loss=0.0002, time=0.02.\n",
            "epoch 105/250, loss=0.0002, time=0.02.\n",
            "epoch 106/250, loss=0.0002, time=0.02.\n",
            "epoch 107/250, loss=0.0002, time=0.02.\n",
            "epoch 108/250, loss=0.0002, time=0.02.\n",
            "epoch 109/250, loss=0.0002, time=0.02.\n",
            "epoch 110/250, loss=0.0002, time=0.02.\n",
            "epoch 111/250, loss=0.0002, time=0.02.\n",
            "epoch 112/250, loss=0.0002, time=0.02.\n",
            "epoch 113/250, loss=0.0002, time=0.02.\n",
            "epoch 114/250, loss=0.0002, time=0.02.\n",
            "epoch 115/250, loss=0.0001, time=0.02.\n",
            "epoch 116/250, loss=0.0001, time=0.02.\n",
            "epoch 117/250, loss=0.0002, time=0.02.\n",
            "epoch 118/250, loss=0.0001, time=0.02.\n",
            "epoch 119/250, loss=0.0001, time=0.02.\n",
            "epoch 120/250, loss=0.0001, time=0.02.\n",
            "epoch 121/250, loss=0.0001, time=0.02.\n",
            "epoch 122/250, loss=0.0001, time=0.02.\n",
            "epoch 123/250, loss=0.0001, time=0.02.\n",
            "epoch 124/250, loss=0.0001, time=0.02.\n",
            "epoch 125/250, loss=0.0001, time=0.02.\n",
            "epoch 126/250, loss=0.0001, time=0.02.\n",
            "epoch 127/250, loss=0.0001, time=0.02.\n",
            "epoch 128/250, loss=0.0001, time=0.02.\n",
            "epoch 129/250, loss=0.0001, time=0.02.\n",
            "epoch 130/250, loss=0.0001, time=0.02.\n",
            "epoch 131/250, loss=0.0001, time=0.02.\n",
            "epoch 132/250, loss=0.0001, time=0.02.\n",
            "epoch 133/250, loss=0.0001, time=0.02.\n",
            "epoch 134/250, loss=0.0001, time=0.02.\n",
            "epoch 135/250, loss=0.0001, time=0.02.\n",
            "epoch 136/250, loss=0.0001, time=0.02.\n",
            "epoch 137/250, loss=0.0001, time=0.02.\n",
            "epoch 138/250, loss=0.0001, time=0.02.\n",
            "epoch 139/250, loss=0.0001, time=0.02.\n",
            "epoch 140/250, loss=0.0001, time=0.02.\n",
            "epoch 141/250, loss=0.0001, time=0.02.\n",
            "epoch 142/250, loss=0.0001, time=0.02.\n",
            "epoch 143/250, loss=0.0001, time=0.02.\n",
            "epoch 144/250, loss=0.0001, time=0.02.\n",
            "epoch 145/250, loss=0.0001, time=0.02.\n",
            "epoch 146/250, loss=0.0001, time=0.02.\n",
            "epoch 147/250, loss=0.0001, time=0.02.\n",
            "epoch 148/250, loss=0.0001, time=0.02.\n",
            "epoch 149/250, loss=0.0001, time=0.02.\n",
            "epoch 150/250, loss=0.0001, time=0.02.\n",
            "epoch 151/250, loss=0.0001, time=0.02.\n",
            "epoch 152/250, loss=0.0001, time=0.02.\n",
            "epoch 153/250, loss=0.0001, time=0.02.\n",
            "epoch 154/250, loss=0.0001, time=0.02.\n",
            "epoch 155/250, loss=0.0001, time=0.02.\n",
            "epoch 156/250, loss=0.0001, time=0.02.\n",
            "epoch 157/250, loss=0.0001, time=0.02.\n",
            "epoch 158/250, loss=0.0001, time=0.02.\n",
            "epoch 159/250, loss=0.0001, time=0.02.\n",
            "epoch 160/250, loss=0.0001, time=0.02.\n",
            "epoch 161/250, loss=0.0001, time=0.02.\n",
            "epoch 162/250, loss=0.0001, time=0.02.\n",
            "epoch 163/250, loss=0.0001, time=0.02.\n",
            "epoch 164/250, loss=0.0001, time=0.02.\n",
            "epoch 165/250, loss=0.0001, time=0.02.\n",
            "epoch 166/250, loss=0.0001, time=0.02.\n",
            "epoch 167/250, loss=0.0001, time=0.02.\n",
            "epoch 168/250, loss=0.0001, time=0.02.\n",
            "epoch 169/250, loss=0.0001, time=0.02.\n",
            "epoch 170/250, loss=0.0001, time=0.02.\n",
            "epoch 171/250, loss=0.0001, time=0.02.\n",
            "epoch 172/250, loss=0.0001, time=0.02.\n",
            "epoch 173/250, loss=0.0001, time=0.02.\n",
            "epoch 174/250, loss=0.0001, time=0.02.\n",
            "epoch 175/250, loss=0.0001, time=0.02.\n",
            "epoch 176/250, loss=0.0001, time=0.02.\n",
            "epoch 177/250, loss=0.0001, time=0.02.\n",
            "epoch 178/250, loss=0.0001, time=0.02.\n",
            "epoch 179/250, loss=0.0001, time=0.02.\n",
            "epoch 180/250, loss=0.0001, time=0.02.\n",
            "epoch 181/250, loss=0.0001, time=0.02.\n",
            "epoch 182/250, loss=0.0001, time=0.02.\n",
            "epoch 183/250, loss=0.0001, time=0.02.\n",
            "epoch 184/250, loss=0.0001, time=0.02.\n",
            "epoch 185/250, loss=0.0001, time=0.02.\n",
            "epoch 186/250, loss=0.0001, time=0.02.\n",
            "epoch 187/250, loss=0.0001, time=0.02.\n",
            "epoch 188/250, loss=0.0001, time=0.02.\n",
            "epoch 189/250, loss=0.0001, time=0.02.\n",
            "epoch 190/250, loss=0.0001, time=0.02.\n",
            "epoch 191/250, loss=0.0001, time=0.02.\n",
            "epoch 192/250, loss=0.0001, time=0.02.\n",
            "epoch 193/250, loss=0.0001, time=0.02.\n",
            "epoch 194/250, loss=0.0001, time=0.02.\n",
            "epoch 195/250, loss=0.0001, time=0.02.\n",
            "epoch 196/250, loss=0.0001, time=0.02.\n",
            "epoch 197/250, loss=0.0001, time=0.02.\n",
            "epoch 198/250, loss=0.0001, time=0.02.\n",
            "epoch 199/250, loss=0.0001, time=0.02.\n",
            "epoch 200/250, loss=0.0001, time=0.02.\n",
            "epoch 201/250, loss=0.0001, time=0.02.\n",
            "epoch 202/250, loss=0.0001, time=0.02.\n",
            "epoch 203/250, loss=0.0001, time=0.02.\n",
            "epoch 204/250, loss=0.0001, time=0.02.\n",
            "epoch 205/250, loss=0.0001, time=0.02.\n",
            "epoch 206/250, loss=0.0001, time=0.02.\n",
            "epoch 207/250, loss=0.0001, time=0.02.\n",
            "epoch 208/250, loss=0.0001, time=0.02.\n",
            "epoch 209/250, loss=0.0001, time=0.02.\n",
            "epoch 210/250, loss=0.0001, time=0.02.\n",
            "epoch 211/250, loss=0.0001, time=0.02.\n",
            "epoch 212/250, loss=0.0001, time=0.02.\n",
            "epoch 213/250, loss=0.0001, time=0.02.\n",
            "epoch 214/250, loss=0.0001, time=0.02.\n",
            "epoch 215/250, loss=0.0001, time=0.02.\n",
            "epoch 216/250, loss=0.0001, time=0.02.\n",
            "epoch 217/250, loss=0.0001, time=0.02.\n",
            "epoch 218/250, loss=0.0001, time=0.02.\n",
            "epoch 219/250, loss=0.0001, time=0.02.\n",
            "epoch 220/250, loss=0.0001, time=0.02.\n",
            "epoch 221/250, loss=0.0001, time=0.02.\n",
            "epoch 222/250, loss=0.0001, time=0.02.\n",
            "epoch 223/250, loss=0.0001, time=0.02.\n",
            "epoch 224/250, loss=0.0001, time=0.02.\n",
            "epoch 225/250, loss=0.0001, time=0.02.\n",
            "epoch 226/250, loss=0.0001, time=0.02.\n",
            "epoch 227/250, loss=0.0001, time=0.02.\n",
            "epoch 228/250, loss=0.0001, time=0.02.\n",
            "epoch 229/250, loss=0.0001, time=0.02.\n",
            "epoch 230/250, loss=0.0001, time=0.02.\n",
            "epoch 231/250, loss=0.0001, time=0.02.\n",
            "epoch 232/250, loss=0.0001, time=0.02.\n",
            "epoch 233/250, loss=0.0001, time=0.02.\n",
            "epoch 234/250, loss=0.0001, time=0.02.\n",
            "epoch 235/250, loss=0.0001, time=0.02.\n",
            "epoch 236/250, loss=0.0001, time=0.02.\n",
            "epoch 237/250, loss=0.0001, time=0.02.\n",
            "epoch 238/250, loss=0.0001, time=0.02.\n",
            "epoch 239/250, loss=0.0001, time=0.02.\n",
            "epoch 240/250, loss=0.0001, time=0.02.\n",
            "epoch 241/250, loss=0.0001, time=0.02.\n",
            "epoch 242/250, loss=0.0001, time=0.02.\n",
            "epoch 243/250, loss=0.0001, time=0.02.\n",
            "epoch 244/250, loss=0.0001, time=0.02.\n",
            "epoch 245/250, loss=0.0001, time=0.02.\n",
            "epoch 246/250, loss=0.0001, time=0.02.\n",
            "epoch 247/250, loss=0.0001, time=0.02.\n",
            "epoch 248/250, loss=0.0001, time=0.02.\n",
            "epoch 249/250, loss=0.0000, time=0.02.\n",
            "epoch 250/250, loss=0.0000, time=0.02.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0174.npz, 0.4667, 0.4667, 0.01\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0059.npz, 0.5000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0054.npz, 0.5000, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0199.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0019.npz, 0.4333, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0134.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0109.npz, 0.5000, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0039.npz, 0.4667, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0049.npz, 0.4667, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0239.npz, 0.4333, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0014.npz, 0.5000, 0.4333, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0194.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0024.npz, 0.4333, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0184.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0139.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0209.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0219.npz, 0.4333, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0154.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0094.npz, 0.5000, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0244.npz, 0.4333, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0149.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0144.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0004.npz, 0.4000, 0.3000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0074.npz, 0.5000, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0129.npz, 0.5000, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0234.npz, 0.4333, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0229.npz, 0.4333, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0124.npz, 0.5000, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0159.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0089.npz, 0.5000, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0169.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0204.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0164.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0034.npz, 0.4333, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0119.npz, 0.5000, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0099.npz, 0.5000, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0214.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0009.npz, 0.5333, 0.3667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0224.npz, 0.4333, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0084.npz, 0.5000, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0179.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0064.npz, 0.5000, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0104.npz, 0.5000, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0189.npz, 0.4667, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0079.npz, 0.5000, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0114.npz, 0.5000, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0069.npz, 0.5000, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0044.npz, 0.4667, 0.5000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0249.npz, 0.4333, 0.4667, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_SmoothSubspace/alst_sc_c_0000_0029.npz, 0.4667, 0.4667, 0.00\n",
            "get model for classifier_simclr_rnnet\n",
            "  get rnnet\n",
            "  get simclr\n",
            "  get classifier\n",
            "epoch 1/250, loss=1.1201, time=0.02.\n",
            "epoch 2/250, loss=1.0684, time=0.02.\n",
            "epoch 3/250, loss=0.8917, time=0.02.\n",
            "epoch 4/250, loss=0.7891, time=0.02.\n",
            "epoch 5/250, loss=0.7350, time=0.02.\n",
            "epoch 6/250, loss=0.6235, time=0.02.\n",
            "epoch 7/250, loss=0.5739, time=0.02.\n",
            "epoch 8/250, loss=0.4941, time=0.01.\n",
            "epoch 9/250, loss=0.4675, time=0.01.\n",
            "epoch 10/250, loss=0.4360, time=0.01.\n",
            "epoch 11/250, loss=0.3381, time=0.01.\n",
            "epoch 12/250, loss=0.2941, time=0.01.\n",
            "epoch 13/250, loss=0.2578, time=0.01.\n",
            "epoch 14/250, loss=0.2155, time=0.01.\n",
            "epoch 15/250, loss=0.1666, time=0.01.\n",
            "epoch 16/250, loss=0.1450, time=0.01.\n",
            "epoch 17/250, loss=0.1095, time=0.01.\n",
            "epoch 18/250, loss=0.1029, time=0.01.\n",
            "epoch 19/250, loss=0.0875, time=0.01.\n",
            "epoch 20/250, loss=0.0576, time=0.01.\n",
            "epoch 21/250, loss=0.0519, time=0.01.\n",
            "epoch 22/250, loss=0.0365, time=0.01.\n",
            "epoch 23/250, loss=0.0370, time=0.01.\n",
            "epoch 24/250, loss=0.0192, time=0.01.\n",
            "epoch 25/250, loss=0.0134, time=0.01.\n",
            "epoch 26/250, loss=0.0096, time=0.01.\n",
            "epoch 27/250, loss=0.0086, time=0.01.\n",
            "epoch 28/250, loss=0.0060, time=0.01.\n",
            "epoch 29/250, loss=0.0054, time=0.01.\n",
            "epoch 30/250, loss=0.0043, time=0.01.\n",
            "epoch 31/250, loss=0.0039, time=0.01.\n",
            "epoch 32/250, loss=0.0039, time=0.01.\n",
            "epoch 33/250, loss=0.0033, time=0.01.\n",
            "epoch 34/250, loss=0.0030, time=0.01.\n",
            "epoch 35/250, loss=0.0024, time=0.01.\n",
            "epoch 36/250, loss=0.0020, time=0.01.\n",
            "epoch 37/250, loss=0.0016, time=0.01.\n",
            "epoch 38/250, loss=0.0015, time=0.01.\n",
            "epoch 39/250, loss=0.0013, time=0.01.\n",
            "epoch 40/250, loss=0.0012, time=0.01.\n",
            "epoch 41/250, loss=0.0009, time=0.01.\n",
            "epoch 42/250, loss=0.0008, time=0.01.\n",
            "epoch 43/250, loss=0.0007, time=0.01.\n",
            "epoch 44/250, loss=0.0008, time=0.01.\n",
            "epoch 45/250, loss=0.0006, time=0.01.\n",
            "epoch 46/250, loss=0.0006, time=0.01.\n",
            "epoch 47/250, loss=0.0006, time=0.01.\n",
            "epoch 48/250, loss=0.0006, time=0.01.\n",
            "epoch 49/250, loss=0.0005, time=0.01.\n",
            "epoch 50/250, loss=0.0005, time=0.01.\n",
            "epoch 51/250, loss=0.0004, time=0.01.\n",
            "epoch 52/250, loss=0.0004, time=0.01.\n",
            "epoch 53/250, loss=0.0004, time=0.01.\n",
            "epoch 54/250, loss=0.0004, time=0.01.\n",
            "epoch 55/250, loss=0.0004, time=0.01.\n",
            "epoch 56/250, loss=0.0003, time=0.01.\n",
            "epoch 57/250, loss=0.0003, time=0.01.\n",
            "epoch 58/250, loss=0.0003, time=0.01.\n",
            "epoch 59/250, loss=0.0003, time=0.01.\n",
            "epoch 60/250, loss=0.0004, time=0.01.\n",
            "epoch 61/250, loss=0.0003, time=0.01.\n",
            "epoch 62/250, loss=0.0003, time=0.01.\n",
            "epoch 63/250, loss=0.0003, time=0.01.\n",
            "epoch 64/250, loss=0.0003, time=0.01.\n",
            "epoch 65/250, loss=0.0004, time=0.01.\n",
            "epoch 66/250, loss=0.0004, time=0.01.\n",
            "epoch 67/250, loss=0.0003, time=0.01.\n",
            "epoch 68/250, loss=0.0004, time=0.01.\n",
            "epoch 69/250, loss=0.0003, time=0.01.\n",
            "epoch 70/250, loss=0.0004, time=0.01.\n",
            "epoch 71/250, loss=0.0003, time=0.01.\n",
            "epoch 72/250, loss=0.0003, time=0.01.\n",
            "epoch 73/250, loss=0.0003, time=0.01.\n",
            "epoch 74/250, loss=0.0003, time=0.01.\n",
            "epoch 75/250, loss=0.0003, time=0.01.\n",
            "epoch 76/250, loss=0.0003, time=0.01.\n",
            "epoch 77/250, loss=0.0003, time=0.01.\n",
            "epoch 78/250, loss=0.0003, time=0.01.\n",
            "epoch 79/250, loss=0.0003, time=0.01.\n",
            "epoch 80/250, loss=0.0002, time=0.01.\n",
            "epoch 81/250, loss=0.0003, time=0.01.\n",
            "epoch 82/250, loss=0.0003, time=0.01.\n",
            "epoch 83/250, loss=0.0003, time=0.01.\n",
            "epoch 84/250, loss=0.0002, time=0.01.\n",
            "epoch 85/250, loss=0.0002, time=0.01.\n",
            "epoch 86/250, loss=0.0002, time=0.01.\n",
            "epoch 87/250, loss=0.0002, time=0.01.\n",
            "epoch 88/250, loss=0.0002, time=0.01.\n",
            "epoch 89/250, loss=0.0002, time=0.01.\n",
            "epoch 90/250, loss=0.0002, time=0.01.\n",
            "epoch 91/250, loss=0.0002, time=0.01.\n",
            "epoch 92/250, loss=0.0002, time=0.01.\n",
            "epoch 93/250, loss=0.0002, time=0.01.\n",
            "epoch 94/250, loss=0.0002, time=0.01.\n",
            "epoch 95/250, loss=0.0002, time=0.01.\n",
            "epoch 96/250, loss=0.0002, time=0.01.\n",
            "epoch 97/250, loss=0.0002, time=0.01.\n",
            "epoch 98/250, loss=0.0002, time=0.01.\n",
            "epoch 99/250, loss=0.0002, time=0.01.\n",
            "epoch 100/250, loss=0.0002, time=0.01.\n",
            "epoch 101/250, loss=0.0002, time=0.01.\n",
            "epoch 102/250, loss=0.0002, time=0.01.\n",
            "epoch 103/250, loss=0.0002, time=0.01.\n",
            "epoch 104/250, loss=0.0002, time=0.01.\n",
            "epoch 105/250, loss=0.0002, time=0.01.\n",
            "epoch 106/250, loss=0.0002, time=0.01.\n",
            "epoch 107/250, loss=0.0002, time=0.01.\n",
            "epoch 108/250, loss=0.0002, time=0.01.\n",
            "epoch 109/250, loss=0.0002, time=0.01.\n",
            "epoch 110/250, loss=0.0002, time=0.01.\n",
            "epoch 111/250, loss=0.0002, time=0.01.\n",
            "epoch 112/250, loss=0.0002, time=0.01.\n",
            "epoch 113/250, loss=0.0002, time=0.01.\n",
            "epoch 114/250, loss=0.0001, time=0.01.\n",
            "epoch 115/250, loss=0.0002, time=0.01.\n",
            "epoch 116/250, loss=0.0002, time=0.01.\n",
            "epoch 117/250, loss=0.0002, time=0.01.\n",
            "epoch 118/250, loss=0.0002, time=0.01.\n",
            "epoch 119/250, loss=0.0002, time=0.01.\n",
            "epoch 120/250, loss=0.0002, time=0.01.\n",
            "epoch 121/250, loss=0.0002, time=0.01.\n",
            "epoch 122/250, loss=0.0001, time=0.01.\n",
            "epoch 123/250, loss=0.0002, time=0.01.\n",
            "epoch 124/250, loss=0.0001, time=0.01.\n",
            "epoch 125/250, loss=0.0001, time=0.01.\n",
            "epoch 126/250, loss=0.0001, time=0.01.\n",
            "epoch 127/250, loss=0.0001, time=0.01.\n",
            "epoch 128/250, loss=0.0001, time=0.01.\n",
            "epoch 129/250, loss=0.0001, time=0.01.\n",
            "epoch 130/250, loss=0.0001, time=0.01.\n",
            "epoch 131/250, loss=0.0001, time=0.01.\n",
            "epoch 132/250, loss=0.0001, time=0.01.\n",
            "epoch 133/250, loss=0.0001, time=0.01.\n",
            "epoch 134/250, loss=0.0001, time=0.01.\n",
            "epoch 135/250, loss=0.0001, time=0.01.\n",
            "epoch 136/250, loss=0.0001, time=0.01.\n",
            "epoch 137/250, loss=0.0001, time=0.01.\n",
            "epoch 138/250, loss=0.0001, time=0.01.\n",
            "epoch 139/250, loss=0.0001, time=0.01.\n",
            "epoch 140/250, loss=0.0001, time=0.01.\n",
            "epoch 141/250, loss=0.0001, time=0.01.\n",
            "epoch 142/250, loss=0.0001, time=0.01.\n",
            "epoch 143/250, loss=0.0001, time=0.01.\n",
            "epoch 144/250, loss=0.0001, time=0.01.\n",
            "epoch 145/250, loss=0.0001, time=0.01.\n",
            "epoch 146/250, loss=0.0001, time=0.01.\n",
            "epoch 147/250, loss=0.0001, time=0.01.\n",
            "epoch 148/250, loss=0.0001, time=0.01.\n",
            "epoch 149/250, loss=0.0001, time=0.01.\n",
            "epoch 150/250, loss=0.0001, time=0.01.\n",
            "epoch 151/250, loss=0.0001, time=0.01.\n",
            "epoch 152/250, loss=0.0001, time=0.01.\n",
            "epoch 153/250, loss=0.0001, time=0.01.\n",
            "epoch 154/250, loss=0.0001, time=0.01.\n",
            "epoch 155/250, loss=0.0001, time=0.01.\n",
            "epoch 156/250, loss=0.0001, time=0.01.\n",
            "epoch 157/250, loss=0.0001, time=0.01.\n",
            "epoch 158/250, loss=0.0001, time=0.01.\n",
            "epoch 159/250, loss=0.0001, time=0.01.\n",
            "epoch 160/250, loss=0.0001, time=0.01.\n",
            "epoch 161/250, loss=0.0001, time=0.01.\n",
            "epoch 162/250, loss=0.0001, time=0.01.\n",
            "epoch 163/250, loss=0.0001, time=0.01.\n",
            "epoch 164/250, loss=0.0001, time=0.01.\n",
            "epoch 165/250, loss=0.0001, time=0.01.\n",
            "epoch 166/250, loss=0.0001, time=0.01.\n",
            "epoch 167/250, loss=0.0001, time=0.01.\n",
            "epoch 168/250, loss=0.0001, time=0.01.\n",
            "epoch 169/250, loss=0.0001, time=0.01.\n",
            "epoch 170/250, loss=0.0001, time=0.01.\n",
            "epoch 171/250, loss=0.0001, time=0.01.\n",
            "epoch 172/250, loss=0.0001, time=0.01.\n",
            "epoch 173/250, loss=0.0001, time=0.01.\n",
            "epoch 174/250, loss=0.0001, time=0.01.\n",
            "epoch 175/250, loss=0.0001, time=0.01.\n",
            "epoch 176/250, loss=0.0001, time=0.01.\n",
            "epoch 177/250, loss=0.0001, time=0.01.\n",
            "epoch 178/250, loss=0.0001, time=0.01.\n",
            "epoch 179/250, loss=0.0001, time=0.01.\n",
            "epoch 180/250, loss=0.0001, time=0.01.\n",
            "epoch 181/250, loss=0.0001, time=0.01.\n",
            "epoch 182/250, loss=0.0001, time=0.01.\n",
            "epoch 183/250, loss=0.0001, time=0.01.\n",
            "epoch 184/250, loss=0.0001, time=0.01.\n",
            "epoch 185/250, loss=0.0001, time=0.01.\n",
            "epoch 186/250, loss=0.0001, time=0.01.\n",
            "epoch 187/250, loss=0.0001, time=0.01.\n",
            "epoch 188/250, loss=0.0001, time=0.01.\n",
            "epoch 189/250, loss=0.0001, time=0.01.\n",
            "epoch 190/250, loss=0.0001, time=0.01.\n",
            "epoch 191/250, loss=0.0001, time=0.01.\n",
            "epoch 192/250, loss=0.0001, time=0.01.\n",
            "epoch 193/250, loss=0.0001, time=0.01.\n",
            "epoch 194/250, loss=0.0001, time=0.01.\n",
            "epoch 195/250, loss=0.0001, time=0.01.\n",
            "epoch 196/250, loss=0.0001, time=0.01.\n",
            "epoch 197/250, loss=0.0001, time=0.01.\n",
            "epoch 198/250, loss=0.0001, time=0.01.\n",
            "epoch 199/250, loss=0.0001, time=0.01.\n",
            "epoch 200/250, loss=0.0001, time=0.01.\n",
            "epoch 201/250, loss=0.0001, time=0.01.\n",
            "epoch 202/250, loss=0.0001, time=0.01.\n",
            "epoch 203/250, loss=0.0001, time=0.01.\n",
            "epoch 204/250, loss=0.0001, time=0.01.\n",
            "epoch 205/250, loss=0.0001, time=0.01.\n",
            "epoch 206/250, loss=0.0001, time=0.01.\n",
            "epoch 207/250, loss=0.0001, time=0.01.\n",
            "epoch 208/250, loss=0.0001, time=0.01.\n",
            "epoch 209/250, loss=0.0001, time=0.01.\n",
            "epoch 210/250, loss=0.0001, time=0.01.\n",
            "epoch 211/250, loss=0.0001, time=0.01.\n",
            "epoch 212/250, loss=0.0001, time=0.01.\n",
            "epoch 213/250, loss=0.0001, time=0.01.\n",
            "epoch 214/250, loss=0.0001, time=0.01.\n",
            "epoch 215/250, loss=0.0001, time=0.01.\n",
            "epoch 216/250, loss=0.0001, time=0.01.\n",
            "epoch 217/250, loss=0.0001, time=0.01.\n",
            "epoch 218/250, loss=0.0001, time=0.01.\n",
            "epoch 219/250, loss=0.0001, time=0.01.\n",
            "epoch 220/250, loss=0.0001, time=0.01.\n",
            "epoch 221/250, loss=0.0001, time=0.01.\n",
            "epoch 222/250, loss=0.0001, time=0.01.\n",
            "epoch 223/250, loss=0.0001, time=0.01.\n",
            "epoch 224/250, loss=0.0001, time=0.01.\n",
            "epoch 225/250, loss=0.0001, time=0.01.\n",
            "epoch 226/250, loss=0.0001, time=0.01.\n",
            "epoch 227/250, loss=0.0001, time=0.01.\n",
            "epoch 228/250, loss=0.0001, time=0.01.\n",
            "epoch 229/250, loss=0.0001, time=0.01.\n",
            "epoch 230/250, loss=0.0001, time=0.01.\n",
            "epoch 231/250, loss=0.0001, time=0.01.\n",
            "epoch 232/250, loss=0.0001, time=0.01.\n",
            "epoch 233/250, loss=0.0001, time=0.01.\n",
            "epoch 234/250, loss=0.0001, time=0.01.\n",
            "epoch 235/250, loss=0.0001, time=0.01.\n",
            "epoch 236/250, loss=0.0001, time=0.01.\n",
            "epoch 237/250, loss=0.0001, time=0.01.\n",
            "epoch 238/250, loss=0.0001, time=0.01.\n",
            "epoch 239/250, loss=0.0001, time=0.01.\n",
            "epoch 240/250, loss=0.0001, time=0.01.\n",
            "epoch 241/250, loss=0.0001, time=0.01.\n",
            "epoch 242/250, loss=0.0001, time=0.01.\n",
            "epoch 243/250, loss=0.0001, time=0.01.\n",
            "epoch 244/250, loss=0.0001, time=0.01.\n",
            "epoch 245/250, loss=0.0001, time=0.01.\n",
            "epoch 246/250, loss=0.0001, time=0.01.\n",
            "epoch 247/250, loss=0.0001, time=0.01.\n",
            "epoch 248/250, loss=0.0001, time=0.01.\n",
            "epoch 249/250, loss=0.0001, time=0.01.\n",
            "epoch 250/250, loss=0.0001, time=0.01.\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0059.npz, 0.8889, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0194.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0124.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0114.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0049.npz, 0.8333, 0.9444, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0244.npz, 0.8333, 0.9444, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0249.npz, 0.8333, 0.9444, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0134.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0154.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0159.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0234.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0219.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0204.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0064.npz, 0.8889, 0.9444, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0199.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0104.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0079.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0019.npz, 0.8889, 0.9444, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0129.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0229.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0164.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0054.npz, 0.8333, 0.9444, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0224.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0174.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0214.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0209.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0149.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0119.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0014.npz, 0.7222, 0.9444, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0089.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0074.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0029.npz, 0.8889, 1.0000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0024.npz, 0.8889, 1.0000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0044.npz, 0.8333, 1.0000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0144.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0139.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0189.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0184.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0094.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0084.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0179.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0239.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0069.npz, 0.8889, 0.9444, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0034.npz, 0.8333, 1.0000, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0109.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0099.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0039.npz, 0.8333, 0.9444, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0009.npz, 0.5000, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0169.npz, 0.8333, 0.8889, 0.00\n",
            "/mnt/drive/MyDrive/STAT940/result/ucr_00_UMD/alst_sc_c_0000_0004.npz, 0.6111, 0.7222, 0.00\n"
          ]
        }
      ],
      "source": [
        "data_name = 'ucr_00'\n",
        "method_name = 'alst_sc_c_0000'\n",
        "fine_tune(data_name, method_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "crZgCh4rATFA",
        "cgsN2EGw_3dM",
        "y7caGl-J-B1h",
        "mSvrdUhC95tO",
        "A6wYhLx79xdD",
        "zOOZ89uJ9tQU",
        "EVAfib1w9kvr",
        "UUG4bA6f8J9M",
        "tyzY5qaa8VRp",
        "Ky82j1EG8mcI",
        "Ed9wkLOYPbds",
        "cdDdqEypgZ8l",
        "S_qlPjIW8hod"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
